<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>R-CNN_Rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation</title>
      <link href="/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/"/>
      <url>/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h1 id="Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation"><a href="#Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation" class="headerlink" title="Rich feature hierarchies for accurate object detection and semantic segmentation"></a><strong>Rich feature hierarchies for accurate object detection and semantic segmentation</strong></h1><p>Ross Girshick Jeff Donahue Trevor Darrell Jitendra Malik</p><p>UC Berkeley</p><p>rbg,jdonahue,trevor,<a href="mailto:&#x6d;&#x61;&#x6c;&#x69;&#x6b;&#x67;&#64;&#101;&#x65;&#99;&#115;&#46;&#98;&#x65;&#114;&#x6b;&#101;&#x6c;&#x65;&#x79;&#x2e;&#101;&#100;&#x75;">&#x6d;&#x61;&#x6c;&#x69;&#x6b;&#x67;&#64;&#101;&#x65;&#99;&#115;&#46;&#98;&#x65;&#114;&#x6b;&#101;&#x6c;&#x65;&#x79;&#x2e;&#101;&#100;&#x75;</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a><strong>Abstract</strong></h2><p>Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012—achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at <a href="http://www.cs.berkeley.edu/rbg/rcnn">http://www.cs.berkeley.edu/rbg/rcnn</a>.</p><p><strong>摘要</strong></p><p>对象检测性能，例如在典型的PASCAL VOC数据集上测量的结果，在过去几年中已经稳定下来，其中表现最好的方案是将多个低级图像特征与高级语境相结合而组成的复杂系统。在本文中，我们提出了一种简单可扩展的检测算法，其相对于以前的VOC2012数据集的最佳结果，平均精度（mAP）提高了30％以上，达到了53.3％<strong>【<strong><strong>以前的方法都是传统算法</strong></strong>】</strong>。我们的方法结合了两个关键点：<strong>（1）将大容量（<strong><strong>深、复杂</strong></strong>）卷积神经网络（CNN）应用于自下而上的<strong><strong>候选区域</strong></strong>，以便定位和分割对象。（2）当标记的训练数据稀缺时，对辅助任务进行<strong><strong>预</strong></strong>训练，然后进行域特定的微调，可以显着提升性能</strong>【总结起来就是：首先将深度神经网络用于了目标检测和分割，其次是应用了迁移学习】<strong>。</strong>由于我们将候选区域与CNN相结合，所以我们称我们的方法为R-CNN：具有CNN特征的区域。我们还将R-CNN与OverFeat进行比较，OverFeat是最近提出的基于类似CNN架构的滑动窗口检测器。我们发现R-CNN在ILSVRC2013 200类的检测数据集上大幅超越OverFeat<strong>【RCNN<strong><strong>的第一版是在OverFeat之前发表的，本文其实是第五版V5，在OverFea</strong></strong>t<strong><strong>之后发表</strong></strong>】</strong>。系统的完整源代码在<a href="http://www.cs.berkeley.edu/rbg/rcnn%E3%80%82">http://www.cs.berkeley.edu/rbg/rcnn。</a></p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a><strong>1. Introduction</strong></h2><p>Features matter. The last decade of progress on various visual recognition tasks has been based considerably on the use of SIFT [29] and HOG [7]. But if we look at performance on the canonical visual recognition task, PASCAL VOC object detection [15], it is generally acknowledged that progress has been slow during 2010-2012, with small gains obtained by building ensemble systems and employing minor variants of successful methods.</p><p>特征很重要。过去的十年中，各种视觉识别任务取得了很大的进步，这取决于SIFT[29]和HOG[7]的使用<strong>【<strong><strong>建议读者对这两种手工提取的特征进行了解</strong></strong>】</strong>。但是,如果我们观察典型的视觉识别任务的性能，如PASCAL VOC对象检测[<a href="#fn:15">15]</a>，会发现2010-2012年进展缓慢，仅通过组合不同模型和使用已有方法的变体来获得很小的改进。</p><p>SIFT and HOG are blockwise orientation histograms, a representation we could associate roughly with complex cells in V1, the first cortical area in the primate visual pathway. But we also know that recognition occurs several stages downstream, which suggests that there might be hierarchical, multi-stage processes for computing features that are even more informative for visual recognition. </p><p>SIFT和HOG是块方向直方图，这是一种可以大致与V1中的复合细胞，灵长类动物视觉途径中的第一皮质区域相关联的表示。但是我们也知道识别发生在后续的几个阶段，这表明可能存在层次化，多阶段的计算特征的过程，这些过程对于视觉识别更为有用<strong>【<strong><strong>由此说明特征提取是一个逐步抽象的过程，需要层次化处理，这正是DCNN所擅长的任务</strong></strong>】</strong>。</p><p>Fukushima’s “neocognitron” [19], a biologically inspired hierarchical and shift-invariant model for pattern recognition, was an early attempt at just such a process. The neocognitron, however, lacked a supervised training algorithm. Building on Rumelhart et al. [33], LeCun et al. [26] showed that stochastic gradient descent via backpropagation was effective for training convolutional neural networks (CNNs), a class of models that extend the neocognitron. </p><p>Fukushima的“神经认知机”是一种受生物学启发的分层和偏移不变的模式识别模型，这只是一个早期的尝试。但是，神经认知机缺乏监督训练算法。卷积神经网络（CNN）是一类神经认知机的扩展模型，建立在Rumelhart和LeCun等提出的通过反向传播进行的随机梯度下降的基础之上。</p><p>CNNs saw heavy use in the 1990s (e.g., [27]), but then fell out of fashion with the rise of support vector machines. In 2012, Krizhevsky et al. [25] rekindled interest in CNNs by showing substantially higher image classification accuracy on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [9, 10]. Their success resulted from training a large CNN on 1.2 million labeled images, together with a few twists on LeCun’s CNN (e.g., max(x, 0) rectifying non-linearities and “dropout” regularization).</p><p>CNN在20世纪90年代有广泛的使用（例如2<a href="#fn:27">7</a>），但是随着支持向量机的兴起，CNN已经逐渐淡出了公众视野。 <strong>2012年，Krizhevsky等在ImageNet大型视觉识别挑战（ILSVRC）上显示出更高的图像分类准确度，重新唤起了人们对CNN的兴趣<strong><strong>【</strong></strong>Alex<strong><strong>Net</strong></strong>这个模型很重要，简单但很经典，建议细读****】</strong>。他们通过使用大型CNN训练120万张带标记图像而，以及对LeCun的CNN的一些改进（例如，max(x，0)修正线性单元和“Dropout”正规化）而成功。</p><p>The significance of the ImageNet result was vigorously debated during the ILSVRC 2012 workshop. The central issue can be distilled to the following: To what extent do the CNN classification results on ImageNet generalize to object detection results on the PASCAL VOC Challenge?</p><p>ImageNet结果的意义在ILSVRC 2012研讨会期间大有争议。中心问题可以归结为：CNN对ImageNet的分类结果在多大程度上可以泛化为PASCAL VOC目标检测挑战赛的结果？<strong>【<strong><strong>简单点说就是图像分类训练的模型对于目标检测类任务有没有帮助，因为图像分类任务的数据集十分丰富，而用于检测任务的数据集却相对较少</strong></strong>】</strong></p><p>We answer this question by bridging the gap between image classification and object detection. This paper is the first to show that a CNN can lead to dramatically higher object detection performance on PASCAL VOC as compared to systems based on simpler HOG-like features. To achieve this result, we focused on two problems: localizing objects with a deep network and training a high-capacity model with only a small quantity of annotated detection data.</p><p>我们通过弥合图像分类和对象检测之间的差距来回答这个问题。本文最先提出：与基于更简单的HOG类特征的系统相比，CNN可以显著提高PASCAL VOC的目标检测性能。为了实现这一结果，<strong>我们主要关注两个问题：使用深度网络定位目标，并只使用少量带标记的检测数据训练高容量模型。</strong></p><p>Unlike image classification, detection requires localizing (likely many) objects within an image. One approach frames localization as a regression problem. However, work from Szegedy et al. [38], concurrent with our own, indicates that this strategy may not fare well in practice (they report a map of 30.5% on VOC 2007 compared to the 58.5% achieved by our method). An alternative is to build a sliding-window detector. CNNs have been used in this way for at least two decades, typically on constrained object categories, such as faces [32, 40] and pedestrians [35]. In order to maintain high spatial resolution, these CNNs typically only have two convolutional and pooling layers. We also considered adopting a sliding-window approach. However, units high up in our network, which has five convolutional layers, have very large receptive fields (195 × 195 pixels) and strides (32×32 pixels) in the input image, which makes precise localization within the sliding-window paradigm an open technical challenge.</p><p>与图像分类不同，检测需要在图像中定位（可能是许多）目标。一种方法是将定位作为回归问题。然而，Szeged等人[38]与我们的工作表明这种策略在实践中可能不会很好（他们在VOC 2007年的map为30.5％，而我们的方法达到了58.5％）。另一种方法是构建一个滑动窗口检测器。 CNN已经以这种方式使用了至少二十年，通常是在限定的物体类别上，如面部和行人。为了保持高空间分辨率，这些CNN通常只有两个卷积和池化层。我们也考虑了采用滑动窗口方法。然而，在我们的网络中，具有五个卷积层的单元在输入图像中具有非常大的接收域（195×195像素）和步长（32×32像素）<strong>【<strong><strong>如何根据卷积步长的不同组合方式来计算感受野的大小很重要，建议熟知</strong></strong>】</strong>，这使得在滑动窗口内的精确定位成为公开的技术挑战。</p><p>Instead, we solve the CNN localization problem by operating within the “recognition using regions” paradigm [21], which has been successful for both object detection [39] and semantic segmentation [5]. At test time, our method generates around 2000 category-independent region proposals for the input image, extracts a fixed-length feature vector from each proposal using a CNN, and then classifies each region with category-specific linear SVMs. We use a simple technique (affine image warping) to compute a fixed-size CNN input from each region proposal, regardless of the region’s shape. Figure 1 presents an overview of our method and highlights some of our results. Since our system combines region proposals with CNNs, we dub the method R-CNN: Regions with CNN features.</p><p>相反，我们通过“基于区域提案的识别”范式来解决CNN的定位问题，这已经成功实现了目标检测和语义分割。<strong>在测试<strong><strong>阶段</strong></strong>，我们的方法为输入图像生成大约2000个类别无关的<strong><strong>候选</strong></strong>区域，使用CNN从每个<strong><strong>提案</strong></strong>中提取固定长度的特征向量，然后对每个区域进行类别特定的线性SVM分类。</strong>我们使用简单的技术（图像仿射变换，就是直接拉伸）来将每个区域提案拉伸到固定大小作为CNN的输入，而不管区域的形状。图1是我们方法的概述，彰显了我们的一些成果。由于我们的系统将区域提案与CNN相结合，所以我们将方法命名为R-CNN：具有CNN特征的区域。</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429215453586-1541060508.png"></p><p>Figure 1: Object detection system overview. Our system (1) takes an input image, (2) extracts around 2000 bottom-up region proposals, (3) computes features for each proposal using a large convolutional neural network (CNN), and then (4) classifies each region using class-specific linear SVMs. R-CNN achieves a mean average precision (map) of 53.7% on PASCAL VOC 2010. For comparison, [39] reports 35.1% map using the same region proposals, but with a spatial pyramid and bag-of-visual-words approach. The popular deformable part models perform at 33.4%. On the 200-class ILSVRC2013 detection dataset, R-CNN’s map is 31.4%, a large improvement over OverFeat [34], which had the previous best result at 24.3%.</p><p>图1：对象检测系统概述。我们的系统（1）输入一张图像，（2）提取约2000个自下而上的区域提案，（3）使用深度卷积神经网络（CNN）计算每个提案的特征，然后（4）使用类别特定的线性SVM对每个提案分类。 <strong>R-CNN在PASCAL VOC 2010中实现了53.7％的平均精度（map）。相较之下，</strong>**[39]<strong><strong>使用了相同的区域提案，但是使用了空间金字塔和</strong></strong>bag-of-visual-words<strong><strong>方法，达到了35.1％的map。主流的可变</strong></strong>形**<strong>部件模型为33.4％。在200类的ILSVRC2013检测数据集上，R-CNN的map为31.4％，超过OverFeat很多，OverFeat最佳结果为24.3％。）</strong></p><p>In this updated version of this paper, we provide a head-to-head comparison of R-CNN and the recently proposed OverFeat [34] detection system by running R-CNN on the 200-class ILSVRC2013 detection dataset. OverFeat uses a sliding-window CNN for detection and until now was the best performing method on ILSVRC2013 detection. We show that R-CNN significantly outperforms OverFeat, with a map of 31.4% versus 24.3%</p><p>在本文的更新版本中，我们让R-CNN和最近提出的OverFeat检测系统在200类的ILSVRC2013检测数据集上运行，提供详细的比较结果。OverFeat使用滑动窗口CNN进行检测，是目前在ILSVRC2013检测中性能最好的方法。我们的R-CNN明显优于OverFeat，map为31.4％，而OverFeat是24.3％。</p><p>A second challenge faced in detection is that labeled data is scarce and the amount currently available is insufficient for training a large CNN. The conventional solution to this problem is to use unsupervised pre-training, followed by supervised fine-tuning (e.g., [35]). The second principle contribution of this paper is to show thatsupervised pre-training on a large auxiliary dataset (ILSVRC), followed by domainspecific fine-tuning on a small dataset (PASCAL), is an effective paradigm for learning high-capacity CNNs when data is scarce. In our experiments, fine-tuning for detection improves map performance by 8 percentage points. After fine-tuning, our system achieves a map of 54% on VOC 2010 compared to 33% for the highly-tuned, HOG-based deformable part model (DPM) [17, 20]. We also point readers to contemporaneous work by Donahue et al. [12], who show that Krizhevsky’s CNN can be used (without finetuning) as a blackbox feature extractor, yielding excellent performance on several recognition tasks including scene classification, fine-grained sub-categorization, and domain adaptation.</p><p><strong>检测面临的第二个挑战是标记的数据很少<strong><strong>且</strong></strong>目前可用的数量不足以训练大型CNN</strong>。这个问题的常规解决方案是使用无监督的预训练，然后进行辅助微调（见<a href="#fn:35">35</a>）。本文的第二个主要贡献是在大型辅助数据集(ILSVRC)上进行监督预训练，然后对小数据集(PASCAL)进行域特定的微调，这是在数据稀缺时训练高容量CNN模型的有效范例。在我们的实验中，微调将检测的map性能提高了8个百分点。微调后，我们的系统在VOC 2010上实现了54％的map，而高度优化的基于HOG的可变形部件模型(DPM)为33％。同时，Donahue等人进行的工作[12]表明可以使用Krizhevsky的CNN（无需微调）作为黑盒特征提取器，在多个识别任务（包括场景分类，细粒度子分类和领域适应）中表现出色。</p><p>Our system is also quite efficient. The only class-specific computations are a reasonably small matrix-vector product and greedy non-maximum suppression. This computational property follows from features that are shared across all categories and that are also two orders of magnitude lower dimensional than previously used region features (cf. [39]).</p><p>我们的系统也很有效率。唯一的类特定计算是相当小的矩阵向量乘积和基于贪心规则的非极大值抑制<strong>【<strong><strong>在进行SVM分类的时候需要用到矩阵乘法，每个类别单独计算，不能共享，后面的NMS也是不能共享计算</strong></strong>】</strong>。这种计算属性来自于跨所有样本类别共享的特征，并且比以前使用的区域特征维度还低两个数量级(这个怎么理解呢？)（参见39）。</p><p>Understanding the failure modes of our approach is also critical for improving it, and so we report results from the detection analysis tool of Hoiem et al. [23]. As an immediate consequence of this analysis, we demonstrate that a simple bounding-box regression method significantly reduces mislocalizations, which are the dominant error mode.</p><p>分析我们方法的不足之处对于改进模型也是至关重要的，因此我们给出了由Hoiem等人提出的检测分析工具的结果。<strong>通过****这一分析，我们证明了一种简单的边界回归方法显著地减少了定位误差，这是主要的误差模式</strong>。</p><p>Before developing technical details, we note that because R-CNN operates on regions it is natural to extend it to the task of semantic segmentation. With minor modifications, we also achieve competitive results on the PASCAL VOC segmentation task, with an average segmentation accuracy of 47.9% on the VOC 2011 test set.</p><p>在发掘技术细节之前，我们注意到：由于R-CNN在区域上运行，将其扩展到语义分割的任务就很自然。经过少量的修改，我们也在PASCAL VOC分割任务中取得了有竞争力的成果，VOC 2011测试集的平均分割精度为47.9％。</p><h2 id="2-Object-detection-with-R-CNN"><a href="#2-Object-detection-with-R-CNN" class="headerlink" title="2. Object detection with R-CNN"></a><strong>2. Object detection with R-CNN</strong></h2><p>Our object detection system consists of three modules. The first generates category-independent region proposals. These proposals define the set of candidate detections available to our detector. The second module is a large convolutional neural network that extracts a fixed-length feature vector from each region. The third module is a set of classspecific linear SVMs. In this section, we present our design decisions for each module, describe their test-time usage, detail how their parameters are learned, and show detection results on PASCAL VOC 2010-12 and on ILSVRC2013. </p><p>我们的目标检测系统由三个模块组成。<strong>第一个生成类别无关区域提案。这些提案定义了可用于我们的检测器的候选检测集。第二个模块是从每个区域提取固定长度特征向量的大型卷积神经网络。第三个模块是一组特定类别的线性SVM。</strong>在本节中，我们介绍每个模块的设计思路，描述其测试时使用情况，详细介绍其参数的学习方式，并给出在PASCAL VOC 2010-12和ILSVRC2013上的检测结果。</p><h3 id="2-1-Module-design"><a href="#2-1-Module-design" class="headerlink" title="2.1. Module design"></a><strong>2.1. Module design</strong></h3><p><strong>Region proposals.</strong> A variety of recent papers offer methods for generating category-independent region proposals. Examples include: objectness [1], selective search [39], category-independent object proposals [14], constrained parametric min-cuts (CPMC) [5], multi-scale combinatorial grouping [3], and Ciresan et al. [6], who detect mitotic cells by applying a CNN to regularly-spaced square crops, which are a special case of region proposals. While R-CNN is agnostic to the particular region proposal method, we use selective search to enable a controlled comparison with prior detection work (e.g., [39, 41]).</p><p><em>*<em>*区域提案。*<em>*<em>最近的各种论文提供了生成类别无关区域提案的方法。例子包括：objectness [1]，选择性搜索[39]，类别无关对象提议[14]，约束参数最小化(CPMC)[5]<strong>【</strong></em>*这些区域提案算法基本上是基于图像的纹理、轮廓、色彩等特征的</em></em>*</em>】**，多尺度组合分组[3]和Ciresan等提出的[6]通过将CNN应用于特定间隔的方块来检测有丝分裂细胞，这是区域提案的特殊情况。具体的区域提案方法对于R-CNN是透明的，但我们使用选择性搜索以便于与先前检测工作的对照比较（例如39,41）。</p><p><strong>Feature extraction</strong>. We extract a 4096-dimensional feature vector from each region proposal using the Caffe [24] implementation of the CNN described by Krizhevsky et al. [25]. Features are computed by forward propagating a mean-subtracted 227 × 227 RGB image through five convolutional layers and two fully connected layers. We refer readers to [24, 25] for more network architecture details. </p><p><em><strong>*特征提取。*<em><strong>我们使用Krizhevsky等人提出的CNN的Caffe[24]实现，从每个区域提案中提取4096维特征向量。将227×227分辨率的RGB图像减去像素平均值后通过五个卷积层和两个全连接层向前传播来计算特征。可以参考24，25以获得更多的网络架构细节</strong>【</em></strong></em>建议参考文章实现一下**<strong>】</strong>。</p><p>In order to compute features for a region proposal, we must first convert the image data in that region into a form that is compatible with the CNN (its architecture requires inputs of a fixed 227 × 227 pixel size). Of the many possible transformations of our arbitrary-shaped regions, we opt for the simplest. Regardless of the size or aspect ratio of the candidate region, we warp all pixels in a tight bounding box around it to the required size. Prior to warping, we dilate the tight bounding box so that at the warped size there are exactly p pixels of warped image context around the original box (we use p = 16). Figure 2 shows a random sampling of warped training regions. Alternatives to warping are discussed in Appendix A.</p><p>为了计算区域提案的特征，我们必须首先将该区域中的图像数据转换为与CNN兼容的格式（其架构需要固定227×227像素大小的输入）。在许多可能的针对任意形状区域的变换中，我们选择最简单的。不管候选区域的大小或横纵比如何，我们将整个区域不保持横纵比缩放到所需的大小。在缩放之前，我们扩大了被缩放的区域，使得在缩放后，原始区域边界到现有区域边界宽度为p像素（我们使用p = 16）。 图2显示了变形训练区域的随机抽样。 变形的替代方案在附录A中讨论<strong>【<strong><strong>就是描述了如何将候选区域拉升到227</strong></strong>X227<strong><strong>的大小作为网络的输入以便提取CNN的4096维特征，具体描述可以参考博客图文理解</strong></strong>】</strong>。</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429215432858-1779548973.png"></p><h3 id="2-2-Test-time-detection"><a href="#2-2-Test-time-detection" class="headerlink" title="2.2. Test-time detection"></a><strong>2.2. Test-time detection</strong></h3><p>At test time, we run selective search on the test image to extract around 2000 region proposals (we use selective search’s “fast mode” in all experiments). We warp each proposal and forward propagate it through the CNN in order to compute features. Then, for each class, we score each extracted feature vector using the SVM trained for that class. Given all scored regions in an image, we apply a greedy non-maximum suppression (for each class independently) that rejects a region if it has an intersection over union (IoU) overlap with a higher scoring selected region larger than a learned threshold. </p><p>在测试时，我们对测试图像进行选择性搜索，以提取大约2000个区域提案（我们在所有实验中使用选择性搜索的“快速模式”）。然后缩放每个区域，并通过CNN向前传播，以计算特征。最后，对于每个类，我们使用针对该类训练的SVM来对每个提取的特征向量进行评分。给定图像中的所有区域的得分后，我们应用贪婪非极大值抑制（每个类别独立进行），在训练时学习一个阈值，如果其与得分较高的区域的重叠部分(IoU)高于这个阈值，则丢弃这个区域。</p><p><strong>Run-time analysis</strong>. Two properties make detection efficient. First, all CNN parameters are shared across all categories. Second, the feature vectors computed by the CNN are low-dimensional when compared to other common approaches, such as spatial pyramids with bag-of-visual-word encodings. The features used in the UVA detection system [39], for example, are two orders of magnitude larger than ours (360k vs. 4k-dimensional). </p><p><em><strong>*性能分析。*<em><strong>两种性质使检测效率高。首先，所有CNN参数都在所有类别中共享</strong>【</em></strong></em>作者这意思是说所有的候选区域都是通过同一个CNN模型提取特征**<strong>】</strong>。其次，与其他常见方法比较，由CNN计算出的特征向量是低维度的，例如具有空间金字塔和bag-of-visual-word的方法。UVA检测系统<a href="#fn:39">39</a>中使用的特征比我们（维度，360k对比4k）大两个数量级。</p><p>The result of such sharing is that the time spent computing region proposals and features (13s/image on a GPU or 53s/image on a CPU) is amortized over all classes. The only class-specific computations are dot products between features and SVM weights and non-maximum suppression. In practice, all dot products for an image are batched into a single matrix-matrix product. The feature matrix is typically 2000×4096 and the SVM weight matrix is 4096×N, where N is the number of classes.</p><p>这种共享的结果是计算区域建议和特征（GPU上的13秒/图像或CPU上的53秒/图像）的时间在所有类别上进行摊销。唯一的类特定计算是特征与SVM权重的点积和非极大值抑制。在实践中，图像的所有点积运算都被整合为单个矩阵与矩阵的相乘。特征矩阵通常为2000×4096，SVM权重矩阵为4096×N，其中N为类别数。</p><p>This analysis shows that R-CNN can scale to thousands of object classes without resorting to approximate techniques, such as hashing. Even if there were 100k classes, the resulting matrix multiplication takes only 10 seconds on a modern multi-core CPU. This efficiency is not merely the result of using region proposals and shared features. The UVA system, due to its high-dimensional features, would be two orders of magnitude slower while requiring 134GB of memory just to store 100k linear predictors, compared to just 1.5GB for our lower-dimensional features. </p><p>如上的分析表明，R-CNN可以扩展到数千个类，而不需要使用如散列这样的技术。即使有10万个类，在现代多核CPU上产生的矩阵乘法只需10秒。这种效率不仅仅是使用区域提案和共享特征的结果。由于其高维度特征，UVA系统的速度将会降低两个数量级，并且需要134GB的内存来存储10万个线性预测器。而对于低维度特征而言，仅需要1.5GB内存<strong>【<strong><strong>这段内容分析好牵强</strong></strong>】</strong>。</p><p>It is also interesting to contrast R-CNN with the recent work from Dean et al. on scalable detection using DPMs and hashing [8]. They report a map of around 16% on VOC 2007 at a run-time of 5 minutes per image when introducing 10k distractor classes. With our approach, 10k detectors can run in about a minute on a CPU, and because no approximations are made map would remain at 59% (Section 3.2).</p><p>将R-CNN与Dean等人最近的工作对比也是有趣的。使用DPM和散列的可扩展检测。在引入1万个干扰类的情况下，每个图像的运行时间为5分钟，其在VOC 2007上的map约为16％。通过我们的方法，1万个检测器可以在CPU上运行大约一分钟，而且由于没有近似值，可以使map保持在59％（见<a href="#%E6%B6%88%E8%9E%8D%E7%A0%94%E7%A9%B6">消融研究</a>）。</p><h3 id="2-3-Training"><a href="#2-3-Training" class="headerlink" title="2.3. Training"></a><strong>2.3. Training</strong></h3><p><strong>Supervised pre-training.</strong> We discriminatively pre-trained the CNN on a large auxiliary dataset (ILSVRC2012 classification) using image-level annotations only (bounding box labels are not available for this data). Pre-training was performed using the open source Caffe CNN library [24]. In brief, our CNN nearly matches the performance of Krizhevsky et al. [25], obtaining a top-1 error rate 2.2 percentage points higher on the ILSVRC2012 classification validation set. This discrepancy is due to simplifications in the training process. </p><p><em><strong>*监督预训练。*<em><strong>我们仅通过使用图像级标记（此数据没有检测框标记）的大型辅助数据集（ILSVRC2012分类数据集）来区分性地对CNN进行预训练。使用开源的Caffe CNN库进行预训练。简而言之，我们的CNN几乎符合Krizhevsky等人的论文中的表现，ILSVRC2012分类验证集获得的top-1错误率高出2.2个百分点。这种差异是由于训练过程中的简化造成的</strong>【</em></strong></em>首先大致复现了AlexNe<strong><strong>t</strong></strong>分类网络的性能**<strong>】。</strong></p><p><strong>Domain-specific fine-tuning</strong>. To adapt our CNN to the new task (detection) and the new domain (warped proposal windows), we continue stochastic gradient descent (SGD) training of the CNN parameters using only warped region proposals. Aside from replacing the CNN’s ImageNet specific 1000-way classification layer with a randomly initialized (N + 1)-way classification layer (where N is the number of object classes, plus 1 for background), the CNN architecture is unchanged. For VOC, N = 20 and for ILSVRC2013, N = 200. We treat all region proposals with ≥ 0.5 IoU overlap with a ground-truth box as positives for that box’s class and the rest as negatives. We start SGD at a learning rate of 0.001 (1/10th of the initial pre-training rate), which allows fine-tuning to make progress while not clobbering the initialization. In each SGD iteration, we uniformly sample 32 positive windows (over all classes) and 96 background windows to construct a mini-batch of size 128. We bias the sampling towards positive windows because they are extremely rare compared to background. </p><p><em><strong>*域特定的微调。*<em><strong>为了使CNN适应新任务（检测）和新域（缩放的提案窗口），我们仅使用缩放后的区域提案继续进行CNN参数的随机梯度下降(SGD)训练。除了用随机初始化的(N + 1)路分类层（其中N是类别数，加1为背景）替换CNN的ImageNet特有的1000路分类层，CNN架构不变</strong>【</em></strong></em>简单介绍了一下如何更改网络**<strong>】</strong>。对于VOC，N = 20，对于ILSVRC2013，N = 200。我们将所有区域提案与检测框真值IoU ≥0.5的区域作为正样本，其余的作为负样本。我们以0.001（初始学习率的1/10）的学习率开始SGD，这样可以在不破坏初始化的情况下进行微调。在每个SGD迭代中，我们从所有类别中统一采样32个正样本和96个负样本，以构建大小为128的小批量。采样的正样本较少是因为它们与背景相比非常罕见。</p><p><strong>Object category classifiers</strong>. Consider training a binary classifier to detect cars. It’s clear that an image region tightly enclosing a car should be a positive example. Similarly, it’s clear that a background region, which has nothing to do with cars, should be a negative example. Less clear is how to label a region that partially overlaps a car. We resolve this issue with an IoU overlap threshold, below which regions are defined as negatives. The overlap threshold, 0.3, was selected by a grid search over {0, 0.1, . . . , 0.5} on a validation set. We found that selecting this threshold carefully is important. Setting it to 0.5, as in [39], decreased map by 5 points. Similarly, setting it to 0 decreased map by 4 points. Positive examples are defined simply to be the ground-truth bounding boxes for each class.</p><p>****目标类别分类器。****考虑训练二分类器来检测汽车。很明显，紧紧围绕汽车的图像区域应该是一个正样例，一个与汽车无关的背景区域应该是一个负样本。较不清楚的是如何标注部分重叠汽车的区域。我们用IoU重叠阈值来解决这个问题，在这个阈值以下的区域被定义为负样本。重叠阈值0.3是通过在验证集上尝试了0,0.1,…,0.50,0.1,…,0.5的不同阈值选择出来的。我们发现选择这个阈值是很重要的。将其设置为0.5，如3<a href="#fn:9">9</a>，map会降低5个点。同样，将其设置为0会将map降低4个点。正样本被简单地定义为每个类的检测框真值。</p><p>Once features are extracted and training labels are applied, we optimize one linear SVM per class. Since the training data is too large to fit in memory, we adopt the standard hard negative mining method [17, 37]. Hard negative mining converges quickly and in practice map stops increasing after only a single pass over all images.</p><p>一旦提取了特征并有了训练标签，我们就可以优化每类线性SVM。由于训练数据太大内存不够，我们采用标准的难分样本挖掘方法[17,37]。难分样本挖掘可以快速收敛，实际上所有图像遍历一遍，map就停止增长了。</p><p>In Appendix B we discuss why the positive and negative examples are defined differently in fine-tuning versus SVM training. We also discuss the trade-offs involved in training detection SVMs rather than simply using the outputs from the final softmax layer of the fine-tuned CNN.</p><p>在<a href="#B.%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E5%92%8CSoftmax">附录B</a>中，我们将讨论为什么在微调与SVM训练中，正样本和负样本的数量不同。我们还讨论了涉及训练检测SVM的权衡，而不是简单地使用微调CNN的最终softmax层的输出。</p><h3 id="2-4-Results-on-PASCAL-VOC-2010-12"><a href="#2-4-Results-on-PASCAL-VOC-2010-12" class="headerlink" title="2.4. Results on PASCAL VOC 2010-12"></a><strong>2.4. Results on PASCAL VOC 2010-12</strong></h3><p>Following the PASCAL VOC best practices [15], we validated all design decisions and hyperparameters on the VOC 2007 dataset (Section 3.2). For final results on the VOC 2010-12 datasets, we fine-tuned the CNN on VOC 2012 train and optimized our detection SVMs on VOC 2012 trainval. We submitted test results to the evaluation server only once for each of the two major algorithm variants (with and without bounding-box regression). </p><p>根据PASCAL VOC最佳实践<a href="#fn:15">15</a>，我们在VOC 2007数据集上验证了所有设计和超参数（见<a href="#%E6%B6%88%E8%9E%8D%E7%A0%94%E7%A9%B6">消融研究</a>）。对于VOC 2010-12数据集的最终结果，我们对VOC 2012 train上对CNN进行了微调，并在VOC 2012 trainval上优化检测SVM。我们将测试结果提交给评估服务器，对于两种主要算法变体（带有和不带有检测框回归）的每一种，都只提交一次。</p><p>Table 1 shows complete results on VOC 2010. We compare our method against four strong baselines, including SegDPM [18], which combines DPM detectors with the output of a semantic segmentation system [4] and uses additional inter-detector context and image-classifier rescoring. The most germane comparison is to the UVA system from Uijlings et al. [39], since our systems use the same region proposal algorithm. To classify regions, their method builds a four-level spatial pyramid and populates it with densely sampled SIFT, Extended Opponent SIFT, and RGBSIFT descriptors, each vector quantized with 4000-word codebooks. Classification is performed with a histogram intersection kernel SVM. Compared to their multi-feature, non-linear kernel SVM approach, we achieve a large improvement in map, from 35.1% to 53.7% map, while also being much faster (Section 2.2). Our method achieves similar performance (53.3% map) on VOC 2011/12 test.</p><p>表1显示了VOC2010的完整结果。我们与其它四种很优秀的方法进行了比较，包括SegDPM，它将DPM检测器与语义分割系统的输出相结合，并使用了像语境和图像类别重排的内部分类器。最具可比性的是Uijlings等人的UVA系统，因为我们的系统使用相同的区域提案算法。为了对区域进行分类，他们的方法构建了一个四级空间金字塔，并用密集采样的SIFT(（扩展对准SIFT和RGB-SIFT描述符，每个矢量用4000字的码本量化），使用直方图交叉核心SVM进行分类。与其多特征非线性内核SVM方法相比，我们将map从35.1％提高到了53.7％，同时也快得多（见<a href="#%E6%B5%8B%E8%AF%95">测试</a>）。我们的方法在VOC 2011/12测试中实现了接近的性能（53.3％的map）。</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429215254546-1338270591.png"></p><p>Table 1: Detection average precision (%) on VOC 2010 test. R-CNN is most directly comparable to UVA and Regionlets since all methods use selective search region proposals. Bounding-box regression (BB) is described in Section C. At publication time, SegDPM was the top-performer on the PASCAL VOC leaderboard. †DPM and SegDPM use context rescoring not used by the other methods.</p><p>表1：（VOC 2010测试的平均检测精度（％）。 R-CNN与UVA和Regionlets最相似，因为所有方法都使用选择性搜索区域提案。检测框回归（BB）在<a href="#C.%E6%A3%80%E6%B5%8B%E6%A1%86%E5%9B%9E%E5%BD%92">附录C</a>中描述。在本文发布时，SegDPM是PASCAL VOC排行榜中表现最好的方法。 †DPM和SegDPM使用上下文重排，其他方法未使用）</p><h3 id="2-5-Results-on-ILSVRC2013-detection"><a href="#2-5-Results-on-ILSVRC2013-detection" class="headerlink" title="2.5. Results on ILSVRC2013 detection"></a><strong>2.5. Results on ILSVRC2013 detection</strong></h3><p>We ran R-CNN on the 200-class ILSVRC2013 detection dataset using the same system hyperparameters that we used for PASCAL VOC. We followed the same protocol of submitting test results to the ILSVRC2013 evaluation server only twice, once with and once without bounding-box regression.</p><p>我们使用与PASCAL VOC相同的系统超参数，在200类的ILSVRC2013检测数据集上运行R-CNN。我们遵循相同的原则，仅提交测试结果给ILSVRC2013评估服务器两次，一次有边界框回归，一次没有。</p><p>Figure 3 compares R-CNN to the entries in the ILSVRC 2013 competition and to the post-competition OverFeat result [34]. R-CNN achieves a map of 31.4%, which is significantly ahead of the second-best result of 24.3% from OverFeat. To give a sense of the AP distribution over classes, box plots are also presented and a table of perclass APs follows at the end of the paper in Table 8. Most of the competing submissions (OverFeat, NEC-MU, UvAEuvision, Toronto A, and UIUC-IFP) used convolutional neural networks, indicating that there is significant nuance in how CNNs can be applied to object detection, leading to greatly varying outcomes. </p><p>图3比较了R-CNN与ILSVRC 2013竞赛中的参赛作品以及赛后OverFeat得结果[34]。 R-CNN的map达到31.4％，远远高于OverFeat的24.3％的第二好成绩。 为了了解类别上的AP分布情况，还提供了箱形图，并在表8的结尾处列出了每类AP的表格。提交的大多数竞争性结果（OverFeat，NEC-MU，UvAEuvision，Toronto A， 和UIUC-IFP）都使用卷积神经网络，表明CNN如何应用于物体检测存在显着的细微差别，导致结果差异很大。</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429215220158-1842995787.png"></p><p>Figure 3: (Left) Mean average precision on the ILSVRC2013 detection test set. Methods preceeded by * use outside training data (images and labels from the ILSVRC classification dataset in all cases). (Right) Box plots for the 200 average precision values per method. A box plot for the post-competition OverFeat result is not shown because per-class APs are not yet available (per-class APs for R-CNN are in Table 8 and also included in the tech report source uploaded to arXiv.org; see R-CNN-ILSVRC2013-APs.txt). The red line marks the median AP, the box bottom and top are the 25th and 75th percentiles. The whiskers extend to the min and max AP of each method. Each AP is plotted as a green dot over the whiskers (best viewed digitally with zoom).</p><p>图3：（左）ILSVRC2013检测测试集的平均精度。 方法之前有符号*表示使用外部训练数据（都使用ILSVRC分类数据集的图像和标签）。 （右）200个类map的箱形图方法。 没有显示赛后OverFeat结果的方框图，因为每类AP尚不可用（R-CNN每类的AP为见表8，也包含在上传到arXiv.org的技术报告中; 见R-CNN-ILSVRC2013-APs.txt）。 这红色线标记中位数AP，方框底部和顶部是第25和第75百分位数。 虚线延伸到每个的最小和最大AP方法。 每个AP在虚线上绘制为绿点（最好以数字方式使用缩放查看）。</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429215207424-608205459.png"></p><p>Table 8: Per-class average precision (%) on the ILSVRC2013 detection test set.</p><p>In Section 4, we give an overview of the ILSVRC2013 detection dataset and provide details about choices that we made when running R-CNN on it.</p><p>在第4节中，我们概述了ILSVRC2013检测数据集，并提供了有关在其上运行R-CNN时所做选择的详细信息。</p><h2 id="3-Visualization-ablation-and-modes-of-error"><a href="#3-Visualization-ablation-and-modes-of-error" class="headerlink" title="3. Visualization, ablation, and modes of error"></a><strong>3. Visualization, ablation, and modes of error</strong></h2><h3 id="3-1-Visualizing-learned-features"><a href="#3-1-Visualizing-learned-features" class="headerlink" title="3.1. Visualizing learned features"></a><strong>3.1. Visualizing learned features</strong></h3><p>First-layer filters can be visualized directly and are easy to understand [25]. They capture oriented edges and opponent colors. Understanding the subsequent layers is more challenging. Zeiler and Fergus present a visually attractive deconvolutional approach in [42]. We propose a simple (and complementary) non-parametric method that directly shows what the network learned. </p><p>第一层卷积核可以直观可视化，易于理解。它们捕获定向边缘和相对颜色。了解后续层次更具挑战性。 Zeiler和Fergus在提出了一种有视觉吸引力的反卷积方法。我们提出一个简单（和补充）非参数方法，直接显示网络学到的内容。</p><p>The idea is to single out a particular unit (feature) in the network and use it as if it were an object detector in its own right. That is, we compute the unit’s activations on a large set of held-out region proposals (about 10 million), sort the proposals from highest to lowest activation, perform nonmaximum suppression, and then display the top-scoring regions. Our method lets the selected unit “speak for itself” by showing exactly which inputs it fires on. We avoid averaging in order to see different visual modes and gain insight into the invariances computed by the unit.</p><p>这个想法是在网络中列出一个特定的单元（特征），并将其用作它自己的目标检测器。也就是说，我们在大量的区域提案（约1000万<strong>【<strong><strong>每张图像约2000个候选区域</strong></strong>】</strong>）中计算这个单元的激活，将提案按激活从大到小排序，然后执行非极大值抑制，然后显示激活最大的提案。通过准确显示它激活了哪些输入，我们的方法让所选单元“自己说话”。我们避免平均，以看到不同的视觉模式，并深入了解这个单元计算的不变性。</p><p>We visualize units from layer pool5 , which is the max pooled output of the network’s fifth and final convolutional layer. The pool5 feature map is 6 × 6 × 256 = 9216- dimensional. Ignoring boundary effects, each pool5 unit has a receptive field of 195×195 pixels in the original 227×227 pixel input. A central pool5 unit has a nearly global view, while one near the edge has a smaller, clipped support.</p><p>我们可以看到来自pool5的单元，这是网络第五，也是最终卷积层的最大池化输出。pool5的特征图维度是6×6×256=9216。忽略边界效应，每个pool5单元在原始227×227像素输入中具有195×195像素的感受野。位于中央的pool5单元具有几乎全局的视野，而靠近边缘的则有一个较小的裁剪的视野。</p><p>Each row in Figure 4 displays the top 16 activations for a pool5 unit from a CNN that we fine-tuned on VOC 2007 trainval. Six of the 256 functionally unique units are visualized (Appendix D includes more). These units were selected to show a representative sample of what the network learns. In the second row, we see a unit that fires on dog faces and dot arrays. The unit corresponding to the third row is a red blob detector. There are also detectors for human faces and more abstract patterns such as text and triangular structures with windows. The network appears to learn a representation that combines a small number of class-tuned features together with a distributed representation of shape, texture, color, and material properties. The subsequent fully connected layer fc6 has the ability to model a large set of compositions of these rich features.</p><p>图4中的每一行都显示了在VOC 2007 trainval上进行微调的CNN中的pool5单元的前16个最大激活的区域<strong>【<strong><strong>也就是置信度最高得几个？</strong></strong>】</strong>，包括256个功能独特的单元中的6个（更多参见<a href="#D.%E9%A2%9D%E5%A4%96%E7%9A%84%E7%89%B9%E5%BE%81%E5%8F%AF%E8%A7%86%E5%8C%96">附录D</a>）。选择这些单位以显示网络学习的有代表性的样本。在第二行，我们看到一个在狗脸和点阵列上触发的单元。与第三行对应的单元是红色斑点检测器。还有用于人脸和更抽象图案的检测器，例如文本和具有窗口的三角形结构。网络似乎学习了一种将少量类别调谐特征与形状，纹理，颜色和材质属性的分布式表示相结合的表示。随后的全连接层fc6f具有对这些丰富特征的大量组合进行建模的能力</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429215136562-1958201579.png"></p><p>Figure 4: Top regions for six pool5 units. Receptive fields and activation values are drawn in white. Some units are aligned to concepts, such as people (row 1) or text (4). Other units capture texture and material properties, such as dot arrays (2) and specular reflections (6).</p><p>图4：六个pool5单元的激活最大的区域。感受野和激活值以白色绘制。某些单元与概念对齐，例如人（第1行）或文本（第4行）。其它单元捕获纹理和材料属性，如点阵列（第2行）和镜面反射（第6行）。）</p><h3 id="3-2-Ablation-studies"><a href="#3-2-Ablation-studies" class="headerlink" title="3.2. Ablation studies"></a><strong>3.2. Ablation studies</strong></h3><p><strong>Performance layer-by-layer, without fine-tuning.</strong> To understand which layers are critical for detection performance, we analyzed results on the VOC 2007 dataset for each of the CNN’s last three layers. Layer pool5 was briefly described in Section 3.1. The final two layers are summarized below.</p><p><em><strong>*逐层分析性能，没有微调。*<em><strong>为了了解哪些层对于检测性能至关重要，我们分析了CNN最后三层在VOC 2007数据集上的结果。<a href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%88%B0%E7%9A%84%E7%89%B9%E5%BE%81">上一节</a>简要描述了pool5。最后两层总结如下</strong>【</em></strong></em>就是将骨干CNN不同层次提取得特征放到SVM中进行分类，看哪个层特征得效果好**<strong>】</strong>。</p><p>Layer fc6 is fully connected to pool5 . To compute features, it multiplies a 4096×9216 weight matrix by the pool5 feature map (reshaped as a 9216-dimensional vector) and then adds a vector of biases. This intermediate vector is component-wise half-wave rectified (x ← max(0, x)). </p><p>层fc6完全连接到pool5。为了计算特征，它将pool5的特征图乘以一个4096×9216的权重矩阵（重构为9216维向量），然后加上一个偏置向量。最后应用ReLU线性纠正。</p><p>Layer fc7 is the final layer of the network. It is implemented by multiplying the features computed by fc6 by a 4096 × 4096 weight matrix, and similarly adding a vector of biases and applying half-wave rectification. </p><p>层fc7是网络的最后一层。这是通过将由fc6计算的特征乘以4096×4096权重矩阵来实现的，并且类似地加上了偏置向量并应用ReLU线性纠正。</p><p>We start by looking at results from the CNN without fine-tuning on PASCAL, i.e. all CNN parameters were pre-trained on ILSVRC 2012 only. Analyzing performance layer-by-layer (Table 2 rows 1-3) reveals that features from fc7 generalize worse than features from fc6. This means that 29%, or about 16.8 million, of the CNN’s parameters can be removed without degrading map. More surprising is that removing both fc7 and fc6 produces quite good results even though pool5 features are computed using only 6% of the CNN’s parameters. Much of the CNN’s representational power comes from its convolutional layers, rather than from the much larger densely connected layers. This finding suggests potential utility in computing a dense feature map, in the sense of HOG, of an arbitrary-sized image by using only the convolutional layers of the CNN. This representation would enable experimentation with sliding-window detectors, including DPM, on top of pool5 features.</p><p>我们首先来看看没有在PASCAL上进行微调的CNN的结果，即所有的CNN参数仅在ILSVRC 2012上进行了预训练。逐层分析性能（如上表，表2第1-3行）显示，fc7的特征总体上差于fc6的特征。这意味着可以删除CNN参数的29％或约1680万<strong>【4096X4096=16777216】</strong>，而不会降低map。更令人惊讶的是，即使使用仅6％的CNN参数来计算pool5特征，除去fc7和fc6也产生相当好的结果。 <strong>CNN的大部分表达能力来自其卷积层，而不是来自于更密集的全连接层。</strong>这一发现表明通过仅使用CNN的卷积层来计算任意大小图像的类似HOG意义上的密集特征图的潜在实用性。这种表示方式可以在pool5特征之上实现包括DPM在内的滑动窗口检测器。</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429215104701-1548122419.png"></p><p>Table 2: Detection average precision (%) on VOC 2007 test. Rows 1-3 show R-CNN performance without fine-tuning. Rows 4-6 show results for the CNN pre-trained on ILSVRC 2012 and then fine-tuned (FT) on VOC 2007 trainval. Row 7 includes a simple bounding-box regression (BB) stage that reduces localization errors (Section C). Rows 8-10 present DPM methods as a strong baseline. The first uses only HOG, while the next two use different feature learning approaches to augment or replace HOG.</p><p>表2：VOC 2007测试集的检测平均精度（％）。 第1-3行显示R-CNN没微调的性能。 第4-6行显示CNN对ILSVRC 2012进行了预训练，然后对VOC 2007 trainval进行了微调（FT）的性能。 第7行包括一个简单的边界框回归（BB）阶段，减少定位错误（C部分）。 第8-10行将DPM方法作为强基线。 第一次使用只有HOG，而接下来的两个使用不同的特征学习方法来增强或替换HOG。</p><p><strong>Performance layer-by-layer,</strong> <strong>with fine-tuning</strong>. We now look at results from our CNN after having fine-tuned its parameters on VOC 2007 trainval. The improvement is striking (Table 2 rows 4-6): fine-tuning increases map by 8.0 percentage points to 54.2%. The boost from fine-tuning is much larger for fc6 and fc7 than for pool5 , which suggests that the pool5 features learned from ImageNet are general and that most of the improvement is gained from learning domain-specific non-linear classifiers on top of them.</p><p>****逐层分析性能，微调。*<em><strong>现在我们来看看在PASCAL上进行微调的CNN的结果。改善情况引人注目（表2第4-6行）：微调使map提高8.0个百分点至54.2％。</strong>对于fc6和fc7，微调的提升比对pool5大得多，这表明从ImageNet中学习的pool 5特性是一般性的，并且大多数改进是从学习域特定的非线性分类器获得的。</em>*</p><p><strong>Comparison to recent feature learning methods.</strong> Relatively few feature learning methods have been tried on PASCAL VOC detection. We look at two recent approaches that build on deformable part models. For reference, we also include results for the standard HOG-based DPM [20]. </p><p>****与近期特征学习方法的比较。****近期在PAS-CAL VOC检测中已经开始尝试了一些特征学习方法。我们来看两种最新的基于DPM模型的方法。作为参考，我们还包括基于标准HOG的DPM的结果<a href="#fn:20">20</a>。</p><p>The first DPM feature learning method, DPM ST [28], augments HOG features with histograms of “sketch token” probabilities. Intuitively, a sketch token is a tight distribution of contours passing through the center of an image patch. Sketch token probabilities are computed at each pixel by a random forest that was trained to classify 35×35 pixel patches into one of 150 sketch tokens or background. </p><p>第一个DPM特征学习方法，DPM ST[<a href="#fn:28">28]</a>，使用“草图表征”概率直方图增强了HOG特征。直观地，草图表征是通过图像片中心的轮廓的紧密分布。草图表征概率在每个像素处被随机森林计算，该森林经过训练，将35 x 35像素的图像片分类为150个草图表征或背景之一。</p><p>The second method, DPM HSC [31], replaces HOG with histograms of sparse codes (HSC). To compute an HSC, sparse code activations are solved for at each pixel using a learned dictionary of 100 7 × 7 pixel (grayscale) atoms. The resulting activations are rectified in three ways (full and both half-waves), spatially pooled, unit `2 normalized, and then power transformed (x ← sign(x)|x| α).</p><p>第二种方法，DPM HSC[31]，使用稀疏码直方图(HSC)替代HOG。为了计算HSC，使用100个7 x 7像素（灰度）元素的学习词典，在每个像素处求解稀疏代码激活。所得到的激活以三种方式整流（全部和两个半波），空间合并，单位L2归一化，和功率变换(x←sign(x)|x|α)(x←sign(x)|x|α)。</p><p>All R-CNN variants strongly outperform the three DPM baselines (Table 2 rows 8-10), including the two that use feature learning. Compared to the latest version of DPM, which uses only HOG features, our map is more than 20 percentage points higher: 54.2% vs. 33.7%—a 61% relative improvement. The combination of HOG and sketch tokens yields 2.5 map points over HOG alone, while HSC improves over HOG by 4 map points (when compared internally to their private DPM baselines—both use nonpublic implementations of DPM that underperform the open source version [20]). These methods achieve mAPs of 29.1% and 34.3%, respectively.</p><p>所有R-CNN的变体都优于三个DPM基线（表2第8-10行），包括两个使用特征学习的。与仅使用HOG特征的最新版本的DPM相比，我们的map提高了20个百分点以上：54.2％对比33.7％，相对改进61％。HOG和草图表征的组合与单独的HOG相比map提高2.5个点，而HSC在HOG上map提高了4个点（使用内部私有的DPM基线进行比较，两者都使用非公开实现的DPM，低于开源版本）。这些方法的map分别达到29.1％和34.3％。</p><h3 id="3-3-Network-architectures"><a href="#3-3-Network-architectures" class="headerlink" title="3.3. Network architectures"></a><strong>3.3. Network architectures</strong></h3><p>Most results in this paper use the network architecture from Krizhevsky et al. [25]. However, we have found that the choice of architecture has a large effect on R-CNN detection performance. In Table 3 we show results on VOC 2007 test using the 16-layer deep network recently proposed by Simonyan and Zisserman [43]. This network was one of the top performers in the recent ILSVRC 2014 classification challenge. The network has a homogeneous structure consisting of 13 layers of 3 × 3 convolution kernels, with five max pooling layers interspersed, and topped with three fully-connected layers. We refer to this network as “O-Net” for OxfordNet and the baseline as “T-Net” for TorontoNet.</p><p>本文的大多数结果使用了Krizhevsky等人的网络架构。然而，我们发现架构的选择对R-CNN检测性能有很大的影响。在表3中，我们显示了使用Simonyan和Zisserman最近提出的16层深层网络的VOC 2007测试结果[43]。 该网络是最近ILSVRC 2014分类挑战中表现最佳的网络之一。 该网络具有由13层3×3卷积核组成的均匀结构，其中穿插有五个最大池化层，并且顶部有三个完全连接的层。 我们将这个网络称为OxfordNet的“O-Net”，并将TorontoNet的基线称为“T-Net”。</p><p>To use O-Net in R-CNN, we downloaded the publicly available pre-trained network weights for the VGG ILSVRC 16 layers model from the Caffe Model Zoo.1 We then fine-tuned the network using the same protocol as we used for T-Net. The only difference was to use smaller minibatches (24 examples) as required in order to fit within GPU memory. The results in Table 3 show that RCNN with O-Net substantially outperforms R-CNN with TNet, increasing map from 58.5% to 66.0%. However there is a considerable drawback in terms of compute time, with the forward pass of O-Net taking roughly 7 times longer than T-Net.</p><p>要在R-CNN中使用O-Net，我们从Caffe模型库下载了预训练的VGG_ILSVRC_16_layers模型（<a href="https://github.com/BVLC/caffe/wiki/Model-Zoo%EF%BC%89%E3%80%82%E7%84%B6%E5%90%8E%E6%88%91%E4%BB%AC%E4%BD%BF%E7%94%A8%E4%B8%8ET-Net%E4%B8%80%E6%A0%B7%E7%9A%84%E6%96%B9%E6%B3%95%E5%AF%B9%E7%BD%91%E7%BB%9C%E8%BF%9B%E8%A1%8C%E4%BA%86%E5%BE%AE%E8%B0%83%E3%80%82%E5%94%AF%E4%B8%80%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E6%A0%B9%E6%8D%AE%E9%9C%80%E8%A6%81%E4%BD%BF%E7%94%A8%E8%BE%83%E5%B0%8F%E7%9A%84%E6%89%B9%E9%87%8F%EF%BC%8824%E4%B8%AA%EF%BC%89%EF%BC%8C%E4%BB%A5%E9%80%82%E5%BA%94GPU%E5%86%85%E5%AD%98%E3%80%82%E8%A1%A83%E4%B8%AD%E7%9A%84%E7%BB%93%E6%9E%9C%E6%98%BE%E7%A4%BA%EF%BC%8C%E5%85%B7%E6%9C%89O-Net%E7%9A%84R-">https://github.com/BVLC/caffe/wiki/Model-Zoo）。然后我们使用与T-Net一样的方法对网络进行了微调。唯一的区别是根据需要使用较小的批量（24个），以适应GPU内存。表3中的结果显示，具有O-Net的R-</a> CNN基本上优于T-网络的R-CNN，将map从58.5％提高到66.0％。然而，在计算时间方面存在相当大的缺陷，O-Net的前进速度比T-Net长约7倍。</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429215038093-237848494.png"></p><p>Table 3: Detection average precision (%) on VOC 2007 test for two different CNN architectures. The first two rows are results from Table 2 using Krizhevsky et al.’s architecture (T-Net). Rows three and four use the recently proposed 16-layer architecture from Simonyan and Zisserman (O-Net) [43].</p><p>表3：两种不同CNN架构的VOC 2007测试的检测平均精度（％）。 前两行是表2中使用Krizhevsky等人的架构（T-Net）的结果。 第三和第四行使用最近提出的Simonyan和Zisserman（O-Net）的16层架构[43]。</p><h3 id="3-4-Detection-error-analysis"><a href="#3-4-Detection-error-analysis" class="headerlink" title="3.4. Detection error analysis"></a><strong>3.4. Detection error analysis</strong></h3><p>We applied the excellent detection analysis tool from Hoiem et al. [23] in order to reveal our method’s error modes, understand how fine-tuning changes them, and to see how our error types compare with DPM. A full summary of the analysis tool is beyond the scope of this paper and we encourage readers to consult [23] to understand some finer details (such as “normalized AP”). Since the analysis is best absorbed in the context of the associated plots, we present the discussion within the captions of Figure 5 and Figure 6.</p><p>为了揭示我们的方法的错误模式，我们应用了Hoiem等人的优秀检测分析工具23，以了解微调如何改变它们，并将我们的错误类型与DPM比较。分析工具的完整介绍超出了本文的范围，可以参考23了解更多的细节（如“标准化AP”）。千言万语不如一张图，我们在下图（图5和图6）中讨论。</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429215013067-632448486.png"></p><p>Figure 5: Distribution of top-ranked false positive (FP) types. Each plot shows the evolving distribution of FP types as more FPs are considered in order of decreasing score. Each FP is categorized into 1 of 4 types: Loc—poor localization (a detection with an IoU overlap with the correct class between 0.1 and 0.5, or a duplicate); Sim—confusion with a similar category; Oth—confusion with a dissimilar object category; BG—a FP that fired on background. Compared with DPM (see [23]), significantly more of our errors result from poor localization, rather than confusion with background or other object classes, indicating that the CNN features are much more discriminative than HOG. Loose localization likely results from our use of bottom-up region proposals and the positional invariance learned from pre-training the CNN for whole-image classification. Column three shows how our simple bounding-box regression method fixes many localization errors.</p><p>图5：最多的假阳性（FP）类型分布。每个图表显示FP类型的演变分布，按照FP数量降序排列。FP分为4种类型：Loc（定位精度差，检测框与真值的IoU在0.1到0.5之间或重复的检测）。Sim（与相似类别混淆）。Oth（与不相似的类别混淆）。BG（检测框标在了背景上）。与DPM（参见<a href="#fn:23">22</a>）相比，我们的Loc显著增加，而不是Oth和BG，表明CNN特征比HOG更具区分度。Loc增加的原因可能是我们使用自下而上的区域提案可能产生松散的定位位置，以及CNN进行全图像分类的预训练模型所获得的位置不变性。第三列显示了我们的简单边界回归方法如何修复许多Loc<strong>【<strong><strong>这个图不是很好看懂，是取每一条的面积来看？</strong></strong>】。</strong></p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214953328-53559836.png"></p><p>Figure 6: Sensitivity to object characteristics. Each plot shows the mean (over classes) normalized AP (see [23]) for the highest and lowest performing subsets within six different object characteristics (occlusion, truncation, bounding-box area, aspect ratio, viewpoint, part visibility). </p><p>对目标特点的敏感度。每个图显示六个不同目标特点（遮挡，截断，边界区域，纵横比，视角，局部可视性）内最高和最低性能的子集的平均值（跨类别）归一化AP（见<a href="#fn:23">22</a>）。</p><p>We show plots for our method (R-CNN) with and without fine-tuning (FT) and bounding-box regression (BB) as well as for DPM voc-release5. Overall, fine-tuning does not reduce sensitivity (the difference between max and min), but does substantially improve both the highest and lowest performing subsets for nearly all characteristics. This indicates that fine-tuning does more than simply improve the lowest performing subsets for aspect ratio and bounding-box area, as one might conjecture based on how we warp network inputs. Instead, fine-tuning improves robustness for all characteristics including occlusion, truncation, viewpoint, and part visibility.</p><p>我们展示了我们的方法（R-CNN）有或没有微调（FT）和边界回归（BB）以及DPM voc-release5的图。总体而言，微调并不会降低敏感度（最大和最小值之间的差异），而且对于几乎所有的特点，都能极大地提高最高和最低性能的子集的性能。这表明微调不仅仅是简单地提高纵横比和边界区域的最低性能子集的性能（在分析之前，基于我们如何缩放网络输入而推测）。相反，微调可以改善所有特点的鲁棒性，包括遮挡，截断，视角和局部可视性。</p><h3 id="3-5-Bounding-box-regression"><a href="#3-5-Bounding-box-regression" class="headerlink" title="3.5. Bounding-box regression"></a><strong>3.5. Bounding-box regression</strong></h3><p>Based on the error analysis, we implemented a simple method to reduce localization errors. Inspired by the bounding-box regression employed in DPM [17], we train a linear regression model to predict a new detection window given the pool5 features for a selective search region proposal. Full details are given in Appendix C. Results in Table 1, Table 2, and Figure 5 show that this simple approach fixes a large number of mislocalized detections, boosting map by 3 to 4 points.</p><p>基于错误分析，我们实现了一种简单的方法来减少定位错误。受DPM中使用边界框回归的启发，我们训练一个线性回归模型使用在区域提案上提取的pool5特征来预测一个新的检测框。完整的细节在<a href="#C.%E6%A3%80%E6%B5%8B%E6%A1%86%E5%9B%9E%E5%BD%92">附录C</a>中给出。表1，表2和图5中的结果表明，这种简单的方法解决了大量的定位错误，将map提高了3到4个点。</p><h3 id="3-6-Qualitative-results"><a href="#3-6-Qualitative-results" class="headerlink" title="3.6. Qualitative results"></a><strong>3.6. Qualitative results</strong></h3><p>Qualitative detection results on ILSVRC2013 are presented in Figure 8 and Figure 9 at the end of the paper. Each image was sampled randomly from the val2 set and all detections from all detectors with a precision greater than 0.5 are shown. Note that these are not curated and give a realistic impression of the detectors in action. More qualitative results are presented in Figure 10 and Figure 11, but these have been curated. We selected each image because it contained interesting, surprising, or amusing results. Here, also, all detections at precision greater than 0.5 are shown.</p><p>论文结尾处的图8和图9介绍了ILSVRC2013的定性检测结果。 所有显示的图像都是从val2集中随机采样的，其精度大于0.5。 请注意，这些都没有特意挑选，并给出了一个实际检测器的检测结果。 更定性结果如图10和图11所示，这些都是专门挑选的。 我们选择了每张图片，因为它包含有趣，令人惊讶或有趣的结果。 这里，此外，还显示了精度大于0.5的所有检测。</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214934254-444672537.png"></p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214925726-1093204290.png"></p><p>Figure 8: Example detections on the val2 set from the configuration that achieved 31.0% map on val2. Each image was sampled randomly (these are not curated). All detections at precision greater than 0.5 are shown. Each detection is labeled with the predicted class and the precision value of that detection from the detector’s precision-recall curve. Viewing digitally with zoom is recommended.</p><p>图8：在val2上达到31.0％map的配置的检测结果示例。每个图像都是随机抽样的（这些都没有刻意挑选）。显示精度大于0.5的所有检测，并标记了预测的类别和精度。可以放大以看清楚</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214852215-1856507923.png"></p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214846667-332285906.png"></p><p>Figure 9: More randomly selected examples. See Figure 8 caption for details. Viewing digitally with zoom is recommended.</p><p>更多示例。详见图8说明。</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214812719-750485410.png"></p><p>Figure 10: Curated examples. Each image was selected because we found it impressive, surprising, interesting, or amusing. Viewing digitally with zoom is recommended.</p><p>图10：挑选的示例。 选择每张图片是因为我们发现它令人印象深刻，令人惊讶，有趣或有趣。 建议使用缩放以数字方式查看</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214800856-461239909.png"></p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214744210-113388426.png"></p><p>Figure 11: More curated examples. See Figure 10 caption for details. Viewing digitally with zoom is recommended.</p><p>图11：更多精选示例。 有关详细信息，请参见图10标题。 建议使用缩放以数字方式查看。</p><h2 id="4-The-ILSVRC2013-detection-dataset"><a href="#4-The-ILSVRC2013-detection-dataset" class="headerlink" title="4. The ILSVRC2013 detection dataset"></a><strong>4. The ILSVRC2013 detection dataset</strong></h2><p>In Section 2 we presented results on the ILSVRC2013 detection dataset. This dataset is less homogeneous than PASCAL VOC, requiring choices about how to use it. Since these decisions are non-trivial, we cover them in this section.</p><p>在<a href="#%E7%94%A8R-CNN%E8%BF%9B%E8%A1%8C%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B">用R-CNN进行目标检测</a>中，我们介绍了ILSVRC2013检测数据集的结果。该数据集与PASCAL VOC不太一致，需要选择如何使用它。由于这些选择不是显而易见的，我们将在这一节中介绍这些选择。</p><h3 id="4-1-Dataset-overview"><a href="#4-1-Dataset-overview" class="headerlink" title="4.1. Dataset overview"></a><strong>4.1. Dataset overview</strong></h3><p>The ILSVRC2013 detection dataset is split into three sets: train (395,918), val (20,121), and test (40,152), where the number of images in each set is in parentheses. The val and test splits are drawn from the same image distribution. These images are scene-like and similar in complexity (number of objects, amount of clutter, pose variability, etc.) to PASCAL VOC images. The val and test splits are exhaustively annotated, meaning that in each image all instances from all 200 classes are labeled with bounding boxes. The train set, in contrast, is drawn from the ILSVRC2013 classification image distribution. These images have more variable complexity with a skew towards images of a single centered object. Unlike val and test, the train images (due to their large number) are not exhaustively annotated. In any given train image, instances from the 200 classes may or may not be labeled. In addition to these image sets, each class has an extra set of negative images. Negative images are manually checked to validate that they do not contain any instances of their associated class. The negative image sets were not used in this work. More information on how ILSVRC was collected and annotated can be found in [11, 36].</p><p>ILSVRC2013检测数据集分为三组：训练(395,918)，验证(20,121)和测试(40,152)，其中每组的图像数目在括号中。验证和测试集是从相同的图像分布中划分的。这些图像与PASCAL VOC图像中的场景和复杂性（目标数量，杂波量，姿态变异性等）类似。验证和测试集是详尽标注的，这意味着在每个图像中，来自所有200个类的所有实例都被标注为边界框。相比之下，训练集来自ILSVRC2013分类图像。这些图像具有更多的可变复杂性，并且倾向于是单个位于图像中心的目标的图像。与验证和测试集不同，训练集（由于它们的数量很多）没有详尽标注。在任何给定的训练图像中，200个类别的实例可能被标注也可能不被标注。除了这些图像集，每个类都有一组额外的负样本。负样本经过人工检查以确认它们不包含任何相关类的实例。本文没有使用负样本。有关如何收集和标注ILSVRC的更多信息可以在[11,36]中找到。</p><p>The nature of these splits presents a number of choices for training R-CNN. The train images cannot be used for hard negative mining, because annotations are not exhaustive. Where should negative examples come from? Also, the train images have different statistics than val and test. Should the train images be used at all, and if so, to what extent? While we have not thoroughly evaluated a large number of choices, we present what seemed like the most obvious path based on previous experience. </p><p>这些数据集的分组的性质为训练R-CNN提供了许多选择。训练图像不能用于难负样本重训练，因为标注不是很好。负样本来自哪里？此外，训练图像具有不同于验证和训练集的分布。是否应该使用训练图像，如果是，在什么程度上？虽然我们还没有彻底评估大量的选择，但是我们根据以往的经验，提出了一个最明显的路径。</p><p>Our general strategy is to rely heavily on the val set and use some of the train images as an auxiliary source of positive examples. To use val for both training and validation, we split it into roughly equally sized “val1” and “val2” sets. Since some classes have very few examples in val (the smallest has only 31 and half have fewer than 110), it is important to produce an approximately class-balanced partition. To do this, a large number of candidate splits were generated and the one with the smallest maximum relative class imbalance was selected.2 Each candidate split was generated by clustering val images using their class counts as features, followed by a randomized local search that may improve the split balance. The particular split used here has a maximum relative imbalance of about 11% and a median relative imbalance of 4%. The val1/val2 split and code used to produce them will be publicly available to allow other researchers to compare their methods on the val splits used in this report.</p><p>我们的总体策略是严重依赖验证集，并使用一些训练图像作为一个辅助正样本来源。为了使用验证集进行训练和验证，我们将其分成大小大致相等的“val1”和“val2”集合。由于某些类在val中的数量非常少（最小的只有31个，连110个的一半都不到），所以产生一个近似类间均衡的划分是很重要的。为此，产生了大量的候选分割，并选择了最大相对类间不平衡的最小值（相对不平衡度被定义为|a-b|/(a+b)，其中a和b是两个集合各自的类计数）。每个候选分裂是通过使用其类计数作为特征聚类的验证集图像来生成的，然后是一个可以改善划分平衡度的随机局部搜索。这里使用的特定划分具有约11％的最大相对类间不平衡和4％的中值相对类间不平衡。val1/val2划分和用于生产它们的代码将被公开提供，以允许其他研究人员将他们的方法与在本文中使用的验证集划分方法进行比较。</p><h3 id="4-2-Region-proposals"><a href="#4-2-Region-proposals" class="headerlink" title="4.2. Region proposals"></a><strong>4.2. Region proposals</strong></h3><p>We followed the same region proposal approach that was used for detection on PASCAL. Selective search [39] was run in “fast mode” on each image in val1, val2, and test (but not on images in train). One minor modification was required to deal with the fact that selective search is not scale invariant and so the number of regions produced depends on the image resolution. ILSVRC image sizes range from very small to a few that are several mega-pixels, and so we resized each image to a fixed width (500 pixels) before running selective search. On val, selective search resulted in an average of 2403 region proposals per image with a 91.6% recall of all ground-truth bounding boxes (at 0.5 IoU threshold). This recall is notably lower than in PASCAL, where it is approximately 98%, indicating significant room for improvement in the region proposal stage. </p><p>我们遵循用于PASCAL检测的区域提案方法。选择性搜索<a href="#fn:39">16</a>在val1，val2中的每个图像上以“快速模式”运行，并进行测试（但不是在训练图像上）。需要一个小的修改来处理选择性搜索不是尺度不变的，所以需要产生的区域数量取决于图像分辨率。 ILSVRC的图像尺寸范围从非常小到少量几百万像素的图像，因此我们在运行选择性搜索之前，将每个图像的大小调整为固定的宽度（500像素）。在验证集上，选择性搜索在每个图像上平均有2403个区域提案，检测框真值（以0.5 IoU阈值）的召回率91.6％。这一召回率明显低于PASCAL的约98％，表明该区域提案阶段有明显的改善空间。</p><h3 id="4-3-Training-data"><a href="#4-3-Training-data" class="headerlink" title="4.3. Training data"></a><strong>4.3. Training data</strong></h3><p>For training data, we formed a set of images and boxes that includes all selective search and ground-truth boxes from val1 together with up to N ground-truth boxes per class from train (if a class has fewer than N ground-truth boxes in train, then we take all of them). We’ll call this dataset of images and boxes val1+trainN . In an ablation study, we show map on val2 for N ∈ {0, 500, 1000} (Section 4.5). </p><p>对于训练数据，我们形成了一套图像和方框，其中包括val1的所有选择性搜索和检测框真值，以及训练集中每个类别最多N个检测框真值（如果一个类别的检测框真值数少于N个，那就有多少用多少）。我们将把这个数据集称为val1+trainN。在消融研究中，我们给出了N∈{0,500,1000}的val2上的map（见<a href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C">消融实验</a>）。</p><p>Training data is required for three procedures in R-CNN: (1) CNN fine-tuning, (2) detector SVM training, and (3) bounding-box regressor training. CNN fine-tuning was run for 50k SGD iteration on val1+trainN using the exact same settings as were used for PASCAL. Fine-tuning on a single NVIDIA Tesla K20 took 13 hours using Caffe. For SVM training, all ground-truth boxes from val1+trainN were used as positive examples for their respective classes. Hard negative mining was performed on a randomly selected subset of 5000 images from val1. An initial experiment indicated that mining negatives from all of val1, versus a 5000 image subset (roughly half of it), resulted in only a 0.5 percentage point drop in map, while cutting SVM training time in half. No negative examples were taken from train because the annotations are not exhaustive. The extra sets of verified negative images were not used. The bounding-box regressors were trained on val1.</p><p>R-CNN中的三个阶段需要训练数据：（1）CNN微调，（2）检测器SVM训练（3）检测框回归训练。使用与用于PASCAL的完全相同的设置，在val1+trainN上进行50k次SGD迭代以微调CNN。使用Caffe在一块NVIDIA Tesla K20上微调花了13个小时。对于SVM训练，使用来自val1+trainN的所有检测框真值作为各自类别的正样本。对来自val1的5000张（大约一半）随机选择的图像的子集执行难负样本重训练。最初的实验表明，难负样本重训练仅使map下降了0.5个百分点，同时将SVM训练时间缩短了一半。没有从训练集中采样负样本，因为没有详尽标注。没有额外的经过确认的负样本。边界框回归器在val1训练。 </p><h3 id="4-4-Validation-and-evaluation"><a href="#4-4-Validation-and-evaluation" class="headerlink" title="4.4. Validation and evaluation"></a><strong>4.4. Validation and evaluation</strong></h3><p>Before submitting results to the evaluation server, we validated data usage choices and the effect of fine-tuning and bounding-box regression on the val2 set using the training data described above. All system hyperparameters (e.g., SVM C hyperparameters, padding used in region warping, NMS thresholds, bounding-box regression hyperparameters) were fixed at the same values used for PASCAL. Undoubtedly some of these hyperparameter choices are slightly suboptimal for ILSVRC, however the goal of this work was to produce a preliminary R-CNN result on ILSVRC without extensive dataset tuning. After selecting the best choices on val2, we submitted exactly two result files to the ILSVRC2013 evaluation server. The first submission was without bounding-box regression and the second submission was with bounding-box regression. For these submissions, we expanded the SVM and boundingbox regressor training sets to use val+train1k and val, respectively. We used the CNN that was fine-tuned on val1+train1k to avoid re-running fine-tuning and feature computation. </p><p>在将结果提交给评估服务器之前，我们使用上述训练数据验证了数据使用选择、微调和检测框回归对val 2集的影响。所有系统超参数（例如，SVM C超参数，区域缩放中使用的边界填充，NMS阈值，检测框回归超参数）固定为与PASCAL相同的值。毫无疑问，这些超参数选择中的一些对ILSVRC来说稍微不太理想，但是这项工作的目标是在没有广泛数据集调优的情况下，在ILSVRC上产生初步的R-CNN结果。在选择val2上的最佳配置后，我们提交了两个结果文件到ILSVRC2013评估服务器。第一个没有检测框回归，第二个有检测框回归。对于这些提交，我们扩展了SVM和检测框回归训练集，分别使用val+train1k和val。我们在val1+train1k上微调CNN来避免重新运行微调和特征计算。</p><h3 id="4-5-Ablation-study"><a href="#4-5-Ablation-study" class="headerlink" title="4.5. Ablation study"></a><strong>4.5. Ablation study</strong></h3><p>Table 4 shows an ablation study of the effects of different amounts of training data, fine-tuning, and boundingbox regression. A first observation is that map on val2 matches map on test very closely. This gives us confidence that map on val2 is a good indicator of test set performance. The first result, 20.9%, is what R-CNN achieves using a CNN pre-trained on the ILSVRC2012 classification dataset (no fine-tuning) and given access to the small amount of training data in val1 (recall that half of the classes in val1 have between 15 and 55 examples). Expanding the training set to val1+trainN improves performance to 24.1%, with essentially no difference between N = 500 and N = 1000. Fine-tuning the CNN using examples from just val1 gives a modest improvement to 26.5%, however there is likely significant overfitting due to the small number of positive training examples. Expanding the fine-tuning set to val1+train1k, which adds up to 1000 positive examples per class from the train set, helps significantly, boosting map to 29.7%. Bounding-box regression improves results to 31.0%, which is a smaller relative gain that what was observed in PASCAL.</p><p>如表（表4）所示：（ILSVRC2013上的数据使用选择、微调和边界回归消融研究。）第一个观察是，val2上的map与测试集上的map非常接近。这使我们有信心相信，val2上的map是测试集性能的良好指标。第一个结果是20.9％，是在ILSVRC2012分类数据集上预训练的CNN（无微调）并允许访问val1中少量训练数据的R-CNN实现（val1中一半的类别，每个类有15到55个样本）。将训练集扩展到val1+trainN将性能提高到24.1％，N = 500和N = 1000之间基本上没有差异。使用仅从val1的样本微调CNN可以稍微改善到26.5％，但是由于用于训练的正样本较少，可能会出现严重的过拟合。将用于微调的数据扩展到val1+train1k，相当于每个类增加了100个正样本用于训练，有助于将map显著提高至29.7％。检测框回归将结果提高到31.0％，这与PASCAL中所观察到的收益相比较小。</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214721268-824158237.png"></p><p>Table 4: ILSVRC2013 ablation study of data usage choices, fine-tuning, and bounding-box regression.</p><p>表4：ILSVRC2013上的数据使用选择、微调和边界回归消融研究。</p><h3 id="4-6-Relationship-to-OverFeat"><a href="#4-6-Relationship-to-OverFeat" class="headerlink" title="4.6. Relationship to OverFeat"></a><strong>4.6. Relationship to OverFeat</strong></h3><p>There is an interesting relationship between R-CNN and OverFeat: OverFeat can be seen (roughly) as a special case of R-CNN. If one were to replace selective search region proposals with a multi-scale pyramid of regular square regions and change the per-class bounding-box regressors to a single bounding-box regressor, then the systems would be very similar (modulo some potentially significant differences in how they are trained: CNN detection fine-tuning, using SVMs, etc.). It is worth noting that OverFeat has a significant speed advantage over R-CNN: it is about 9x faster, based on a figure of 2 seconds per image quoted from [34]. This speed comes from the fact that OverFeat’s sliding windows (i.e., region proposals) are not warped at the image level and therefore computation can be easily shared between overlapping windows. Sharing is implemented by running the entire network in a convolutional fashion over arbitrary-sized inputs. Speeding up R-CNN should be possible in a variety of ways and remains as future work.</p><p>R-CNN和OverFeat之间有一个有趣的关系：OverFeat可以看作（大致上）是R-CNN的一个特例。如果用一个多尺度的正方形区域的金字塔取代选择性搜索区域提案，并将每个类别的检测框回归器改变为一个单一的检测框回归函数，则两个系统将是非常相似的（训练上有一些潜在的显著差异：CNN微调、使用SVM等）。值得注意的是，OverFeat比R-CNN具有显着的速度优势：根据<a href="#fn:34">18</a>引用的图中显示每张图像2秒，速度约为RCNN的9倍。这种速度来自于OverFeat的滑动窗口（即区域提案）在图像级别没有缩放的事实，因此可以在重叠窗口之间轻松共享计算。通过在任意大小的输入上以卷积方式运行整个网络来实现共享。加快R-CNN的速度应该有很多可行的办法，未来的工作中将会考虑。</p><h2 id="5-Semantic-segmentation"><a href="#5-Semantic-segmentation" class="headerlink" title="5. Semantic segmentation"></a><strong>5. Semantic segmentation</strong></h2><p>Region classification is a standard technique for semantic segmentation, allowing us to easily apply R-CNN to the PASCAL VOC segmentation challenge. To facilitate a direct comparison with the current leading semantic segmentation system (called O2P for “second-order pooling”) [4], we work within their open source framework. O2P uses CPMC to generate 150 region proposals per image and then predicts the quality of each region, for each class, using support vector regression (SVR). The high performance of their approach is due to the quality of the CPMC regions and the powerful second-order pooling of multiple feature types (enriched variants of SIFT and LBP). We also note that Farabet et al. [16] recently demonstrated good results on several dense scene labeling datasets (not including PASCAL) using a CNN as a multi-scale per-pixel classifier. </p><p>区域分类是语义分割的基础，这使我们可以轻松地将R-CNN应用于PASCAL VOC分割挑战。为了便于与当前领先的语义分割系统（称为“二阶池化”的O2P）的直接比较，我们在其开源框架内修改。O2P使用CPMC为每个图像生成150个区域提案，然后使用支持向量回归(SVR)来预测对于每个类别的每个区域的质量。他们的方法的高性能是由于CPMC区域的高质量和强大的多种特征类型（SIFT和LBP的丰富变体）的二阶池化。我们还注意到，Farabet等最近使用CNN作为多尺度像素级分类器在几个密集场景标记数据集（不包括PAS-CAL）上取得了良好的结果。</p><p>We follow [2, 4] and extend the PASCAL segmentation training set to include the extra annotations made available by Hariharan et al. [22]. Design decisions and hyperparameters were cross-validated on the VOC 2011 validation set. Final test results were evaluated only once.</p><p>我们遵循并扩展PASCAL分割训练集，以包含Hariharan等提供的额外注释。在VOC 2011验证集上，交叉验证我们的设计决策和超参数。最终测试结果仅提交一次。</p><p><strong>CNN features for segmentation</strong>. We evaluate three strategies for computing features on CPMC regions, all of which begin by warping the rectangular window around the region to 227 × 227. The first strategy (full) ignores the region’s shape and computes CNN features directly on the warped window, exactly as we did for detection. However, these features ignore the non-rectangular shape of the region. Two regions might have very similar bounding boxes while having very little overlap. Therefore, the second strategy (fg) computes CNN features only on a region’s foreground mask. We replace the background with the mean input so that background regions are zero after mean subtraction. The third strategy (full+fg) simply concatenates the full and fg features; our experiments validate their complementarity.</p><p>****用于分割的CNN特征。****我们评估了在CPMC区域上计算特征的三个策略，所有这些策略都是将区域缩放为227 x 227。第一个策略（full）忽略了该区域的形状，并直接在缩放后的区域上计算CNN特征，就像我们缩放区域提案那样。然而，这些特征忽略了区域的非矩形形状。两个区域可能具有非常相似的边界框，同时具有非常小的重叠。因此，第二个策略（fg）仅在区域的前景掩码上计算CNN特征。我们用图像均值替换背景，使得背景区域在减去图像均值后为零。第三个策略（full + fg）简单地连接full和fg特征。我们的实验验证了它们的互补性。</p><p>Results on VOC 2011. Table 5 shows a summary of our results on the VOC 2011 validation set compared with O2P. (See Appendix E for complete per-category results.) Within each feature computation strategy, layer fc6 always outperforms fc7 and the following discussion refers to the fc6 features. The fg strategy slightly outperforms full, indicating that the masked region shape provides a stronger signal, matching our intuition. However, full+fg achieves an average accuracy of 47.9%, our best result by a margin of 4.2% (also modestly outperforming O2P), indicating that the context provided by the full features is highly informative even given the fg features. Notably, training the 20 SVRs on our full+fg features takes an hour on a single core, compared to 10+ hours for training on O2P features.</p><p>关于VOC 2011的结果。表5显示了我们在VOC 2011验证集上与O2P相比的结果，每个类别的完整结果参见<a href="#E.%E6%AF%8F%E4%B8%AA%E7%B1%BB%E5%88%AB%E7%9A%84%E5%88%86%E5%89%B2%E7%BB%93%E6%9E%9C">附录E</a>。在每种特征计算策略中，fc6总是超过fc7，以下讨论是指fc6的特征。fg策略稍微优于full，表明掩码区域的形状提供了更强的信号，与我们的直觉相匹配。然而，full + fg的平均准确度达到47.9％，比我们的fg最佳结果高4.2％（也略胜于O2P），表明full特征提供大量的信息，即使给定fg特征。值得注意的是，在full + fg特征上使用一个CPU核心训练20个SVR需要花费一个小时，相比之下，在O2P特征上训练需要10个多小时。</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214656057-1946138586.png"></p><p>Table 5: Segmentation mean accuracy (%) on VOC 2011 validation. Column 1 presents O2P; 2-7 use our CNN pre-trained on ILSVRC 2012.</p><p>表5：VOC 2011验证的分段平均准确度（％）。第1列呈现O2P; 2-7使用我们的CNN预训练</p><p>ILSVRC 2012。</p><p>In Table 6 we present results on the VOC 2011 test set, comparing our best-performing method, fc6 (full+fg), against two strong baselines. Our method achieves the highest segmentation accuracy for 11 out of 21 categories, and the highest overall segmentation accuracy of 47.9%, averaged across categories (but likely ties with the O2P result under any reasonable margin of error). Still better performance could likely be achieved by fine-tuning.</p><p>在表6我们提供了VOC 2011测试集的结果，将我们的最佳表现方法fc6(full + fg)与两个强大的基线进行了比较。我们的方法在21个类别中的11个中达到了最高的分割精度，最高的分割精度为47.9％，跨类别平均（但可能与任何合理的误差范围内的O2P结果有关）。微调可能会实现更好的性能。</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214630704-320959759.png"></p><p>Table 6: Segmentation accuracy (%) on VOC 2011 test. We compare against two strong baselines: the “Regions and Parts” (R&amp;P) method of [2] and the second-order pooling (O2P) method of [4]. Without any fine-tuning, our CNN achieves top segmentation performance, outperforming R&amp;P and roughly matching O2P.</p><p>表6：VOC 2011测试的分段准确度（％）。 我们比较两个强大的基线：“区域和部分”（R＆P）[2]的方法和[4]的二阶合并（O2P）方法。 没有任何微调，我们的CNN实现了最高的细分性能，优于R＆P并大致匹配O2P。</p><h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a><strong>6. Conclusion</strong></h2><p>In recent years, object detection performance had stagnated. The best performing systems were complex ensembles combining multiple low-level image features with high-level context from object detectors and scene classifiers. This paper presents a simple and scalable object detection algorithm that gives a 30% relative improvement over the best previous results on PASCAL VOC 2012. </p><p>近年来，物体检测性能停滞不前。性能最好的系统是复杂的组合，将多个低级图像特征与来自物体检测器和场景分类器的高级语境相结合。本文提出了一种简单且可扩展的对象检测算法，相对于PASCAL VOC 2012上的前最佳结果，相对改进了30％。</p><p>We achieved this performance through two insights. The first is to apply high-capacity convolutional neural networks to bottom-up region proposals in order to localize and segment objects. The second is a paradigm for training large CNNs when labeled training data is scarce. We show that it is highly effective to pre-train the network— with supervision—for a auxiliary task with abundant data (image classification) and then to fine-tune the network for the target task where data is scarce (detection). We conjecture that the “supervised pre-training/domain-specific finetuning” paradigm will be highly effective for a variety of data-scarce vision problems. </p><p>我们通过两个关键的改进实现了这一效果。第一个是将大容量卷积神经网络应用于自下而上的区域提案，以便定位和分割对象。第二个是在有标记的训练数据很少的情况下训练大型CNN的方法。我们发现，通过使用大量的图像分类数据对辅助任务进行有监督的预训练，然后对数据稀缺的目标检测任务进行微调，是非常有效的。我们相信，“监督的预训练/领域特定的微调”的方法对于各种数据缺乏的视觉问题都将是非常有效的。</p><p>We conclude by noting that it is significant that we achieved these results by using a combination of classical tools from computer vision and deep learning (bottom-up region proposals and convolutional neural networks). Rather than opposing lines of scientific inquiry, the two are natural and inevitable partners. </p><p>我们通过使用计算机视觉中的经典工具与深度学习（自下而上的区域提案和卷积神经网络）的组合达到了很好的效果。而不是仅仅依靠纯粹的科学探究。</p><p>Acknowledgments. This research was supported in part by DARPA Mind’s Eye and MSEE programs, by NSF awards IIS-0905647, IIS-1134072, and IIS-1212798, MURI N000014-10-1-0933, and by support from Toyota. The GPUs used in this research were generously donated by the NVIDIA Corporation.</p><p>****致谢：****该研究部分由DARPA Mind的Eye与MSEE项目支持，NSF授予了IIS-0905647，IIS-1134072和IIS-1212798，以及丰田支持的MURI N000014-10-1-0933。本研究中使用的GPU由NVIDIA公司慷慨捐赠。</p><p><strong>Appendix A.</strong> </p><h3 id="A-Object-proposal-transformations"><a href="#A-Object-proposal-transformations" class="headerlink" title="A. Object proposal transformations"></a>A. <strong>Object proposal transformations</strong></h3><p>The convolutional neural network used in this work requires a fixed-size input of 227 × 227 pixels. For detection, we consider object proposals that are arbitrary image rectangles. We evaluated two approaches for transforming object proposals into valid CNN inputs.</p><p>本文中使用的卷积神经网络需要227×227像素的固定大小输入。为了检测，我们认为目标提案是任意矩形的图像。我们评估了将目标提案转换为有效的CNN输入的两种方法。</p><p>The first method (“tightest square with context”) encloses each object proposal inside the tightest square and then scales (isotropically) the image contained in that square to the CNN input size. Figure 7 column (B) shows this transformation. A variant on this method (“tightest square without context”) excludes the image content that surrounds the original object proposal. Figure 7 column (C) shows this transformation. The second method (“warp”) anisotropically scales each object proposal to the CNN input size. Figure 7 column (D) shows the warp transformation. </p><p>第一个方法将目标提案扩充为正方形并缩放到所需大小，如图7(B)所示。这种方法还有一种变体，仅扩充为方框，扩充部分不填充图像内容，如图7(C)所示。第二种方法是将目标提案不保留横纵比的情况下缩放到所需大小，如图7(D)所示。</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214557587-759761815.png"></p><p>Figure 7: Different object proposal transformations. (A) the original object proposal at its actual scale relative to the transformed CNN inputs; (B) tightest square with context; (C) tightest square without context; (D) warp. Within each column and example proposal, the top row corresponds to p = 0 pixels of context padding while the bottom row has p = 16 pixels of context padding.</p><p>图7：不同的对象提议转换。 （一）原始对象提案的实际规模相对于转换的CNN输入; （B）具有背景的最严格的正方形; （C）最紧张没有背景的广场; （D）翘曲。 在每列内和示例提议，顶行对应于上下文的p = 0像素填充，而底行有p = 16像素的上下文填充。</p><p>For each of these transformations, we also consider including additional image context around the original object proposal. The amount of context padding (p) is defined as a border size around the original object proposal in the transformed input coordinate frame. Figure 7 shows p = 0 pixels in the top row of each example and p = 16 pixels in the bottom row. In all methods, if the source rectangle extends beyond the image, the missing data is replaced with the image mean (which is then subtracted before inputing the image into the CNN). A pilot set of experiments showed that warping with context padding (p = 16 pixels) outperformed the alternatives by a large margin (3-5 map points). Obviously more alternatives are possible, including using replication instead of mean padding. Exhaustive evaluation of these alternatives is left as future work.</p><p>对于这些转换中的每一个，我们还考虑在原始目标提案四周包括附加图像内容。内容填充的量(pp)被定义为在缩放后图像中，原始目标提案周围的边界大小。图7显示了每个示例的顶行中p=0像素，底行中p=16像素。在所有方法中，如果矩形框超出图像边缘，超出的部分将被填充为图像均值（然后在将图像输入到CNN之前减去）。一组实验表明，采用上下文填充（ p=16像素）的缩放可以明显提高map（提高3-5个点）。显然还有更多其它可行的方案，包括使用复制而不是平均填充。对这些方案的详尽评估将作为未来的工作。</p><h3 id="B-Positive-vs-negative-examples-and-softmax"><a href="#B-Positive-vs-negative-examples-and-softmax" class="headerlink" title="B. Positive vs. negative examples and softmax"></a><strong>B. Positive vs. negative examples and softmax</strong></h3><p>Two design choices warrant further discussion. The first is: Why are positive and negative examples defined differently for fine-tuning the CNN versus training the object detection SVMs? To review the definitions briefly, for finetuning we map each object proposal to the ground-truth instance with which it has maximum IoU overlap (if any) and label it as a positive for the matched ground-truth class if the IoU is at least 0.5. All other proposals are labeled “background” (i.e., negative examples for all classes). For training SVMs, in contrast, we take only the ground-truth boxes as positive examples for their respective classes and label proposals with less than 0.3 IoU overlap with all instances of a class as a negative for that class. Proposals that fall into the grey zone (more than 0.3 IoU overlap, but are not ground truth) are ignored. </p><p>有两个设计选择值得进一步讨论。第一个是：为什么在微调CNN和训练目标检测SVM时定义的正负样本不同？首先简要回顾下正负样本的定义，对于微调，我们将每个目标提案映射到它具有最大IoU重叠（如果有的话）的检测框真值上，如果其IoU至少为0.5，并将其标记为对应类别的正样本。剩下的提案都标记为“背景”（即所有类的负样本）。对于训练SVM，相比之下，我们只采用检测框真值作为各自类别的正样本。与某一类别所有的正样本的IoU都小于0.3的目标提案将被标记为该类别的负样本。其它（IoU超过0.3，但不是检测框真值）的提案被忽略。</p><p>Historically speaking, we arrived at these definitions because we started by training SVMs on features computed by the ImageNet pre-trained CNN, and so fine-tuning was not a consideration at that point in time. In that setup, we found that our particular label definition for training SVMs was optimal within the set of options we evaluated (which included the setting we now use for fine-tuning). When we started using fine-tuning, we initially used the same positive and negative example definition as we were using for SVM training. However, we found that results were much worse than those obtained using our current definition of positives and negatives. </p><p>从时序上讲，我们得出这些定义是因为我们一开始通过由ImageNet预先训练的CNN计算出的特征训练SVM，因此微调在这个时间点不是一个需要考虑因素。在这种情况下，我们发现，在我们评估的一组设置（包括我们现在用于微调的设置）中，我们当前使用的训练SVM的设置是最佳的。当我们开始使用微调时，我们最初使用与我们用于SVM训练的正负样本的定义相同的定义。然而，我们发现结果比使用我们当前定义的正负样本获得的结果差得多。</p><p>Our hypothesis is that this difference in how positives and negatives are defined is not fundamentally important and arises from the fact that fine-tuning data is limited. Our current scheme introduces many “jittered” examples (those proposals with overlap between 0.5 and 1, but not ground truth), which expands the number of positive examples by approximately 30x. We conjecture that this large set is needed when fine-tuning the entire network to avoid overfitting. However, we also note that using these jittered examples is likely suboptimal because the network is not being fine-tuned for precise localization. </p><p>我们的假设是，如何定义正负样本的差异对训练结果影响不大，结果的差异主要是由微调数据有限这一事实引起的。我们目前的方案引入了许多“抖动”的样本（这些提案与检测框真值的重叠在0.5和1之间，但并不是检测框真值），这将正样本的数量增加了大约30倍。我们推测，需要使用如此大量的样本以避免在微调网络时的过拟合。然而，我们还注意到，使用这些抖动的例子可能不是最佳的，因为网络没有被微调以进行精确的定位。</p><p>This leads to the second issue: Why, after fine-tuning, train SVMs at all? It would be cleaner to simply apply the last layer of the fine-tuned network, which is a 21-way softmax regression classifier, as the object detector. We tried this and found that performance on VOC 2007 dropped from 54.2% to 50.9% map. This performance drop likely arises from a combination of several factors including that the definition of positive examples used in fine-tuning does not emphasize precise localization and the softmax classifier was trained on randomly sampled negative examples rather than on the subset of “hard negatives” used for SVM training. </p><p>这导致了第二个问题：为什么微调之后，训练SVM呢？简单地将最后一层微调网络（21路Softmax回归分类器）作为对象检测器将变得更加简洁。我们尝试了这一点，发现VOC 2007的表现从54.2％下降到了50.9％的map。这种性能下降可能来自几个因素的组合，包括微调中使用的正样本的定义不强调精确定位，并且softmax分类器是在随机抽样的负样本上训练的，而不是用于训练SVM的“更严格的负样本”子集。</p><p>This result shows that it’s possible to obtain close to the same level of performance without training SVMs after fine-tuning. We conjecture that with some additional tweaks to fine-tuning the remaining performance gap may be closed. If true, this would simplify and speed up R-CNN training with no loss in detection performance.</p><p>这个结果表明，微调之后可以获得接近SVM水平的性能，而无需训练SVM。我们推测，通过一些额外的调整来微调以达到更接近的水平。如果是这样，这样可以简化和加速R-CNN训练，而不会在检测性能方面有任何损失。</p><h3 id="C-Bounding-box-regression"><a href="#C-Bounding-box-regression" class="headerlink" title="C. Bounding-box regression"></a><strong>C. Bounding-box regression</strong></h3><p>We use a simple bounding-box regression stage to improve localization performance. After scoring each selective search proposal with a class-specific detection SVM, we predict a new bounding box for the detection using a class-specific bounding-box regressor. This is similar in spirit to the bounding-box regression used in deformable part models [17]. The primary difference between the two approaches is that here we regress from features computed by the CNN, rather than from geometric features computed on the inferred DPM part locations. </p><p>我们使用一个简单的检测框回归来提高定位性能。在使用类特定检测SVM对每个选择性搜索提案进行评分之后，我们使用类别特定的边界回归器预测新的检测框。这与在可变部件模型中使用的检测框回归相似<a href="#fn:17">19</a>。这两种方法之间的主要区别在于，我们使用CNN计算的特征回归，而不是使用在推测的DPM部件位置上计算的几何特征。</p><p>The input to our training algorithm is a set of N training pairs {(P i , Gi )}i=1,…,N , where P i = (P ix , Piy , Piw, Pih ) specifies the pixel coordinates of the center of proposal P i ’s bounding box together with P i ’s width and height in pixels. Hence forth, we drop the superscript i unless it is needed. Each ground-truth bounding box G is specified in the same way: G = (Gx, Gy, Gw, Gh). Our goal is to learn a transformation that maps a proposed box P to a ground-truth box G. </p><p>我们的训练算法的输入是一组N个训练对(Pi,Gi)i=1,…,N，其中Pi=(Pix,Piy,Piw,Pih)指定提案Pi的边界框中心的像素坐标以及宽度和高度（以像素为单位）。注意，除非需要，下文中我们不再写出上标i。每个检测框真值G以相同的方式指定：G=(Gx,Gy,Gw,Gh)。我们的目标是学习将提案框PP映射到检测框真值G的转换。</p><p>We parameterize the transformation in terms of four functions dx(P), dy(P), dw(P), and dh(P). The first two specify a scale-invariant translation of the center of P’s bounding box, while the second two specify log-space translations of the width and height of P’s bounding box. After learning these functions, we can transform an input proposal P into a predicted ground-truth box Gˆ by applying the transformation</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214537888-1055372362.png"></p><p>我们使用四个函数dx(P)，d(yP)，d(Pw)和dh(P)参数化这个转换。前两个指定P的边界框的中心的比例不变的平移，后两个指定PP的边界框的宽度和高度的对数空间转换。在学习了这些函数后，我们可以通过应用转换将输入提案P转换成预测的检测框真值G^。</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214528595-2105199529.png"></p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214525470-16620114.png"></p><p>每个函数d⋆(P)d⋆(P)（⋆⋆表示x,y,w,h中的一个）都建模为提案P的pool5特征（记为ϕ5(P)，对图像数据的依赖隐含的假定）的线性函数。即d⋆(P)=wT⋆ϕ5(P)，其中w⋆表示模型和训练参数的一个向量，通过优化正则化最小二乘法的目标（脊回归）来学习w⋆。</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214456199-1782881344.png"></p><p>We found two subtle issues while implementing bounding-box regression. The ﬁrst is that regularization is important: we set λ = 1000 based on a validation set. The second issue is that care must be taken when selecting which training pairs (P,G) to use. Intuitively, if P is far from all ground-truth boxes, then the task of transforming P to a ground-truth box G does not make sense. Using examples like P would lead to a hopeless learning problem. Therefore, we only learn from a proposal P if it is nearby at least one ground-truth box. We implement “nearness” by assigning P to the ground-truth box G with which it has maximum IoU overlap (in case it overlaps more than one) if and only if the overlap is greater than a threshold (which we set to 0.6 using a validation set). All unassigned proposals are discarded. We do this once for each object class in order to learn a set of class-speciﬁc bounding-box regressors.</p><p>我们在实现边界框回归的时候发现了两个微妙的问题。第一个就是正则化是非常重要的：基于验证集，我们设置λ=1000。第二个问题是，在选择使用哪个训练对（P；G）时必须小心。直观地，如果远离所有的真实框，那么将P转换到真实框G的任务就没有意义。使用像P这样的例子将会导致一个无望的学习问题。因此，我们只从这样的候选P中进行学习，其至少与一个真实框离的比较近。我们通过将P分配给真实框G，当且仅当重叠大于阈值（我们使用一个验证集设置成0.6）时，它与其具有最大的IoU重叠（以防重叠超过一个）。所有未分配的候选区域都被丢弃。对于每一个对象类我们只做一次，以便学习一组特定类边界框的回归器。 </p><p>At test time, we score each proposal and predict its new detection window only once. In principle, we could iterate this procedure (i.e., re-score the newly predicted bounding box, and then predict a new bounding box from it, and so on). However, we found that iterating does not improve results.</p><p>在测试的时候，我们为每一个候选框打分，并且预测一次它的新检测窗口。原则上来说，我们可以迭代这个过程（即，为新得到的预测框重新打分，然后从它再预测一个新的边界框，以此类推）。然而，我们发现迭代没有改善结果。</p><h3 id="D-Additional-feature-visualizations"><a href="#D-Additional-feature-visualizations" class="headerlink" title="D. Additional feature visualizations"></a><strong>D. Additional feature visualizations</strong></h3><p>Figure 12 shows additional visualizations for 20 pool5 units. For each unit, we show the 24 region proposals that maximally activate that unit out of the full set of approximately 10 million regions in all of VOC 2007 test. We label each unit by its (y, x, channel) position in the 6 × 6 × 256 dimensional pool5 feature map. Within each channel, the CNN computes exactly the same function of the input region, with the (y, x) position changing only the receptive ﬁeld.</p><p>图12为20个pool5单元展示了附加的可视化。对于每一个单元来说，我们展示了可以最大激活VOC 2007 测试集的全部的大约1000万个区域中的24个候选区域。我们在6∗6∗256维的pool5特征图上为每个单元都标记了它的（y,x,channel）位置。在每个通道内，CNN计算输入区域的完全相同的函数。</p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214436200-2108732848.png"></p><p><img src="/blog/blog/2022/04/29/r-cnn-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/R-CNN-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.assets/1571518-20220429214408465-1880873570.png"></p><p>Figure 12: We show the 24 region proposals, out of the approximately 10 million regions in VOC 2007 test, that most strongly activate each of 20 units. Each montage is labeled by the unit’s (y, x, channel) position in the 6×6×256 dimensional pool5 feature map. Each image region is drawn with an overlay of the unit’s receptive ﬁeld in white. The activation value (which we normalize by dividing by the max activation value over all units in a channel) is shown in the receptive ﬁeld’s upper-left corner. Best viewed digitally with zoom.</p><p>图12：我们展示了在VOC2007测试中大约1000万个候选区域中的24个候选区域，其最强烈地激活20个单元中的每一个。每个剪辑都用6∗6∗256维的5pool5特征图的单元（y, x, channel）的位置标记。每一个图像区域都用白色的单元的感受野的覆盖图绘制。激活值（我们通过除以通道中所有单元的最大激活值来进行归一化）显示在接受域的左上角。建议放大来看</p><h3 id="E-Per-category-segmentation-results"><a href="#E-Per-category-segmentation-results" class="headerlink" title="E. Per-category segmentation results"></a><strong>E. Per-category segmentation results</strong></h3><p>In Table 7 we show the per-category segmentation accuracy on VOC 2011 val for each of our six segmentation methods in addition to the O2P method [4]. These results show which methods are strongest across each of the 20 PASCAL classes, plus the background class.</p><p>在表7中，我们展示了我们6个分割方法中的每一个（除了O2PO2P方法）在VOC 2011val集上的每类分割准确度。这些结果展示了对于20个PASCAL类别加上背景类，哪一个方法是最强的。</p><p><img src="file:////tmp/wps-linxu/ksohtml/wpsKfy8ga.jpg" alt="img"> </p><p>Table7: Per-category segmentation accuracy (%) on the VOC 2011 validation set</p><h3 id="F-Analysis-of-cross-dataset-redundancy"><a href="#F-Analysis-of-cross-dataset-redundancy" class="headerlink" title="F. Analysis of cross-dataset redundancy"></a><strong>F. Analysis of cross-dataset redundancy</strong></h3><p>One concern when training on an auxiliary dataset is that there might be redundancy between it and the test set. Even though the tasks of object detection and whole-image classiﬁcation are substantially different, making such cross-set redundancy much less worrisome, we still conducted thorough investigation that quantiﬁes the extent to which PASCAL test images are contained within the ILSVRC 2012 training and validation sets. Our ﬁndings may be useful to researchers who are interested in using ILSVRC 2012 as training data for the PASCAL image classiﬁcation task. We performed two checks for duplicate (and near duplicate) images. The ﬁrst test is based on exact matches of ﬂickr image IDs, which are included in the VOC 2007 test annotations (these IDs are intentionally kept secret for subsequent PASCAL test sets). All PASCAL images, and about half of ILSVRC, were collected from ﬂickr.com. This check turned up 31 matches out of 4952 (0.63%).</p><p>当在辅助数据集上进行训练时，一个问题是它与测试集之间可能存在冗余。即使对象检测和整个图像分类的任务有很大的不同，为了使这种交叉冗余不那么令人担忧，我们仍然进行了彻底的调查，量化了PASCAL测试图像包含在ILSVRC2012训练和验证集的程度。我们发现可能对那些有兴趣使用ILSVRC2012作为PASCAL图像分类任务的训练数据的研究人员有用。我们对重复（和近重复）图像执行了再次检查。第一个测试是基于flicker图像ID的精确匹配，这些ID包括在VOC 2007测试注释中（这些ID有意的为后续的PASCAL测试集保密）。所有的PASCAL图像，和约一半的ILSVRC图像，从flickr.com收集。这个检查证明了在4952有31个匹配（0.63%）。</p><p>The second check uses GIST [30] descriptor matching, which was shown in [13] to have excellent performance at near-duplicate image detection in large (&gt; 1million) image collections. Following [13], we computed GIST descriptors on warped 32×32 pixel versions of all ILSVRC 2012 trainval and PASCAL 2007 test images. Euclidean distance nearest-neighbor matching of GIST descriptors revealed 38 near-duplicate images (including all 31 found by ﬂickr ID matching). The matches tend to vary slightly in JPEG compression level and resolution, and to a lesser extent cropping. These ﬁndings show that the overlap is small, less than 1%. For VOC 2012, because ﬂickr IDs are not available, we used the GIST matching method only. Based on GIST matches, 1.5% of VOC 2012 test images are in ILSVRC 2012 trainval. The slightly higher rate for VOC 2012 is likely due to the fact that the two datasets were collected closer together in time than VOC 2007 and ILSVRC 2012 were.</p><p>第二个检测使用了GIST描述符匹配，在[13]中显示在大（&gt;100万）图像集合中的近似图像检测中具有优异的性能。在[13]之后，我们计算了所有的ILSVRC2012trainval和PASCAL 2007测试图像的扭曲32*32像素版本上的GIST描述符。GIST描述符的欧氏距离最近邻匹配揭示了38个近似重复图像（包括通过flickrID匹配找到的31个）。匹配在JPEG压缩级别和分辨率略有变化，并且趋向较小程度的裁剪。这些发现表明重叠是小的，小于1%。对于VOC 2012来说，因为flickrID是不可用的，我们只使用了GIST匹配方法。基于GIST匹配，VOC 2012测试图像的1.5%是在ILSVRC 2012trainval中的。对于VOC 2012略高的比率可能是由于这两个数据集在时间上收集的比VOC 2007和ILSVRC 2012更接近。</p><h3 id="G-Document-change-log"><a href="#G-Document-change-log" class="headerlink" title="G. Document change log"></a><strong>G. Document change log</strong></h3><p>This document tracks the progress of R-CNN. To help readers understand how it has changed over time, here’s a brief change log describing the revisions.</p><p><strong>v1</strong> Initial version. </p><p><strong>v2</strong> CVPR 2014 camera-ready revision. Includes substantial improvements in detection performance brought about by(1) starting ﬁne-tuning from a higher learning rate(0.001 instead of 0.0001), (2) using context padding when preparing CNN inputs, and (3) bounding-box regression to ﬁx localization errors. </p><p><strong>v3</strong> Results on the ILSVRC2013 detection dataset and comparison with OverFeat were integrated into several sections (primarily Section 2 and Section 4).</p><p><strong>v4</strong> The softmax vs. SVM results in Appendix B contained an error, which has been ﬁxed. We thank Sergio Guadarrama for helping to identify this issue. </p><p><strong>v5</strong> Added results using the new 16-layer network architecture from Simonyan and Zisserman [43] to Section 3.3 and Table 3.</p><p>本文跟踪的R-CNN的进度。为了帮助读者它是怎样随着时间改变的，这里是一个简明的更改日志描述了这些修定。<br><em><strong>*V1*</strong></em> 最初的版本<br><em><strong>*V2*</strong></em> CVPR相机就绪版本。包括在检测性能上的大量改进，由以下带来的：（1）以更高的学习率开始微调（0.001而不是0.0001），（2）当准备CNN时使用上下文填充，（3）边界框回归用于修复定位误差。<br><em><strong>*V3*</strong></em> 在ILSVRC 2013检测数据集上的结果，并且与OverFeat的比较被集成到几个章节内（主要在第2和第4节）。<br><em><strong>*V4*</strong></em> 在附录B上的softmax对SVM的结果包含了一个错误，已经被修复了。我们要感谢Sergio Guadarrama帮助指出了这个问题。<br><em><strong>*V5*</strong></em> 将使用Simonyan and Zisserman 16层网络结构得到的结果添加到了3.3节，和表3中。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper_Read </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A_Single-Shot_Object_Detector_based_on_Multi_Level_Feature_Pyramid_Network</title>
      <link href="/blog/2022/04/29/a-single-shot-object-detector-based-on-multi-level-feature-pyramid-network/"/>
      <url>/blog/2022/04/29/a-single-shot-object-detector-based-on-multi-level-feature-pyramid-network/</url>
      
        <content type="html"><![CDATA[<h1 id="M2Det-A-Single-Shot-Object-Detector-based-on-Multi-Level-Feature-Pyramid-Network"><a href="#M2Det-A-Single-Shot-Object-Detector-based-on-Multi-Level-Feature-Pyramid-Network" class="headerlink" title="M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network"></a><strong>M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</strong></h1><p>QijieZhao1, TaoSheng1, YongtaoWang1∗, ZhiTang1, YingChen2, LingCai2 and HaibinLing3 </p><p>1 Institute of Computer Science and Technology, Peking University, Beijing, P.R. China </p><p>2 AI Labs, DAMO Academy, Alibaba Group </p><p>3 Computer and Information Sciences Department, Temple University {zhaoqijie, shengtao, wyt, tangzhi}@pku.edu.cn, {cailing.cl, chenying.ailab}@alibaba-inc.com, {hbling}@temple.edu</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a><strong>Abstract</strong></h2><p>Feature pyramids are widely exploited by both the state-of the-art one-stage object detectors (e.g., DSSD, RetinaNet, ReﬁneDet) and the two-stage object detectors (e.g., Mask RCNN, DetNet) to alleviate the problem arising from scale variation across object instances. Although these object detectors with feature pyramids achieve encouraging results, they have some limitations due to that they only simply construct the feature pyramid according to the inherent multiscale, pyramidal architecture of the backbones which are originally designed for object classiﬁcation task. Newly, in this work, we present Multi-Level Feature Pyramid Network (MLFPN) to construct more effective feature pyramids for detecting objects of different scales. First, we fuse multi-level features (i.e. multiple layers) extracted by backbone as the base feature. Second, we feed the base feature into a block of alternating joint Thinned U-shape Modules and Feature Fusion Modules and exploit the decoder layers of each U shape module as the features for detecting objects. Finally, we gather up the decoder layers with equivalent scales(sizes) to develop a feature pyramid for object detection, in which every feature map consists of the layers (features) from multiple levels. To evaluate the effectiveness of the proposed MLFPN, we design and train a powerful end-to-end one stage object detector we call M2Det by integrating it into the architecture of SSD, and achieve better detection performance than state-of-the-art one-stage detectors. Speciﬁcally, on MS-COCO benchmark, M2Det achieves AP of 41.0 at speed of 11.8 FPS with single-scale inference strategy and AP of 44.2 with multi-scale inference strategy, which are the new state-of-the-art results among one-stage detectors. The code will be made available on <a href="https://github.com/qijiezhao/M2Det">https://github.com/qijiezhao/M2Det</a>.</p><p><strong>摘要</strong>：现在性能较好的一阶段物体探测器（如DSSD，RetinaNet，RefineDet）和两阶段物体探测器（如Mask RCNN，DetNet）都广泛使用了特征金字塔，从而缓解对象实例的比例大小变化带来的差异问题。尽管这些具有特征金字塔的物体探测器获得了不错的结果，但它们也有一定的局限性：它们只是简单地根据内在的多尺度构造特征金字塔，这种骨干网络的金字塔架构本是为了分类任务而设计。最近，在这项工作中，我们提出了多层次特征金字塔网络（MLFPN）来构建更有效的特征金字塔，用于检测不同尺度的对象。<strong>首先</strong>，我们融合由骨干网络提取的多级特征（即多层）作为基本特征。<strong>然后</strong>，我们将上述基本特征送入一组交替连接的简化U形模块和特征融合模块，并利用每个U形模块的解码器层作为检测对象的特征。<strong>最后</strong>，我们将具有等效尺度(大小)的解码器层集合（组合）起来，形成一个用于目标检测的特征金字塔，其中每个特征图由多个层次的层(特征)组成。为了评估所提出的MLFPN的有效性，我们设计并训练了一个功能强大的端到端一阶段物体探测器，将其集成到SSD架构中，我们称为M2Det，获得了比现有技术更好的检测性能。具体而言，在MS-COCO基准测试中，M2Det采用单尺度推理策略时，以11.8 FPS的速度实现了41.0的AP，当使用多尺度推理策略时，AP为44.2。这是一种新的最先进一阶段探测器。该代码将在<a href="https://github.com/qijiezhao/M2Det%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/qijiezhao/M2Det上提供。</a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a><strong>Introduction</strong></h2><p>Scale variation across object instances is one of the major challenges for the object detection task (Lin et al. 2017a; He etal.2015; Singh and Davis2018), and usually there are two strategies to solve the problem arising from this challenge. The ﬁrst one is to detect objects in an image pyramid (i.e. a series of resized copies of the input image)( Singh and Davis2018), which can only be exploited at the testing time. Obviously, this solution will greatly increase memory and computational complexity, thus the efﬁciency of such object detectors drop dramatically. The second one is to detect objects in a feature pyramid extracted from the input image (Liu et al. 2016; Lin et al. 2017a), which can be exploited at both training and testing phases. Compared with the ﬁrst solution that uses an image pyramid, it has less memory and computational cost. Moreover, the feature pyramid constructing module can be easily integrated into the state-of the-art Deep Neural Networks based detectors, yielding an end-to-end solution.</p><p>实例之间的尺度差异是对象检测任务的主要挑战之一（Lin等人2017a; He etal.2015; Singh和Davis2018），通常有两种策略来解决由此挑战引起的问题。第一种是检测<strong>图像金字塔</strong>中的对象（即一系列已调整输入图像大小的副本）（Singh和Davis2018），这种方法只能在测试时使用。显然，这种解决方案将大大增加内存和计算复杂性，因此这种对象检测器的效率急剧下降。第二种是检测从输入图像中提取的<strong>特征金字塔</strong>中的对象（Liu et al.2016; Lin et al.2017a），可以在训练和测试阶段进行利用。与使用图像金字塔的第一种解决方案相比，第二种方案需要更少的内存和计算成本。此外，特征金字塔构建模块可以很容易地嵌入到基于深度神经网络的最先进检测器中，从而产生端到端的解决方案。</p><p>Although the object detectors with feature pyramids (Liu et al. 2016; Lin et al. 2017a; Lin et al. 2017b; He et al. 2017) achieve encouraging results, they still have some limitations due to that they simply construct the feature pyramid according to the inherent multi-scale, pyramidal architecture of the backbones which are actually designed for object classiﬁcation task. For example, as illustrated in Fig. 1, SSD (Liu et al. 2016) directly and independently uses two layers of the backbone (i.e. VGG16) and four extra layers obtained by stride 2 convolution to construct the feature pyramid; STDN (Zhou et al. 2018) only uses the last dense block of DenseNet (Huang et al. 2017) to construct feature pyramid by pooling and scale-transfer operations; FPN (Lin et al. 2017a) constructs the feature pyramid by fusing the deep and shallow layers in a top-down manner. Generally speaking, the above-mentioned methods have the two following limitations. First, feature maps in the pyramid are not representative enough for the object detection task, instead they are simply constructed from the layers (features) of the backbone designed for object classiﬁcation task. Second, each feature map in the pyramid(used for detecting objects in a speciﬁc range of size) is mainly or even solely constructed from single-level layers of the backbone, that is, it mainly or only contains single-level information. In general, high-level features in the deeper layers are more discriminative for classiﬁcation subtask while low-level features in the shallower layers can be helpful for object location regression sub-task. Moreover, low-level features are more suitable to characterize objects with simple appearances while high-level features are appropriate for objects with complex appearances. In practice, the appearances of the object instances with similar size can be quite different. For example, a trafﬁc light and a faraway person may have comparable size, and the appearance of the person is much more complex. Hence, each feature map (used for detecting objects in a speciﬁc range of size) in the pyramid mainly or only consists of single-level features will result in suboptimal detection performance.</p><p>尽管具有特征金字塔的物体检测器（Liu等人2016; Lin等人2017a; Lin等人2017b; He等人2017）取得了不错的结果，但由于他们只是根据内在多尺度金字塔结构的骨架网络构建了特征金字塔，这种骨架网络实际上是为物体分类任务设计的，因此这些方法仍然存在一些局限性。例如，如图1所示，SSD（Liu等人，2016）直接单独使用两层骨架网络的特征（即VGG16）和通过步幅为2的卷积获得的四个额外层来构建特征金字塔; STDN（Zhou et al.2018）仅使用DenseNet的最后一个Dense块（Huang et al.2017），通过池化和尺度变换操作构建特征金字塔; FPN（Lin et al.2017a）通过以自上而下的方式融合深层和浅层的特征来构造特征金字塔。一般而言，上述方法具有以下<strong>两个限制</strong>。<strong>首先</strong>，金字塔中的特征图对于对象检测任务而言不够典型（表达能力不够），它们只是简单地从为对象分类任务设计的骨干网络的层（特征）中构造。<strong>其次</strong>，金字塔中的每个特征图（用于检测特定大小范围内的对象）主要或甚至仅从骨干网络的单层构建，即，它主要或仅包含单层信息。<strong>通常，较深层中的高级特征对分类子任务更具区别性，而较浅层中的低级特征可有助于对象位置回归子任务。此外****，低级特征更适合于表征具有简单外观的对象，而高级特征适合于具有复杂外观的对象。</strong>实际上，具有相似大小的对象实例的外观可能完全不同。例如，交通灯和遥远的人可能具有相当的尺寸，但是人的外观要复杂得多。因此，金字塔中的每个特征图（用于检测特定尺寸范围内的对象）主要或仅由单级特征组成将致使检测性能欠佳。</p><p><img src="/blog/blog/2022/04/29/a-single-shot-object-detector-based-on-multi-level-feature-pyramid-network/A-Single-Shot-Object-Detector-based-on-Multi-Level-Feature-Pyramid-Network.assets/1571518-20220429213547342-1276487038.png"></p><p>The goal of this paper is to construct a more effective feature pyramid for detecting objects of different scales, while avoid the limitations of the existing methods as above mentioned. As shown in Fig.2, to achieve this goal, we ﬁrst fuse multi-level features (i.e. multiple layers) extracted by backbone as base feature, and then feed it into a block of alternating joint Thinned U-shape Modules (TUM) and Feature Fusion Modules (FFM)to extract more representative, multilevel multi-scale features. It is worth noting that, decoder layers in each U-shape Module share a similar depth. Finally, we gather up the feature maps with equivalent scales to construct the ﬁnal feature pyramid for object detection. Obviously, decoder layers that form the ﬁnal feature pyramid are much deeper than the layers in the backbone, namely, they are more representative. Moreover, each feature map in the ﬁnal feature pyramid consists of the decoder layers from multiple levels. Hence, we call our feature pyramid block Multi-Level Feature Pyramid Network (MLFPN).</p><p>本文的目的是构建一个更有效的特征金字塔，用于检测不同尺度的物体，同时避免上述现有方法的局限性。如图2所示，为了实现这个目标，我们首先融合由骨干网络提取的多级特征（即多个层）作为基本特征，然后将其馈送到交替连接的简化U形模块（TUM）和特征融合模块（FFM），从而提取更具代表性的多级多尺度特征。值得注意的是，每个U形模块中的解码器层共享相似的深度。最后，我们收集（组合，融合）具有等效尺度的特征图，以构建用于对象检测的最终特征金字塔。显然，形成最终特征金字塔的解码器层比骨干中的层深得多，即它们更具代表性。此外，最终特征金字塔中的每个特征图都包含来自多个级别的解码器层。因此，我们将我们的特征金字塔块称为多级特征金字塔网络（MLFPN）。</p><p>To evaluate the effectiveness of the proposed MLFPN, we design and train a powerful end-to-end one-stage object detector we call M2Det (according to that it is built up on multilevel and multi-scale features) by integrating MLFPN into the architecture of SSD (Liu et al. 2016). M2Det achieves the new state-of-the-art result (i.e. AP of 41.0 at speed of 11.8 FPS with single-scale inference strategy and AP of 44.2 with multi-scale inference strategy), outperforming the one stage detectors on MS-COCO (Lin et al. 2014) benchmark.</p><p>为了评估所提出的MLFPN的有效性，我们将MLFPN嵌入到SSD（Liu et al.2016）架构中来设计和训练一个功能强大的端到端一阶段对象检测器，我们称之为M2Det（根据它建立在多级和多尺度特征上）。 M2Det实现了最新的最佳结果（即AP为41.0，速度为11.8 FPS，采用单尺度推理策略，AP为44.2，具有多尺度推理策略），优于MS-COCO上的一级探测器（ 林等人，2014）的基准。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a><strong>Related Work</strong></h2><p>Researchers have put plenty of efforts into improving the detection accuracy of objects with various scales – no matter what kind of detector it is, either an one-stage detector or a two-stage one. To the best of our knowledge, there are mainly two strategies to tackle this scale-variation problem.</p><p>研究人员已经投入了大量精力来提高不同尺度物体的探测精度（无论是哪种探测器，无论是一阶段检测器还是两阶段检测器）。 据我们所知，主要有两种策略来解决这种尺度差异问题。</p><p>The ﬁrst one is featurizing image pyramids (i.e. a series of resized copies of the input image) to produce semantically representative multi-scale features. Features from images of different scales yield predictions separately and these predictions work together to give the ﬁnal prediction. In terms of recognition accuracy and localization precision, features from various-sized images do surpass features that are based merely on single-scale images. Methods such as (Shrivastava et al. 2016) and SNIP (Singh and Davis 2018) employed this tactic. Despite the performance gain, such a strategy could be costly time-wise and memory-wise, which forbid its application in real-time tasks. Considering this major drawback, methods such as SNIP (Singh and Davis 2018) can choose to only employ featurize image pyramids during the test phase as a fallback, whereas other methods including Fast R-CNN (Girshick 2015) and Faster R-CNN (Ren et al. 2015) chose not to use this strategy by default.</p><p>第一个是利用图像金字塔（即一系列调整输入图像大小的副本）以产生具有语义代表性的多尺度特征。 来自不同尺度的图像的特征分别产生预测，并且这些预测一起工作（最后融合）以给出最终预测。 在识别精度和定位精度方面，来自各种尺寸图像的特征确实超越仅基于单尺度图像的特征。 诸如（Shrivastava等人2016）和SNIP（Singh和Davis 2018）之类的方法采用了这种策略。 尽管性能提升，但这样的策略在时间和内存方面耗费较大，这限制了其在实时任务中的应用。 考虑到这个主要缺点，像SNIP（Singh和Davis 2018）这样的方法可以选择在测试阶段仅使用图像特征金字塔作为后备手段，而其他方法包括Fast R-CNN（Girshick 2015）和Faster R-CNN（Ren） 等人，2015）默认选择不使用此策略。</p><p>The second one is detecting objects in the feature pyramid extracted from inherent layers within the network while merely taking a single-scale image. This strategy demands signiﬁcantly less additional memory and computational cost than the ﬁrst one, enabling deployment during both the training and test phases in real-time networks. Moreover, the feature pyramid constructing module can be easily revised and ﬁt into state-of-the-art Deep Neural Networks based detectors. MS-CNN(Caietal.2016), SSD(Liuetal.2016), DSSD(Fu et al. 2017), FPN (Lin et al. 2017a), YOLOv3 (Redmon and Farhadi 2018), RetinaNet (Lin et al. 2017b), and ReﬁneDet (Zhang et al. 2018) adopted this tactic in different ways.</p><p>第二种是从网络内的固有层提取的金字塔特征中检测对象，同时仅采用单一尺度的图像。 与第一种策略相比，该策略需要的内存和计算成本显着降低，从而可以在实时网络的训练和测试阶段进行部署。 此外，特征金字塔构建模块可以很容易地修改并嵌入最先进的基于深度神经网络的检测器。 MS-CNN（Caietal.2016），SSD（Liuetal.2016），DSSD（Fu等人2017），FPN（Lin等人2017a），YOLOv3（Redmon和Farhadi 2018），RetinaNet（Lin等人2017b） 和RefineDet（Zhang et al.2018）以不同的方式采用了这种策略。</p><p>To the best of our knowledge, MS-CNN (Cai et al. 2016) proposed two sub-networks and ﬁrst incorporated multiscale features into deep convolutional neural networks for object detection. The proposal sub-net exploited feature maps of several resolutions to detect multi-scale objects in an image. SSD(Liuetal.2016) exploited feature maps from the later layers of VGG16 base-net and extra feature layers for predictions at multiple scales. FPN (Lin et al. 2017a) utilized lateral connections and a top-down pathway to produce a feature pyramid and achieved more powerful representations. DSSD (Fu et al. 2017) implemented deconvolution layers for aggregating context and enhancing the high level semantics for shallow features. ReﬁneDet (Zhang et al. 2018) adopted two-step cascade regression, which achieves a remarkable progress on accuracy while keeping the efﬁciency of SSD.</p><p>据我们所知，MS-CNN（Cai等人，2016）提出了两个子网络，首先将多尺度特征结合到用于物体检测的深度卷积神经网络中。 提议子网利用几种分辨率的特征图来检测图像中的多尺度对象。SSD（Liuetal.2016）利用来自VGG16基础网络的后面层和额外特征层的特征图进行多尺度预测。FPN（Lin et al.2017a）利用横向连接和自上而下的路径来产生特征金字塔并实现更强大的表示。DSSD（Fu et al.2017）实现了反卷积层，用于聚合上下文并增强浅层特征的高级语义。RefineDet（Zhang etal.2018）采用了两步级联回归，在保持SSD效率的同时，在准确性方面取得了显着进步。</p><h2 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a><strong>Proposed Method</strong></h2><p>The overall architecture of M2Det is shown in Fig.2. M2Det uses the backbone and the Multi-Level Feature Pyramid Network (MLFPN) to extract features from the input image, and then similar to SSD, produces dense bounding boxes and category scores based on the learned features, followed by the non-maximum suppression (NMS) operation to produce the ﬁnal results. MLFPN consists of three modules, i.e. Feature Fusion Module (FFM), Thinned U-shape Module (TUM) and Scale-wise Feature Aggregation Module (SFAM). FFMv1 enriches semantic information into base features by fusing feature maps of the backbone. Each TUM generates a group of multi-scale features, and then the alternating joint TUMs and FFMv2s extract multi-level multiscale features. In addition, SFAM aggregates the features into the multi-level feature pyramid through a scale-wise feature concatenation operation and an adaptive attention mechanism. More details about the three core modules and network conﬁgurations in M2Det are introduced in the following.</p><p>M2Det的整体架构如图2所示。 <strong>M2Det使用骨干网和多级特征金字塔网络（MLFPN）从输入图像中提取特征，然后类似于SSD，根据学习的特征生成密集的边界框和类别分数，<strong><strong>最</strong></strong>后是非最大抑制（ NMS）操作以产生最终结果</strong>。 MLFPN由三个模块组成，即特征融合模块（FFM），简化的U形模块（TUM）和按基于尺度的特征聚合模块（SFAM）。 FFMv1通过融合骨干网络的特征图，将语义信息丰富为基本特征。每个TUM生成一组多尺度特征，然后交替连接的TUM和FFMv2提取多级多尺度特征。此外，SFAM通过按比例缩放的特征连接操作和自适应注意机制将特征聚合到多级特征金字塔中。下面介绍有关M2Det中三个核心模块和网络配置的更多详细信息。</p><p><img src="/blog/blog/2022/04/29/a-single-shot-object-detector-based-on-multi-level-feature-pyramid-network/A-Single-Shot-Object-Detector-based-on-Multi-Level-Feature-Pyramid-Network.assets/1571518-20220429213522543-1152371227.png"></p><p>Figure 2: An overview of the proposed M2Det(320×320). M2Det utilizes the backbone and the Multi-level Feature Pyramid Network (MLFPN) to extract features from the input image, and then produces dense bounding boxes and category scores. In MLFPN, FFMv1 fuses feature maps of the backbone to generate the base feature. Each TUM generates a group of multi-scale features, and then the alternating joint TUMs and FFMv2s extract multi-level multi-scale features. Finally, SFAM aggregates the features into a multi-level feature pyramid. In practice, we use 6 scales and 8 levels mostly.</p><p>图2：M2Det（320×320）的概述。 M2Det利用骨干网和多级特征金字塔网络（MLFPN）从输入图像中提取特征，然后生成密集的边界框和类别分数。 在MLFPN中，FFMv1融合骨干的特征图以生成基本特征。 每个TUM生成一组多尺度特征，然后交替连接的TUM和FFMv2s提取多级多尺度特征。 最后，SFAM将特征聚合为多级特征金字塔。 在实践中，我们主要使用6个等级和8个等级。</p><p>Multi-level Feature Pyramid Network As shown in Fig. 2, MLFPN contains three parts. Firstly, FFMv1 fuses shallow and deep features to produce the base feature, e.g., conv4 3 and conv5 3 of VGG (Simonyan and Zisserman 2015), which provide multi-level semantic information for MLFPN. Secondly, several TUMs and FFMv2 are stacked alternately. Specially, each TUM generates several feature maps with different scales. The FFMv2 fuses the base feature and the largest output feature map of the previous TUM. And the fused feature maps are fed to the next TUM. Note that the ﬁrst TUM has no prior knowledge of any other TUMs, so it only learns from Xbase. The output multi-level multi-scale features are calculated as:</p><p>如图2所示的多级特征金字塔网络，MLFPN包含三个部分。 <strong>首先</strong>，FFMv1融合浅和深的特征以产生基本特征，例如VGG的conv4 3和conv5 3（Simonyan和Zisserman 2015），其为MLFPN提供多级语义信息。 <strong>其次</strong>，交替堆叠多个TUM和FFMv2。 特别地，每个TUM生成具有不同尺度的若干特征图。 FFMv2融合了基本特征和前一个TUM的最大输出特征图。<strong>最后</strong>，融合的特征图被馈送到下一个TUM。 请注意，第一个TUM没有任何其他TUM的先验知识，因此它只从Xbase学习。 输出多级多尺度特征计算如下：</p><p><img src="/blog/blog/2022/04/29/a-single-shot-object-detector-based-on-multi-level-feature-pyramid-network/A-Single-Shot-Object-Detector-based-on-Multi-Level-Feature-Pyramid-Network.assets/1571518-20220429213453284-138359457.png"></p><p>where Xbase denotes the base feature, xli denotes the feature with the i-th scale in the l-th TUM, L denotes the number of TUMs, Tl denotes the l-th TUM processing, and F denotes FFMv1 processing. Thirdly, SFAM aggregates the multi-level multi-scale features by a scale-wise feature concatenation operation and a channel-wise attention mechanism.</p><p>其中Xbase表示基本特征，xli表示在第l个TUM中具有第i个尺度的特征，L表示TUM的数量，T1表示第l个TUM处理，并且F表示FFMv1处理。 第三，SFAM通过按比例缩放的特征连接操作和通道注意力机制来聚合多级多尺度特征。</p><h3 id="FFMs"><a href="#FFMs" class="headerlink" title="FFMs"></a><strong>FFMs</strong></h3><p>FFMs fuse features from different levels in M2Det, which are crucial to constructing the ﬁnal multi-level feature pyramid. They use 1x1 convolution layers to compress the channels of the input features and use concatenation operation to aggregate these feature maps. Especially, since FFMv1takes two feature maps with different scales in backbone as input, it adopts one upsample operation to rescale the deep features to the same scale before the concatenation operation. Meanwhile, FFMv2 takes the base feature and the largest output feature map of the previous TUM –these two are of the same scale – as input, and produces the fused feature for the next TUM. Structural details of FFMv1 and FFMv2 are shown in Fig. 4 (a) and (b), respectively.</p><p>FFM融合了M2Det中不同层次的特征，这对于构建最终的多级特征金字塔至关重要。 它们使用1x1卷积层来压缩输入特征的通道，并使用连接操作来聚合这些特征图。 特别是，由于FFMv1以骨干网络中不同比例的两个特征图作为输入，因此它采用一个上采样操作，在连接操作之前将深度特征重新缩放到相同的尺度。 同时，FFMv2采用基本特征和前一个TUM的最大输出特征图 - 这两个具有相同的比例 - 作为输入，并产生下一个TUM的融合特征。 FFMv1和FFMv2的结构细节分别如图4（a）和（b）所示。</p><p><img src="/blog/blog/2022/04/29/a-single-shot-object-detector-based-on-multi-level-feature-pyramid-network/A-Single-Shot-Object-Detector-based-on-Multi-Level-Feature-Pyramid-Network.assets/1571518-20220429213415208-1696828853.png"></p><p>Figure 4: Structural details of some modules. (a) FFMv1, (b)FFMv2,(c)TUM. The inside numbers of each block denote: input channels, Conv kernel size, stride size, output channels.</p><p>图4：一些模块的结构细节。 （a）FFMv1，（b）FFMv2，（c）TUM。 每个块的内部数字表示：输入通道，Conv内核大小，步幅大小，输出通道。</p><h3 id="TUMs"><a href="#TUMs" class="headerlink" title="TUMs"></a><strong>TUMs</strong></h3><p>Different from FPN (Lin et al. 2017a) and RetinaNet (Lin et al. 2017b), TUM adopts a thinner U-shape structure as illustrated in Fig. 4 (c). The encoder is a series of 3x3 convolution layers with stride 2. And the decoder takes the outputs of these layers as its reference set of feature maps, while the original FPN chooses the output of the last layer of each stage in ResNet backbone. In addition, we add 1x1 convolution layers after upsample and elementwise sum operation at the decoder branch to enhance learning ability and keep smoothness for the features (Lin, Chen, and Yan 2014). All of the outputs in the decoder of each TUM form the multi-scale features of the current level. As a whole, the outputs of stacked TUMs form the multi-level multi-scale features, while the front TUM mainly provides shallow-level features, the middle TUM provides medium level features, and the back TUM provides deep-level features.</p><p>TUM不同于FPN（Lin等人2017a）和RetinaNet（Lin等人2017b），TUM采用简化的U形结构，如图4（c）所示。 编码器是一系列3x3，步长为2的卷积层.并且解码器将这些层的输出作为其参考特征集，而原始FPN选择ResNet主干网络中每个阶段的最后一层的输出。 此外，我们在解码器分支的上采样层后添加1x1卷积层和按元素求和的操作，以增强学习能力并保持特征的平滑性（Lin，Chen和Yan 2014）。 每个TUM的解码器中的所有输出形成当前级别的多尺度特征。 整体而言，堆叠TUM的输出形成多层次多尺度特征，而前TUM主要提供浅层特征，中间TUM提供中等特征，后TUM提供深层特征。</p><h3 id="SFAM"><a href="#SFAM" class="headerlink" title="SFAM"></a><strong>SFAM</strong></h3><p>SFAM aims to aggregate the multi-level multiscale features generated by TUMs into a multi-level feature pyramid as shown in Fig. 3. The ﬁrst stage of SFAM is to concatenate features of the equivalent scale together along the channel dimension. The aggregated feature pyramid can be presented as X = [X1,X2,…,Xi], where Xi = Concat(x1i,x2i,…,xLi ) ∈ RWi×Hi×C refers to the features of the i-th largest scale. Here, each scale in the aggregated pyramid contains features from multi-level depths. However, simple concatenation operations are not adaptive enough. In the second stage, we introduce a channel-wise attention module to encourage features to focus on channels that they beneﬁt most. Following SE block (Hu, Shen, and Sun 2017), we use global average pooling to generate channel-wise statistics z ∈ RC at the squeeze step. And to fully capture channel-wise dependencies, the following excitation step learns the attention mechanism via two fully connected layers:</p><p>SFAM旨在将由TUM生成的多级多尺度特征聚合成多级特征金字塔，如图3所示.SFAM的第一阶段是沿着信道维度将等效尺度的特征连接在一起。 聚合特征金字塔可以表示为X = [X1,X2,…,Xi]，其中Xi = Concat(x1i,x2i,…,xLi ) ∈ RWi×Hi×C指的是尺度第i个最大的特征。 这里，聚合金字塔中的每个比例都包含来自多级深度的特征。 但是，简单的连接操作不太适合。 在第二阶段，我们引入了通道注意模块，以促使特征集中在最有益的通道。 在SE区块（Hu，Shen和Sun 2017）之后，我们使用全局平均池化来在挤压步骤中生成通道统计z∈RC。 为了完全捕获通道依赖性，以下激励步骤通过两个完全连接的层学习注意机制：</p><p><img src="/blog/blog/2022/04/29/a-single-shot-object-detector-based-on-multi-level-feature-pyramid-network/A-Single-Shot-Object-Detector-based-on-Multi-Level-Feature-Pyramid-Network.assets/1571518-20220429213345004-1004696163.png"></p><p>Figure 3: Illustration of Scale-wise Feature Aggregation Module. The ﬁrst stage of SFAM is to concatenate features with equivalent scales along channel dimension. Then the second stage uses SE attention to aggregate features in an adaptive way.</p><p>图3：按缩放比例的特征聚合模块的图示。 SFAM的第一阶段是沿着信道维度连接具有等效比例的特征。 然后第二阶段使用SE注意机制以自适应方式聚合特征。</p><p><img src="/blog/blog/2022/04/29/a-single-shot-object-detector-based-on-multi-level-feature-pyramid-network/A-Single-Shot-Object-Detector-based-on-Multi-Level-Feature-Pyramid-Network.assets/1571518-20220429213319447-230540625.png"></p><p>where σ refers to the ReLU function, δ refers to the sigmoid function, W1 ∈ RC/r ×C , W2∈RC×C /r , r is the reduction ratio (r = 16 in our experiments). The ﬁnal output is obtained by reweighting the input X with activation s:</p><p><img src="/blog/blog/2022/04/29/a-single-shot-object-detector-based-on-multi-level-feature-pyramid-network/A-Single-Shot-Object-Detector-based-on-Multi-Level-Feature-Pyramid-Network.assets/1571518-20220429213257879-297175714.png"></p><p>where ˜ Xi = [˜ X1i, ˜ X2i,…, ˜ XCi ], each of the features is enhanced or weakened by the rescaling operation.</p><p>其中σ表示RELU激活函数，δ为sigmoid函数，W1 ∈ RC/r ×C , W2∈RC×C /r ,r是减少的比例（实验中为16）. 最后通过激活s对输入X重新加权得到输出:</p><h2 id="Network-Conﬁgurations"><a href="#Network-Conﬁgurations" class="headerlink" title="Network Conﬁgurations"></a><strong>Network Conﬁgurations</strong></h2><p>We assemble M2Det with two kinds of backbones (Simonyan and Zisserman 2015; He et al. 2016). Before training the whole network, the backbones need to be pre-trained on the ImageNet 2012 dataset(Russakovskyetal.2015).All of the default conﬁgurations of MLFPN contain 8 TUMs, each TUM has 5 striding-Convs and 5 Upsample operations, so it will output features with 6 scales. To reduce the number of parameters, we only allocate 256 channels to each scale of their TUM features, so that the network could be easy to train on GPUs. As for input size, we follow the original SSD, ReﬁneDet and RetinaNet, i.e., 320, 512 and 800.</p><p>我们用两种主干网络组装M2Det（Simonyan和Zisserman 2015; He等人2016）。 在训练整个网络之前，骨干网络需要在ImageNet 2012数据集上进行预训练（Russakovsky et al.2015）.MLFPN的所有默认配置包含8个TUM，每个TUM有5个跨步卷积（编码器）和5个Upsample操作，因此它将输出6种尺度的特征。 为了减少参数的数量，我们只为其TUM功能的每个比例分配256个通道，以便网络可以很容易地在GPU上训练。 至于输入尺寸，我们遵循原始SSD，RefineDet和RetinaNet，即320,512和800。</p><p>At the detection stage, we add two convolution layers to each of the 6 pyramidal features to achieve location regression and classiﬁcation respectively. The detection scale ranges of the default boxes of the six feature maps follow the setting of the original SSD. And when input size is 800×800, the scale ranges increase proportionally except keeping the minimum size of the largest feature map. At each pixel of the pyramidal features, we set six anchors with three ratios entirely. Afterward, we use a probability score of 0.05 as threshold to ﬁlter out most anchors with low scores. Then we use soft-NMS (Bodlaetal.2017) with a linear kernel for post-processing, leaving more accurate boxes. Decreasing the threshold to 0.01 can generate better detection results, but it will slow down the inference time a lot, we do not consider it for pursuing better practical values.</p><p>在检测阶段，我们为6个金字塔特征中的每一个添加两个卷积层，以分别实现位置回归和分类。 六个特征图的默认框的检测比例范围遵循原始SSD的设置。 当输入尺寸为800×800时，除了保持最大特征图的最小尺寸外，比例范围按比例增加。 在金字塔特征的每个像素处，我们设置六个锚点，完全具有三个比率。 之后，我们使用0.05的概率分数作为阈值来筛选出分数较低的大多数锚点。 然后我们使用带有线性内核的soft-NMS（Bodlaetal.2017）进行后处理，留下更精确的盒子。 将阈值降低到0.01可以产生更好的检测结果，但它会大大减慢推理时间，我们不认为这样会有更好的实用价值。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a><strong>Experiments</strong></h2><p>In this section, we present experimental results on the bounding box detection task of the challenging MS-COCO benchmark. Following the protocol in MS-COCO, we use the trainval35k set for training, which is a union of 80k images from train split and a random 35 subset of images from the 40k image val split. To compare with state-of the-art methods, we report COCO AP on the test-dev split, which has no public labels and requires the use of the evaluation server. And then, we report the results of ablation studies evaluated on the mini val split for convenience.</p><p>在本节中，我们将介绍具有挑战性的MS-COCO边界框检测任务基准测试的实验结果。 遵循MS-COCO中的协议，我们使用trainval35k集进行训练，这是来自训练集分割的80k图像和来自40k验证集图像分割的随机35个图像子集的并集。 为了与最先进的方法进行比较，我们在test-dev split上报告COCO AP，它没有公开标签，需要使用评估服务器。 然后，为方便起见，我们报告了在小的验证集中评估消融的研究结果。</p><p>Our experiment section includes 4 parts: (1) introducing implement details about the experiments; (2) demonstrating the comparisons with state-of-the-art approaches; (3) ablation studies about M2Det; (4) comparing different settings about the internal structure of MLFPN and introducing several version of M2Det.</p><p>我们的实验部分包括4个部分：（1）介绍实验的工具细节; （2）展示与最先进方法的比较; （3）关于M2Det的消融研究; （4）比较MLFPN内部结构的不同设置，并介绍几种版本的M2Det。</p><h3 id="Implementation-details"><a href="#Implementation-details" class="headerlink" title="Implementation details"></a><strong>Implementation details</strong></h3><p>For all experiments based on M2Det, we start training with warm-up strategy for 5 epochs, initialize the learning rate as 2×10−3, and then decrease it to 2×10−4 and 2×10−5 at 90 epochs and 120 epochs, and stop at 150 epochs. M2Det is developed with PyTorch v0.4.01. When input size is 320and 512, we conduct experiments on a machine with 4 NVIDIA Titan X GPUs, CUDA 9.2 and cuDNN 7.1.4, while for input size of 800, we train the network on NVIDIA Tesla V100 to get results faster. The batch size is set to 32 (16 each for 2 GPUs, or 8 each for 4 GPUs). On NVIDIA Titan Xp that has 12 GB memory, the training performance is limited if batch size on a single GPU is less than 5. Notably, for Resnet101, M2Det with the input size of 512 is not only limited in the batch size (only 4 is available), but also takes a long time to train, so we train it on V100.</p><p>对于基于M2Det的所有实验，我们用5个epoch的预热策略开始进行训练，将学习率初始化为2×10-3，然后在90和120个epoch时将其降低到2×10-4和2×10-5。在150个epoch时停止训练。 M2Det是使用PyTorch v0.4.01开发的。 当输入大小为320和512时，我们在具有4个NVIDIA Titan X GPU，CUDA 9.2和cuDNN 7.1.4的机器上进行实验，而对于800的输入大小，我们在NVIDIA Tesla V100上训练网络以更快地获得结果。 批量大小设置为32（2个GPU各16个，4个GPU各8个）。 在具有12 GB内存的NVIDIA Titan Xp上，如果单个GPU上的批量大小小于5，则训练性能有限。值得注意的是，对于Resnet101，输入大小为512的M2Det不仅限于批量大小（仅限4个） 可用），但也需要很长时间训练，所以我们在V100上进行训练。</p><p>For training M2Det with the VGG-16 backbone when input size is 320×320 and 512×512 on 4 Titan X devices, the total training time costs 3 and 6 days respectively, and with the ResNet-101 backbone when 320×320 costs 5 days. While for training M2Det with ResNet-101 when input size is 512×512 on 2 V100 devices, it costs 11 days. The most accurate model is M2Det with the VGG backbone and 800×800 input size, it costs 14 days.</p><p>当在4个Titan X设备上输入尺寸为320×320和512×512时，使用VGG-16主干训练M2Det，总训练时间分别为3天和6天，而当使用ResNet-101为主干网络，320×320为输入尺寸时需要5天。在2台V100设备上输入尺寸为512×512时使用ResNet-101训练M2Det，需要11天。 最准确的模型是使用VGG骨干网络和800×800输入尺寸的M2Det，它需要14天。</p><h3 id="Comparison-with-State-of-the-art"><a href="#Comparison-with-State-of-the-art" class="headerlink" title="Comparison with State-of-the-art"></a><strong>Comparison with State-of-the-art</strong></h3><p>We compare the experimental results of the proposed M2Det with state-of-the-art detectors in Table 1. For these experiments, we use 8 TUMs and set 256 channels for each TUM. The main information involved in the comparison includes the input size of the model, the test method (whether it uses multi-scale strategy), the speed of the model, and the test results. Test results of M2Det with 10 different setting versions are reported in Table 1, which are produced by testing it on MS-COCO test-dev split, with a single NVIDIA Titan X PASCAL and the batch size 1. Other statistical results stem from references. It is noteworthy that, M2Det-320 with VGG backbone achieves AP of 38.9, which has surpassed most object detectors with more powerful backbones and larger input size, e.g., AP of Deformable R-FCN(Daiet al.2017) is 37.5, AP of Faster R-CNN with FPN is 36.2. As sembled with ResNet-101 can further improve M2Det, the single-scale version obtains AP of 38.8, which is competitive with state-of-the-art two-stage detectors Mask R-CNN (He et al. 2017). In addition, based on the optimization of PyTorch, it can run at 15.8 FPS. ReﬁneDet (Zhang et al. 2018) inherits the merits of one-stage detectors and two stage detectors, gets AP of 41.8, CornerNet (Law and Deng 2018) proposes key point regression for detection and borrows the advantages of Hourglass (Newell, Yang, and Deng 2016) and focal loss(Linetal.2017b),thus gets AP of 42.1. In contrast, our proposed M2Det is based on the regression method of original SSD, with the assistance of Multi-scale Multi-level features, obtains 44.2 AP, which exceeds all one stage detectors. Most approaches do not compare the speed of multi-scale inference strategy due to different methods or tools used, so we also only focus on the speed of single-scale inference methods.</p><p>我们将所提出的M2Det与现有技术检测器的实验结果在表1中进行比较。对于这些实验，我们使用8个TUM并为每个TUM设置256个通道。比较中涉及的主要信息包括模型的输入大小，测试方法（是否使用多尺度策略），模型的速度和测试结果。表1中报告了具有10种不同设置版本的M2Det的测试结果，其通过在MS-COCO test-dev split上进行测试而产生，具有单个NVIDIA Titan X PASCAL和批量1。其他统计结果源自参考文献。值得注意的是，具有VGG骨干的M2Det-320达到了38.9的AP，超过了大多数具有更强大骨干和更大输入尺寸的物体探测器，例如，可变形R-FCN（Daiet al.2017）的AP为37.5，AP为FPN更快的R-CNN为36.2。与ResNet-101相结合可以进一步改善M2Det，单级版本获得38.8的AP，这与最先进的两级探测器Mask R-CNN（He et al.2017）相比具有竞争力。此外，基于PyTorch的优化，它可以以15.8 FPS运行。 Re fi neDet（Zhang et al.2018）继承了一级探测器和两级探测器的优点，获得了41.8的AP，CornerNet（Law和Deng 2018）提出了用于探测的关键点回归，并借用了Hourglass（Newell，Yang，和邓2016）和焦点损失（Linetal.2017b），因此获得42.1的AP。相比之下，我们提出的M2Det基于原始SSD的回归方法，在多尺度多级特征的帮助下，获得44.2 AP，超过了所有一级探测器。由于使用的方法或工具不同，大多数方法都没有比较多尺度推理策略的速度，因此我们也只关注单尺度推理方法的速度。</p><p>In addition, in order to emphasize that the improvement of M2Det is not entirely caused by the deepened depth of the model or the gained parameters, we compare with state of-the-art one-stage detectors and two-stage detectors. CornerNet with Hourglass has 201M parameters, Mask R-CNN (He et al. 2017) with ResNeXt-101-32x8d-FPN (Xie et al. 2017) has 205M parameters. By contrast, M2Det 800-VGG has only 147M parameters. Besides, consider the comparison of depth, it is also not dominant.</p><p>此外，为了强调M2Det的改进并非完全由模型的深度或获得的参数引起，我们将与现有最好的一级检测器和两级检测器进行比较。 带有沙漏的CornerNet具有201M参数，Mask R-CNN（He等人2017）与ResNeXt-101-32x8d-FPN（Xie等人2017）具有205M参数。 相比之下，M2Det 800-VGG只有147M参数。 此外，考虑深度的比较，它也不占优势。</p><p><img src="/blog/blog/2022/04/29/a-single-shot-object-detector-based-on-multi-level-feature-pyramid-network/A-Single-Shot-Object-Detector-based-on-Multi-Level-Feature-Pyramid-Network.assets/1571518-20220429213134547-914354692.png"></p><p>Table 1: Detection accuracy comparisons in terms of mAP percentage on MS COCO test-dev set.</p><p><strong>Ablation study</strong></p><p>Since M2Det is composed of multiple subcomponents, we need to verify each of its effectiveness to the ﬁnal performance. The baseline is a simple detector based on the original SSD, with 320×320 input size and VGG-16 reduced backbone.</p><p>由于M2Det由多个子组件组成，因此我们需要验证其对最终性能的有效性。 基线是基于原始SSD的简单检测器，具有320×320输入大小和VGG-16修剪的主干网络。</p><p><strong>TUM</strong></p><p>To demonstrate the effectiveness of TUM, we conduct three experiments. First, following DSSD, we extend the baseline detector with a series of Deconv layers, and the AP has improved from 25.8 to 27.5 as illustrated in the third column in Table2.Then we replace with MLFPN into it .As for the U-shape module, we ﬁrstly stack 8 s-TUMs, which is modiﬁed to decrease the 1×1 Convolution layers shown in Fig. 4, then the performance has improved 3.1 compared with the last operation, shown in the forth column in Table 2.Finally,replacingTUM by s-TUM in the ﬁfth column has reached the best performance, it comes to AP of 30.8.</p><p>为了证明TUM的有效性，我们进行了三次实验。 首先，在DSSD之后，我们使用一系列反卷积层扩展基线检测器，并且AP已经从25.8改进到27.5，如表2中的第三列所示。然后我们将MLFPN替换为它。对于U形模块，我们 首先堆叠8个s-TUM，其被修改以减少图4中所示的1×1卷积层，然后与上一个操作相比，性能提高了3.1，如表2的第四列所示。最后用s-TUM替换TUM达到了最佳性能，如第五列所示，它达到了30.8的AP。</p><p><img src="/blog/blog/2022/04/29/a-single-shot-object-detector-based-on-multi-level-feature-pyramid-network/A-Single-Shot-Object-Detector-based-on-Multi-Level-Feature-Pyramid-Network.assets/1571518-20220429213205288-658328198.png"></p><p>Table 2: Ablation study of M2Det. The detection results are evaluated on minival set</p><p><strong>Base feature</strong> </p><p>Although stacking TUMs can improve detection, but it is limited by input channels of the ﬁrst TUM. That is, decreasing the channels will drop the abstraction of MLFPN, while increasing them will highly increase the parameters number. Instead of using base feature only once, We afferent base feature at the input of each TUM to alleviate the problem. For each TUM, the embedded base feature provides necessary localization information since it contains shallow features. The AP percentage increases to 32.7, as shown in the sixth column in Table 2. </p><p>尽管堆叠TUM可以改善检测，但是它受到第一个TUM的输入通道的限制。 也就是说，减少通道将减少MLFPN的抽象能力，而增加它们将大大增加参数数量。不是仅使用基本特征一次，我们在每个TUM的输入处传达基本特征以缓解该问题。 对于每个TUM，嵌入基本特征提供必要的定位信息，因为它包含浅特征。 AP百分比增加到32.7，如表2中的第六列所示。</p><p><strong>SFAM</strong> </p><p>As shown in the seventh column in Table 2, compared with the architecture that without SFAM, all evaluation metrics have been upgraded. Speciﬁcally, all boxes including small, medium and large become more accurate.</p><p>如表2第7列所示，与没有SFAM的架构相比，所有评估指标都已升级。 具体而言，所有包括小型，中型和大型的盒子都变得更加准确。</p><p><strong>Backbone feature</strong></p><p>As in many visual tasks, we observe a noticeable AP gain from 33.2 to 34.1 when we use well Tested ResNet-101(Heetal.2016) instead of VGG-16 as the backbone network. As shown in Table 2, such observation remains true and consistent with other AP metrics.</p><p>与许多视觉任务一样，当我们使用ResNet-101（Heetal.2016）而不是VGG-16作为骨干网络时，我们观察到AP从33.2到34.1的明显增益。 如表2所示，这种观察仍然是正确的并且与其他AP指标一致。</p><h3 id="Variants-of-MLFPN"><a href="#Variants-of-MLFPN" class="headerlink" title="Variants of MLFPN"></a><strong>Variants of MLFPN</strong></h3><p>The Multi-scale Multi-level Features have been proved to be effective. But what is the boundary of the improvement brought by MLFPN? Step forward, how to design TUM and how many TUMs should be OK? We implement a group of variants to ﬁnd the regular patterns. To be more speciﬁc, we ﬁx the backbone as VGG-16 and the input image size as 320x320, and then tune the number of TUMs and the number of internal channels of each TUM.</p><p>多尺度多级特征已被证明是有效的。 但是，MLFPN带来的改善的边界是什么？ 更进一步讲，如何设计TUM以及多少TUM应该可以？ 我们实施一组差异对比实验来找到常规模式。 更具体地说，我们将主干网络设为VGG-16，输入图像尺寸为320x320，然后调整每个TUM的TUM数和内部通道数。</p><p>As shown in Table3, M2Det with different conﬁgurations of TUMs is evaluated on COCO minival set. Comparing the number of TUMs when ﬁxing the channels, e.g.,256, it can be concluded that stacking more TUMs brings more promotion in terms of detection accuracy. Then ﬁxing the number of TUMs, no matter how many TUMs are assembled, more channels consistently beneﬁt the results. Furthermore, assuming that we take a version with 2 TUMS and 128 channels as the baseline, using more TUMs could bring larger improvement compared with increasing the internal channels, while the increase in parameters remains similar.</p><p>如表3所示，在COCO小的验证集上评估具有不同TUM配置的M2Det。 比较固定通道时的TUM数量，例如256，可以得出结论，堆叠更多TUM在检测精度方面带来更多提升。 然后确定TUM的数量，无论组装多少TUM，更多的通道始终有利于结果。 此外，假设我们采用具有2个TUMS和128个通道的版本作为基线，与增加内部通道相比，使用更多TUM可以带来更大的改进，而参数的增加保持相似。</p><p><img src="/blog/blog/2022/04/29/a-single-shot-object-detector-based-on-multi-level-feature-pyramid-network/A-Single-Shot-Object-Detector-based-on-Multi-Level-Feature-Pyramid-Network.assets/1571518-20220429213049287-1749518578.png"></p><p>Table 3: Different conﬁgurations of MLFPN in M2Det. The backbone is VGG and input image is 320×320.</p><h3 id="Speed"><a href="#Speed" class="headerlink" title="Speed"></a><strong>Speed</strong></h3><p>We compare the inference speed of M2Det with state-of the-art approaches. Since VGG-16 (Simonyan and Zisserman 2015) reduced backbone has removed FC layers, it is very fast to use it for extracting base feature. We set the batch size to 1, take the sum of the CNN time and NMS time of 1000 images, and divide by 1000 to get the inference time of a single image. We assembleVGG-16 to M2Det and propose the fast version M2Det with the input size 320×320, the standard version M2Det with 512×512 input size and the most accurate version M2Det with 800×800 input size. Based on the optimization of PyTorch, M2Det can achieve accurate results with high speed. As shown in Fig. 5, M2Det beneﬁts the advantage of one-stage detection and our proposed MLFPN structure, draws a significantly better speed-accuracy curve compared with other methods. For fair comparison, we reproduce and test the speed of SSD321-ResNet101, SSD513-ResNet101(Fuetal. 2017), ReﬁneDet512-ResNet101, ReﬁneDet320-ResNet101 (Zhangetal.2018) and CornerNet (Law and Deng 2018)on our device. It is clear that M2Det performs more accurately and efﬁciently. In addition, replacing Soft-NMS with Hard NMS, the M2Det-VGG-800 can even achieve a speed of 20 fps, only sacriﬁce little accuracy.</p><p>我们将M2Det的推理速度与最先进的方法进行比较。由于VGG-16（Simonyan和Zisserman 2015）减少了主干网（已经删除了FC层），因此使用它来提取基本特征非常快。我们将批量大小设置为1，取1000个图像的CNN时间和NMS时间之和，除以1000得到单个图像的推理时间。我们将VGAG-16组装到M2Det，并提出输入尺寸为320×320的快速版M2Det，具有512×512输入尺寸的标准版M2Det和具有800×800输入尺寸的最准确版M2Det。基于PyTorch的优化，M2Det可以高速实现精确的结果。如图5所示，M2Det得益于一阶段检测和我们提出的MLFPN结构优势，与其他方法相比，能得到明显更好的速度 - 准确度曲线。为了公平比较，我们在我们的设备上重现并测试SSD321-ResNet101，SSD513-ResNet101（Fuetal。2017），Re fi neDet512-ResNet101，Re fi neDet320-ResNet101（Zhangetal.2018）和CornerNet（Law和Deng 2018）的速度。很明显，M2Det可以更准确，更有效地执行。此外，用Hard NMS取代Soft-NMS，M2Det-VGG-800甚至可以达到20 fps的速度，但只有很少的精度损失。</p><p><img src="/blog/blog/2022/04/29/a-single-shot-object-detector-based-on-multi-level-feature-pyramid-network/A-Single-Shot-Object-Detector-based-on-Multi-Level-Feature-Pyramid-Network.assets/1571518-20220429213022909-325707835.png"></p><p>Figure5:Speed(ms) vs. accuracy(mAP) on COCO test-dev.</p><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a><strong>Discussion</strong></h3><p>We think the detection accuracy improvement of M2Det is mainly brought by the proposed MLFPN. On one hand, we fuse multi-level features extracted by backbone as the base feature, and then feed it into a block of alternating joint Thinned U-shape Modules and Feature Fusion Modules to extract more representative, multi-level multi-scale features, i.e. the decoder layers of each TUM. Obviously, these decoder layers are much deeper than the layers in the backbone, and thus more representative for object detection. Contrasted with our method, the existing detectors (Zhang et al. 2018; Lin et al. 2017a; Fu et al. 2017) just use the layers of the backbone or extra layers with few depth increase. Hence, our method can achieve superior detection performance. On the other hand, each feature map of the multilevel feature pyramid generated by the SFAM consists of the decoder layers from multiple levels. In particular, at each scale, we use multi-level features to detect objects, which would be better for handling complex appearance variation across object instances.</p><p>我们认为M2Det的检测精度提高主要是由提出的MLFPN带来的。一方面，我们将骨干提取的多层特征融合为基本特征，然后将其馈入交替连接的简化U形模块和特征融合模块，以提取更具代表性的多层次多尺度特征，即每个TUM的解码器层。显然，这些解码器层比主干中的层深得多，因此用于物体检测时更有代表性。与我们的方法相比，现有的探测器（Zhang等人，2018; Lin等人，2017a; Fu等人，2017）只使用骨架层或额外层，几乎没有深度增加。因此，我们的方法可以实现卓越的检测性能。另一方面，由SFAM生成的多级特征金字塔的每个特征图包括来自多个级别的解码器层。特别是，在每个比例下，我们使用多级功能来检测对象，这样可以更好地处理跨对象实例的复杂外观变化。</p><p>To verify that the proposed MLFPN can learn effective feature for detecting objects with different scales and complex appearances, we visualize the activation values of classiﬁcation Conv layers along scale and level dimensions, such an example shown in Fig. 6. The input image contains two persons, two cars and a trafﬁc light. Moreover, the sizes of the two persons are different, as well as the two cars. And the trafﬁc light, the smaller person and the smaller car have similar sizes. We can ﬁnd that: 1) compared with the smaller person, the larger person has strongest activation value at the feature map of large scale, so as to the smaller car and larger car; 2) the trafﬁc light, the smaller person and the smaller car have strongest activation value at the feature maps of the same scale; 3) the persons, the cars and the trafﬁc light have strongest activation value at the highest-level, middle-level, lowest-level feature maps respectively. This example presents that: 1) our method learns very effective features to handle scale variation and complex appearance variation across the object instances; 2) it is necessary to use multilevel features to detect objects with similar size.</p><p>为了验证所提出的MLFPN能够学习用于检测具有不同尺度和复杂外观的物体的有效特征，我们可以根据比例和水平尺寸来显示分类Conv层的激活值，如图6所示。输入图像包含两个人，两辆车和一个交通灯。此外，两个人以及两辆车的大小不同。交通灯，较小的人和较小的汽车有相似的尺寸。我们可以发现：1）与较小的人相比，较大的人在大尺度的特征图上具有最强的激活值，对于较小的汽车和较大的汽车也一样; 2）交通灯，较小的人和较小的汽车在相同比例的特征图上具有最强的激活值; 3）人员，汽车和交通灯分别在最高级别，中级别，最低级别的特征地图中具有最强的激活值。这个例子表明：1）我们的方法学习非常有效的特征来处理对象实例中的尺度变化和复杂的外观变化; 2）有必要使用多级特征来检测具有相似大小的对象。</p><p><img src="/blog/blog/2022/04/29/a-single-shot-object-detector-based-on-multi-level-feature-pyramid-network/A-Single-Shot-Object-Detector-based-on-Multi-Level-Feature-Pyramid-Network.assets/1571518-20220429212855538-1928659565.png"></p><p>Figure 6: Example activation values of multi-scale multilevel features. Best view in color.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><strong>Conclusion</strong></h2><p>In this work, a novel method called Multi-Level Feature Pyramid Network (MLFPN) is proposed to construct effective feature pyramids for detecting objects of different scales. MLFPN consists of several novel modules. First, multi-level features (i.e. multiple layers) extracted by backbone are fused by a Feature Fusion Module (FFMv1) as the base feature. Second, the base feature is fed into a block of alternating joint Thinned U-shape Modules(TUMs) and Fature Fusion Modules (FFMv2s) and multi-level multi-scale features (i.e. the decoder layers of each TUM) are extracted. Finally, the extracted multi-level multi-scale features with the same scale (size) are aggregated to construct a feature pyramid for object detection by a Scale-wise Feature Aggregation Module (SFAM). A powerful end-to-end one-stage object detector called M2Det is designed based on the proposed MLFPN, which achieves a new state-of-the-art result (i.e. AP of 41.0 at speed of 11.8 FPS with single-scale inference strategy and AP of 44.2 with multi-scale inference strategy) among the one-stage detectors on MS-COCO benchmark. Additional ablation studies further demonstrate the effectiveness of the proposed architecture and the novel modules。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper_Read </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning_Efficient_Convolutional_Networks_through_Network_Slimming</title>
      <link href="/blog/2022/04/29/learning-efficient-convolutional-networks-through-network-slimming/"/>
      <url>/blog/2022/04/29/learning-efficient-convolutional-networks-through-network-slimming/</url>
      
        <content type="html"><![CDATA[<p><strong>Learning Efficient Convolutional Networks through Network Slimming</strong></p><p>Zhuang Liu1∗ Jianguo Li2 Zhiqiang Shen3 Gao Huang4 Shoumeng Yan2 Changshui Zhang1 </p><p>1CSAI, TNList, Tsinghua University 2Intel Labs China 3Fudan University 4Cornell University {liuzhuangthu, zhiqiangshen0214}@gmail.com, {jianguo.li, shoumeng.yan}@intel.com, <a href="mailto:&#x67;&#x68;&#x33;&#52;&#57;&#64;&#x63;&#111;&#114;&#110;&#x65;&#108;&#x6c;&#46;&#101;&#x64;&#x75;">&#x67;&#x68;&#x33;&#52;&#57;&#64;&#x63;&#111;&#114;&#110;&#x65;&#108;&#x6c;&#46;&#101;&#x64;&#x75;</a>, <a href="mailto:&#x7a;&#x63;&#115;&#64;&#x6d;&#x61;&#x69;&#108;&#x2e;&#116;&#115;&#x69;&#110;&#x67;&#x68;&#117;&#97;&#x2e;&#x65;&#x64;&#x75;&#46;&#99;&#x6e;">&#x7a;&#x63;&#115;&#64;&#x6d;&#x61;&#x69;&#108;&#x2e;&#116;&#115;&#x69;&#110;&#x67;&#x68;&#117;&#97;&#x2e;&#x65;&#x64;&#x75;&#46;&#99;&#x6e;</a></p><p><strong>Abstract</strong></p><p>The deployment of deep convolutional neural networks (CNNs) in many Real-World applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insigniﬁcant channels are automatically identiﬁed and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classiﬁcation datasets. For VGGNet, a multi-pass version of network slimming gives a 20× reduction in model size and a 5× reduction in computing operations.</p><p>在许多实际应用中部署深度卷积神经网络（CNN）很大程度上受到其计算成本高的限制。在本文中，我们提出了一种新的CNNs学习方案，能同时1）减小模型大小; 2）减少运行时内存占用; 3）在不影响准确率的情况下降低计算操作的数量。这种学习方案是<strong>通过在网络中<strong><strong>进行通</strong></strong>道<strong><strong>层次</strong></strong>稀疏来实现<strong><strong>，</strong></strong>简单<strong><strong>而</strong></strong>有效</strong>。与许多现有方法不同，我们所提出的方法直接应用于现代CNN架构，引入训练过程的开销最小，并且所得模型不需要特殊软件/硬件加速器。我们将我们的方法称为网络瘦身（network slimming），此方法以大型网络作为输入模型，但在训练过程中，无关紧要的通道被自动识别和剪枝，从而产生精度相当但薄而紧凑（高效）的模型。在几个最先进的CNN模型（包括VGGNet，ResNet和DenseNet）上，我们使用各种图像分类数据集，凭经验证明了我们方法的有效性。对于VGGNet，网络瘦身后的多通道版本使模型大小减少20倍，计算操作减少5倍。</p><p><strong>1. Introduction</strong></p><p>In recent years, convolutional neural networks (CNNs) have become the dominant approach for a variety of computer vision tasks, e.g., image classification [22], object detection [8], semantic segmentation [26]. Large-scale datasets, high-end modern GPUs and new network architectures allow the development of unprecedented large CNN models. For instance, from AlexNet [22], VGGNet [31] and GoogleNet [34] to ResNets [14], the ImageNet Classification Challenge winner models have evolved from 8 layers to more than 100 layers.</p><p>近年来，卷积神经网络（CNN）已成为各种计算机视觉任务的主要方法，例如图像分类[22]，物体检测[8]，语义分割[26]。 大规模数据集，高端现代GPU和新的网络架构允许开发前所未有的大型CNN模型。 例如，从AlexNet [22]，VGGNet [31]和GoogleNet [34]到ResNets [14]，ImageNet分类挑战赛冠军模型已经从8层发展到100多层。</p><p>However, large CNNs, although with stronger representation power, are more resource-hungry. For instance, a 152-layer ResNet [14] has more than 60 million parameters and requires more than 20 Giga float-point-operations (FLOPs) when inferencing an image with resolution 224× 224. This is unlikely to be affordable on resource constrained platforms such as mobile devices, wearables or Internet of Things (IoT) devices.</p><p>然而，大型CNN虽然具有更强的代表(表现，提取特征，表征)能力，却也更耗费资源。 例如，一个152层的ResNet [14]具有超过6000万个参数，在推测(处理)分辨率为224×224的图像时需要超过20个Giga的浮点运算（FLOP）-即20G flop的运算量。这在资源受限的情况下不可能负担得起，如在移动设备，可穿戴设备或物联网（IoT）设备等平台上。</p><p>The deployment of CNNs in real world applications are mostly constrained by 1) Model size: CNNs’ strong representation power comes from their millions of trainable parameters. Those parameters, along with network structure information, need to be stored on disk and loaded into memory during inference time. As an example, storing a typical CNN trained on ImageNet consumes more than 300MB space, which is a big resource burden to embedded devices. 2) Run-time memory: During inference time, the intermediate activations/responses of CNNs could even take more memory space than storing the model parameters, even with batch size 1. This is not a problem for high-end GPUs, but unaffordable for many applications with low computational power. 3) Number of computing operations: The convolution operations are computationally intensive on high resolution images. A large CNN may take several minutes to process one single image on a mobile device, making it unrealistic to be adopted for real applications.</p><p>CNN在实际应用中的部署主要受以下因素的限制：1）模型大小：CNN的强大表现力来自其数百万可训练参数。这些参数以及网络结构信息需要存储在磁盘上并在推理期间加载到内存中。例如，存储一个ImageNet上训练的典型CNN会占用超过300MB的空间，这对嵌入式设备来说是一个很大的资源负担。 2）运行时内存（占用情况）：在推理期间，即使批量大小为1，CNN的中间激活/响应占用的内存空间甚至可以比存储模型的参数更多.这对于高端GPU来说不是问题，但是对于许多计算能力低的应用而言是无法承受的。 3）计算操作的数量：在高分辨率图像的卷积操作上是计算密集的。大型CNN在移动设备上处理单个图像可能需要几分钟，这使得在实际应用中采用大型CNN是不现实的。</p><p>Many works have been proposed to compress large CNNs or directly learn more efficient CNN models for fast inference. These include low-rank approximation [7], network quantization [3, 12] and binarization [28, 6], weight pruning [12], dynamic inference [16], etc. However, most of these methods can only address one or two challenges mentioned above. Moreover, some of the techniques require specially designed software/hardware accelerators for execution speedup [28, 6, 12].</p><p>许多工作已经提出了压缩大型CNN或直接学习更有效的CNN模型以进行快速推理。 这些工作包括低秩逼近（这么翻译正确吗？）[7]，网络量化[3,12]和网络二值化[28,6]，权重剪枝[12]，动态推理[16]等。但是，这些方法大多数只能解决一个或两个上面提到的挑战。 此外，一些技术需要专门设计的软件/硬件加速器来实行加速[28,6,12]。</p><p>Another direction to reduce the resource consumption of large CNNs is to sparse the network. Sparsity can be imposed on different level of structures [2, 37, 35, 29, 25], which yields considerable model-size compression and inference speedup. However, these approaches generally require special software/hardware accelerators to harvest the gain in memory or time savings, though it is easier than non-structured sparse weight matrix as in [12].</p><p>减少大型CNN资源消耗的另一个方向是稀疏化网络。 可以对不同级别的结构施加稀疏性[2,37,35,29,25]，这产生了相当大的模型大小压缩和推理加速。 尽管它比[12]中的非结构化稀疏权重矩阵更容易，然而，这些方法通常需要特殊的软件/硬件加速器来获得内存增益或节省时间。</p><p>In this paper, we propose network slimming, a simple yet effective network training scheme, which addresses all the aforementioned challenges when deploying large CNNs under limited resources. Our approach imposes L1 regularization on the scaling factors in batch normalization (BN) layers, thus it is easy to implement without introducing any change to existing CNN architectures. Pushing the values of BN scaling factors towards zero with L1 regularization enables us to identify insignificant channels (or neurons), as each scaling factor corresponds to a specific convolutional channel (or a neuron in a fully-connected layer). This facilitates the channel-level pruning at the followed step. The additional regularization term rarely hurt the performance. In fact, in some cases it leads to higher generalization accuracy. Pruning unimportant channels may sometimes temporarily degrade the performance, but this effect can be compensated by the followed fine-tuning of the pruned network. After pruning, the resulting narrower network is much more compact in terms of model size, runtime memory, and computing operations compared to the initial wide network. The above process can be repeated for several times, yielding a multi-pass network slimming scheme which leads to even more compact network.</p><p>在本文中，我们提出了<strong>网络瘦身</strong>，这是一种简单而有效的网络训练方案，它解决了在有限资源的条件下应用大型CNN时所面临的所有挑战。我们的方法对批量归一化（BN）层中的缩放因子强加L1正则化，因此很容易实现，而不会对现有CNN架构进行任何更改。通过L1正则化将BN缩放因子的值逼近零使得我们能够识别不重要的通道（或神经元），因为每个缩放因子对应于特定的卷积通道（或完全连接的层中的神经元）。这有助于在随后的步骤中进行通道层次的修剪。额外的正则化术语很少会影响性能。实际上，在某些情况下，它会导致更高的泛化精度。修剪不重要的通道有时会暂时降低性能，但是这种影响可以通过修剪网络的后续调整来补偿。修剪后，与初始的大型网络相比，由此产生的轻量化网络在模型大小，运行时内存和计算操作方面更加紧凑。上述过程可以重复多次以产生多通道网络瘦身方案，这能产生更紧凑的网络。</p><p>Experiments on several benchmark datasets and different network architectures show that we can obtain CNN models with up to 20x mode-size compression and 5x reduction in computing operations of the original ones, while achieving the same or even higher accuracy. Moreover, our method achieves model compression and inference speedup with conventional hardware and deep learning software packages, since the resulting narrower model is free of any sparse storing format or computing operations.</p><p>在几个基准数据集和不同网络架构上的实验表明，我们获得的CNN模型，其大小压缩高达20倍，原始计算操作减少5倍，同时实现了相同甚至更高的精度。此外，我们的方法利用传统硬件和深度学习软件包实现模型压缩和推理加速，因此得到的轻量化模型不需要任何稀疏存储格式或特殊的计算操作。</p><p><strong>2. Related Work</strong></p><p>In this section, we discuss related work from five aspects. Low-rank Decomposition approximates weight matrix in neural networks with low-rank matrix using techniques like Singular Value Decomposition (SVD) [7]. This method works especially well on fully-connected layers, yielding ∼3x model-size compression however without notable speed acceleration, since computing operations in CNN mainly come from convolutional layers.</p><p>在本节中，我们将从五个方面讨论相关工作。用奇异值分解（SVD）等技术使具有较低秩的矩阵去逼近神经网络中的权重矩阵[7]。 这种方法在全连接的层上工作得特别好，产生~3倍模型大小的压缩，但没有明显的速度加速，因为CNN中的计算操作主要来自卷积层。</p><p><strong>Weight Quantization.</strong> HashNet [3] proposes to quantize the network weights. Before training, network weights are hashed to different groups and within each group weight the value is shared. In this way only the shared weights and hash indices need to be stored, thus a large amount of storage space could be saved. [12] uses a improved quantization technique in a deep compression pipeline and achieves 35x to 49x compression rates on AlexNet and VGGNet. However, these techniques can neither save run-time memory nor inference time, since during inference shared weights need to be restored to their original positions.</p><p>权重量化。 HashNet [3]建议量化网络权重。 在训练之前，网络权重被散列到不同的组，并且在每个组内共享该权重值。 这样，只需要存储共享权重和哈希索引，从而可以节省大量的存储空间。 [12]在深度压缩流程中使用改进的量化技术，在AlexNet和VGGNet上实现35x到49x的压缩率。 但是，这些技术既不能节省运行时内存也不能节省推理时间，因为在推理期间，需要将共享权重恢复到其原始位置。</p><p>[28, 6] quantize real-valued weights into binary/ternary weights (weight values restricted to {−1,1} or {−1,0,1}). This yields a large amount of model-size saving, and signiﬁcant speedup could also be obtained given bitwise operation libraries. However, this aggressive low-bit approximation method usually comes with a moderate accuracy loss.</p><p>[28,6]将实值权重量化为二进制/三进制权重（权重值限制为{-1,1}或{-1,0,1}）。 这样可以节省大量的模型空间，并且在给定按位运算库的情况下也可以获得显着的加速。 然而，这种积极的（意思是压缩力度过大？）低位近似方法通常具有一定的精度损失。</p><p><strong>Weight Pruning / Sparsifying</strong>. [12] proposes to prune the unimportant connections with small weights in trained neural networks. The resulting network’s weights are mostly zeros thus the storage space can be reduced by storing the model in a sparse format. However, these methods can only achieve speedup with dedicated sparse matrix operation libraries and/or hardware. The run-time memory saving is also very limited since most memory space is consumed by the activation maps (still dense) instead of the weights.</p><p>权重剪枝/稀疏。 [12]提出在训练好的神经网络中修剪不重要的小权重连接。 由此产生的网络权重大多为零，可以通过以稀疏格式存储模型来减少存储空间。 但是，这些方法只能通过专用的稀疏矩阵运算库和/或硬件实现加速。 运行时内存节省也非常有限，因为大多数内存空间被激活映射（仍然密集）而不是权重消耗。</p><p>In [12], there is no guidance for sparsity during training. [32] overcomes this limitation by explicitly imposing sparse constraint over each weight with additional gate variables, and achieve high compression rates by pruning connections with zero gate values. This method achieves better com pression rate than [12], but suffers from the same drawback.</p><p>在[12]中，没有关于训练期间如何稀疏的指导。 [32]通过使用额外的门变量明确地对每个权重施加稀疏约束来克服此限制，并通过修剪具有零门值的连接来实现高压缩率。 该方法实现了比[12]更好的压缩率，但也存在同样的缺点。</p><p><strong>Structured Pruning / Sparsifying.</strong> Recently, [23] proposes to prune channels with small incoming weights in trained CNNs, and then fine-tune the network to regain accuracy. [2] introduces sparsity by random deactivating input-output channel-wise connections in convolutional layers before training, which also yields smaller networks with moderate accuracy loss. Compared with these works, we explicitly impose channel-wise sparsity in the optimization objective during training, leading to smoother channel pruning process and little accuracy loss.</p><p>结构化修剪/稀疏化。 最近，[23]提出在训练好的CNN中修剪具有较小输入权重的信道，然后对网络进行微调以恢复准确性。 [2]通过在训练之前在卷积层中随机停用输入 - 输出信道连接的方式来引入稀疏性，这能产生具有中等精度损失的较小网络。 与这些工作相比，我们在训练期间明确地在优化目标中强加了通道方式稀疏性，导致更平滑的通道修剪过程和很少的准确性损失。</p><p>[37] imposes neuron-level sparsity during training thus some neurons could be pruned to obtain compact networks. [35] proposes a Structured Sparsity Learning (SSL) method to sparsify different level of structures (e.g. ﬁlters, channels or layers) in CNNs. Both methods utilize group sparsity regualarization during training to obtain structured sparsity. Instead of resorting to group sparsity on convolutional weights, our approach imposes simple L1 sparsity on channel-wise scaling factors, thus the optimization objective is much simpler.</p><p>[37]在训练期间强加神经元水平的稀疏性，因此可以修剪一些神经元以获得紧凑的网络。 [35]提出了一种结构化稀疏度学习（SSL）方法，用于稀疏CNN中不同级别的结构（例如滤波器，信道或层）。 两种方法都在训练期间利用群组稀疏性规则化来获得结构化稀疏性。 我们的方法不是在卷积权重上采用群稀疏度，而是在通道方面的缩放因子上强加简单的L1稀疏性，因此优化目标要简单得多。</p><p>Since these methods prune or sparsify part of the network structures (e.g., neurons, channels) instead of individual weights, they usually require less specialized libraries (e.g. for sparse computing operation) to achieve inference speedup and run-time memory saving. Our network slimming also falls into this category, with absolutely no special libraries needed to obtain the benefits.</p><p>由于这些方法修剪或稀疏网络结构的一部分（例如，神经元，信道）而不是单独的权重，它们通常需要较少的专用库（例如，用于稀疏计算操作）以实现推理加速和运行时存储器节省。 我们的网络瘦身也属于这一类，完全不需要特殊的库来获得增益。</p><p><strong>Neural Architecture Learning</strong>. While state-of-the-art CNNs are typically designed by experts [22, 31, 14], there are also some explorations on automatically learning network architectures. [20] introduces sub-modular/supermodular optimization for network architecture search with a given resource budget. Some recent works [38, 1] propose to learn neural architecture automatically with reinforcement learning. The searching space of these methods are extremely large, thus one needs to train hundreds of models to distinguish good from bad ones. Network slimming can also be treated as an approach for architecture learning, despite the choices are limited to the width of each layer. However, in contrast to the aforementioned methods, network slimming learns network architecture through only a single training process, which is in line with our goal of efficiency.</p><p>神经结构学习。 虽然最先进的CNN通常由专家[22,31,14]设计，但也有一些关于自动学习网络架构的探索。 [20]引入了用于给定资源预算的网络架构搜索的子模块/超模块优化。 最近的一些工作[38,1]提出通过强化学习自动学习神经结构。 这些方法的搜索空间非常大，因此需要训练数百个模型来区分好与坏。 网络瘦身也可以被视为架构学习的一种方法，尽管选择仅限于每层的宽度。 然而，与上述方法相比，网络瘦身仅通过一个训练过程来学习网络架构，这符合我们的效率目标。</p><p><strong>3. Network slimming</strong></p><p>We aim to provide a simple scheme to achieve channel-level sparsity in deep CNNs. In this section, we first discuss the advantages and challenges of channel-level sparsity, and introduce how we leverage the scaling layers in batch normalization to effectively identify and prune unimportant channels in the network.</p><p>我们的目标是提供一个简单的方案来实现深度CNN中的信道层次的稀疏。 在本节中，我们首先讨论了信道层次稀疏的优势和挑战，并介绍了如何利用批量规范化中的扩展层（缩放因子）来有效地识别和修剪网络中不重要的信道。</p><p><strong>Advantages of Channel-level Sparsity</strong>. As discussed in prior works [35, 23, 11], sparsity can be realized at different levels, e.g., weight-level, kernel-level, channel-level or layer-level. Fine-grained level (e.g., weight-level) sparsity gives the highest flexibility and generality leads to higher compression rate, but it usually requires special software or hardware accelerators to do fast inference on the sparse model [11]. On the contrary, the coarsest layer-level sparsity does not require special packages to harvest the inference speedup, while it is less flexible as some whole layers need to be pruned. In fact, removing layers is only effective when the depth is sufficiently large, e.g., more than 50 layers [35, 18]. In comparison, channel-level sparsity provides a nice tradeoff between flexibility and ease of implementation. It can be applied to any typical CNNs or fully connected networks (treat each neuron as a channel), and the resulting network is essentially a “thinned” version of the unpruned network, which can be efficiently inferenced on conventional CNN platforms.</p><p>通道层次稀疏性的优势。如在先前的工作[35,23,11]中所讨论的，稀疏性可以在不同的层次实现，例如，权重级，内核级，通道级或层级。细粒度级别（例如，权重级别）稀疏性提供最高的灵活性和通用性导致更高的压缩率，但它通常需要特殊的软件或硬件加速器来对稀疏模型进行快速推理[11]。相反，粗糙层级稀疏性不需要特殊的包来获得推理加速，而它不太灵活，因为需要修剪一些整个层。实际上，去除整层仅在模型深度足够大时才有效，例如，超过50层[35,18]。相比之下，通道层次稀疏性提供了灵活性和易于实现之间的良好折衷。它可以应用于任何典型的CNN或全连接的网络（将每个神经元视为信道），并且所得到的网络本质上是未修整网络的“稀疏”版本，其可以在传统CNN平台上被有效地推断。</p><p><strong>Challenges</strong>. Achieving channel-level sparsity requires pruning all the incoming and outgoing connections associated with a channel. This renders the method of directly pruning weights on a pre-trained model ineffective, as it is unlikely that all the weights at the input or output end of a channel happen to have near zero values. As reported in [23], pruning channels on pre-trained ResNets can only lead to a reduction of∼10% in the number of parameters without suffering from accuracy loss. [35] addresses this problem by enforcing sparsity regularization into the training objective. Specifically, they adopt group LASSO to push all the filter weights corresponds to the same channel towards zero simultaneously during training. However, this approach requires computing the gradients of the additional regularization term with respect to all the filter weights, which is nontrivial. We introduce a simple idea to address the above challenges, and the details are presented below.</p><p>挑战。 实现通道层次稀疏性需要修剪与通道关联的所有传入和传出连接，这使得在预训练模型上直接修剪权重的方法无效，因为通道的输入或输出端处的所有权重不可能恰好具有接近零的值。 如[23]所述，预训练的ResNets上的通道修剪只能减少参数数量的~10％才不会导致精度损失。 [35]通过将稀疏正规化强制纳入训练目标来解决这个问题。 具体而言，他们采用组LASSO在训练期间将所有对应于同一通道的滤波器权重同时逼近零。 然而，这种方法需要相对于所有滤波器权重来计算附加正则化项的梯度，这是非常重要的。 我们引入一个简单的想法来解决上述挑战，详情如下。</p><p><strong>Scaling Factors and Sparsity-induced Penalty.</strong> Our idea is introducing a scaling factor γ for each channel, which is multiplied to the output of that channel. Then we jointly train the network weights and these scaling factors, with sparsity regularization imposed on the latter. Finally we prune those channels with small factors, and fine-tune the pruned network. Specifically, the training objective of our approach is given by</p><p>缩放因素和稀疏性惩罚。 我们的想法是为每个通道引入一个比例因子γ，它乘以该通道的输出。 然后我们联合训练网络权重和这些比例因子，并对后者施加稀疏正则化。 最后，我们修剪这些小因子通道，并调整修剪后的网络。 具体而言，我们的方法的训练目标是：</p><p><img src="/blog/blog/2022/04/29/learning-efficient-convolutional-networks-through-network-slimming/Learning-Efficient-Convolutional-Networks-through-Network-Slimming.assets/1571518-20220429211930598-1557631132.png"></p><p>where (x, y) denote the train input and target, W denotes the trainable weights, the first sum-term corresponds to the normal training loss of a CNN, g(·) is a sparsity-induced penalty on the scaling factors, and λ balances the two terms. In our experiment, we choose g(s)=|s|, which is known as L1-norm and widely used to achieve sparsity. Subgradient descent is adopted as the optimization method for the nonsmooth L1 penalty term. An alternative option is to replace the L1 penalty with the smooth-L1 penalty [30] to avoid using sub-gradient at non-smooth point.</p><p>其中（x，y）表示训练输入和目标，W表示可训练的权重，第一个和项对应于CNN的正常训练损失，g（·）是比例因子的稀疏性引起的惩罚，以及 λ平衡这两个损失。 在我们的实验中，我们选择g（s）= | s |，它被称为L1范数并广泛用于实现稀疏性。 采用次梯度下降作为非光滑L1惩罚项的优化方法。 另一种选择是将L1惩罚替换为平滑L1惩罚[30]，以避免在非平滑点使用子梯度。</p><p>As pruning a channel essentially corresponds to removing all the incoming and outgoing connections of that channel, we can directly obtain a narrow network (see Figure 1) without resorting to any special sparse computation packages. The scaling factors act as the agents for channel selection. As they are jointly optimized with the network weights, the network can automatically identity insignificant channels, which can be safely removed without greatly affecting the generalization performance.</p><p>修剪一个通道基本上对应于删除该通道的所有传入和传出连接，我们可以直接获得一个轻量化的网络（见图1），而不需要使用任何特殊的稀疏计算包。 缩放因子充当频道选择的代理。 由于它们与网络权重共同优化，因此网络可以自动识别无关紧要的通道，这些通道可以安全地移除而不会极大地影响泛化性能。</p><p><img src="/blog/blog/2022/04/29/learning-efficient-convolutional-networks-through-network-slimming/Learning-Efficient-Convolutional-Networks-through-Network-Slimming.assets/1571518-20220429211858645-1827544675.png"></p><p><strong>Leveraging the Scaling Factors in BN Layers</strong>. Batch normalization [19] has been adopted by most modern CNNs as a standard approach to achieve fast convergence and better generalization performance. The way BN normalizes the activations motivates us to design a simple and efficient method to incorporates the channel-wise scaling factors. Particularly, BN layer normalizes the internal activations using mini-batch statistics. Let zin and zout be the input and output of a BN layer, B denotes the current minibatch, BN layer performs the following transformation:</p><p>利用BN图层中的缩放因子。 批量归一化[19]已被大多数现代CNN采用作为实现快速收敛和更好的泛化性能的标准方法。 BN规范化激活的方式促使我们设计一种简单有效的方法来合并通道方式的缩放因子。 特别地，BN层使用小批量统计来标准化内部激活。 令zin和zout为BN层的输入和输出，B表示当前的小批量，BN层执行以下转换：</p><p><img src="/blog/blog/2022/04/29/learning-efficient-convolutional-networks-through-network-slimming/Learning-Efficient-Convolutional-Networks-through-Network-Slimming.assets/1571518-20220429211835286-56389754.png"></p><p>where µB and σB are the mean and standard deviation values of input activations over B, γ and β are trainable affine transformation parameters (scale and shift) which provides the possibility of linearly transforming normalized activations back to any scales.</p><p>其中μB和σB是B上输入激活的平均值和标准偏差值，γ和β是可以通过训练变换的参数（比例和偏移），这提供了将归一化激活线性转换到任何尺度的可能性。</p><p>It is common practice to insert a BN layer after a convolutional layer, with channel-wise scaling/shifting parameters. Therefore, we can directly leverage the γ parameters in BN layers as the scaling factors we need for network slimming. It has the great advantage of introducing no overhead to the network. In fact, this is perhaps also the most effective way we can learn meaningful scaling factors for channel pruning. 1), if we add scaling layers to a CNN without BN layer, the value of the scaling factors are not meaningful for evaluating the importance of a channel, because both convolution layers and scaling layers are linear transformations. One can obtain the same results by decreasing the scaling factor values while amplifying the weights in the convolution layers. 2), if we insert a scaling layer before a BN layer, the scaling effect of the scaling layer will be completely canceled by the normalization process in BN. 3), if we insert scaling layer after BN layer, there are two consecutive scaling factors for each channel.</p><p>通常的做法是在卷积层之后插入BN层，保留通道缩放/移位参数。因此，我们可以直接利用BN层中的γ参数作为网络瘦身所需的比例因子。它具有不向网络引入任何开销的巨大优势。事实上，这也许是我们学习有用的通道修剪缩放因子的最有效方法。 1），如果我们将缩放层添加到没有BN层的CNN，则缩放因子的值对于评估通道的重要性没有意义，因为卷积层和缩放层都是线性变换。通过放大卷积层中的权重的同时减小缩放因子值，可以获得相同的结果。 2），如果我们在BN层之前插入缩放层，缩放层的缩放效果将被BN中的归一化处理完全取消。 3），如果我们在BN层之后插入缩放层，则每个通道有两个连续的缩放因子。</p><p><strong>Channel Pruning and Fine-tuning.</strong> After training under channel-level sparsity-induced regularization, we obtain a model in which many scaling factors are near zero (see Figure 1). Then we can prune channels with near-zero scaling factors, by removing all their incoming and outgoing connections and corresponding weights. We prune channels with a global threshold across all layers, which is defined as a certain percentile of all the scaling factor values. For instance, we prune 70% channels with lower scaling factors by choosing the percentile threshold as 70%. By doing so, we obtain a more compact network with less parameters and run-time memory, as well as less computing operations.</p><p>通道剪枝和微调。 在通道层次稀疏诱导正则化训练之后，我们获得了一个模型，其中许多比例因子接近于零（见图1）。 然后我们可以通过删除所有传入和传出连接以及相应的权重来修剪具有接近零比例因子的通道。 我们使用全局阈值在所有层上修剪通道，其被定义为所有比例因子值的特定百分位数。 例如，我们通过选择百分比阈值为70％来修剪具有较低缩放因子的70％通道。 通过这样做，我们获得了一个更紧凑的网络，具有更少的参数和运行时内存，以及更少的计算操作。</p><p>Pruning may temporarily lead to some accuracy loss, when the pruning ratio is high. But this can be largely compensated by the followed fine-tuning process on the pruned network. In our experiments, the fine-tuned narrow network can even achieve higher accuracy than the original unpruned network in many cases.</p><p>当修剪比例高时，修剪可能暂时导致一些精确度损失。 但是，这可以通过修剪网络上的后续微调过程得到很大程度的补偿。 在我们的实验中，在许多情况下，微调的轻量化网络甚至可以实现比原始未修网络更高的精度。</p><p><strong>Multi-pass Scheme</strong>. We can also extend the proposed method from single-pass learning scheme (training with sparsity regularization, pruning, and fine-tuning) to a multi-pass scheme. Specifically, a network slimming procedure results in a narrow network, on which we could again apply the whole training procedure to learn an even more compact model. This is illustrated by the dotted-line in Figure 2. Experimental results show that this multi-pass scheme can lead to even better results in terms of compression rate.</p><p>多通道方案。 我们还可以将所提出的方法从单程学习方案（具有稀疏正则化，修剪和微调的训练）扩展到多程方案。 具体而言，网络瘦身过程导致网络狭窄，我们可以再次应用整个训练程序来学习更紧凑的模型。 这由图2中的虚线说明。实验结果表明，这种多次通过方案可以在压缩率方面产生更好的结果。</p><p><img src="/blog/blog/2022/04/29/learning-efficient-convolutional-networks-through-network-slimming/Learning-Efficient-Convolutional-Networks-through-Network-Slimming.assets/1571518-20220429211805524-1635494843.png"></p><p><strong>Handling Cross Layer Connections and Pre-activation Structure.</strong> The network slimming process introduced above can be directly applied to most plain CNN architectures such as AlexNet [22] and VGGNet [31]. While some adaptations are required when it is applied to modern networks with cross layer connections and the pre-activation design such as ResNet [15] and DenseNet [17]. For these networks, the output of a layer may be treated as the input of multiple subsequent layers, in which a BN layer is placed before the convolutional layer. In this case, the sparsity is achieved at the incoming end of a layer, i.e., the layer selectively uses a subset of channels it received. To harvest the parameter and computation savings at test time, we need to place a channel selection layer to mask out insignificant channels we have identified.</p><p>处理跨层连接和预激活结构。 上面介绍的网络瘦身过程可以直接应用于大多数简单的CNN架构，如AlexNet [22]和VGGNet [31]。 当它应用于具有跨层连接的现代网络和预激活设计（如ResNet [15]和DenseNet [17]）时，需要进行一些调整。 对于这些网络，层的输出可以被视为多个后续层的输入，其中BN层被放置在卷积层之前。 在这种情况下，在层的输入端实现稀疏性，即，该层选择性地使用它接收的通道子集。 为了在测试时获得参数和计算节省，我们需要设置一个通道选择层来屏蔽我们识别出的无关紧要的通道。</p><p><strong>4. Experiments</strong></p><p>We empirically demonstrate the effectiveness of network slimming on several benchmark datasets. We implement our method based on the publicly available Torch [5] implementation for ResNets by [10]. The code is available at <a href="https://github.com/liuzhuang13/slimming">https://github.com/liuzhuang13/slimming</a></p><p>我们经验性地证明了网络瘦身对几个基准数据集的有效性。 我们基于[10]的ResNets的公开可用的Torch 版本[5]实现来验证我们的方法。 该代码可在<a href="https://github.com/liuzhuang13/slimming%E8%8E%B7%E5%BE%97">https://github.com/liuzhuang13/slimming获得</a></p><p><strong>4.1. Datasets</strong></p><p><strong>CIFAR</strong>. The two CIFAR datasets [21] consist of natural images with resolution 32×32. CIFAR-10 is drawn from 10 and CIFAR-100 from 100 classes. The train and test sets contain 50,000 and 10,000 images respectively. On CIFAR10, a validation set of 5,000 images is split from the training set for the search of λ (in Equation 1) on each model. We report the final test errors after training or fine-tuning on all training images. A standard data augmentation scheme (shifting/mirroring) [14, 18, 24] is adopted. The input data is normalized using channel means and standard deviations. We also compare our method with [23] on CIFAR datasets.</p><p>CIFAR。 两个CIFAR数据集[21]由分辨率为32×32的自然图像组成。 CIFAR-10有10个类，CIFAR-100有100个类。 训练和测试集分别包含50,000和10,000个图像。 在CIFAR10上，从训练集中分离出5,000个图像的验证集，用于在每个模型上搜索λ（在等式1中）。 我们在训练或微调所有训练图像后报告最终的测试错误。 采用标准数据增强方案（移位/镜像）[14,18,24]。 使用通道平均值和标准偏差对输入数据进行标准化。 我们还将我们的方法与[23]在CIFAR数据集上进行了比较。</p><p><strong>SVHN</strong>. The Street View House Number (SVHN) dataset [27] consists of 32x32 colored digit images. Following common practice [9, 18, 24] we use all the 604,388 training images, from which we split a validation set of 6,000 images for model selection during training. The test set contains 26,032 images. During training, we select the model with the lowest validation error as the model to be pruned (or the baseline model). We also report the test errors of the models with lowest validation errors during ﬁne-tuning.</p><p>SVHN。 街景房号（SVHN）数据集[27]由32x32彩色数字图像组成。 按照惯例[9,18,24]，我们使用了所有604,388个训练图像，我们在训练期间从中分割出6,000个图像的验证集，用于模型选择。 测试集包含26,032个图像。 在训练期间，我们选择具有最低验证误差的模型作为要修剪的模型（或基线模型）。 我们还报告了模型的测试错误，在调整期间具有最低的验证错误。</p><p><strong>ImageNet</strong>. The ImageNet dataset contains 1.2 million training images and 50,000 validation images of 1000 classes. We adopt the data augmentation scheme as in [10]. We report the single-center-crop validation error of the ﬁnal model.</p><p>ImageNet。 ImageNet数据集包含120万个训练图像和50,000个验证图像，总共有1000个类。 我们采用[10]中的数据增强方案。 我们报告了最终模型的单中心作物验证错误。</p><p><strong>MNIST</strong>. MNIST is a handwritten digit dataset containing 60,000 training images and 10,000 test images. To test the effectiveness of our method on a fully-connected network (treating each neuron as a channel with 1×1 spatial size), we compare our method with [35] on this dataset.</p><p>MNIST。 MNIST是一个手写的数字数据集，包含60,000个训练图像和10,000个测试图像。 为了测试我们的方法在完全连接的网络上的有效性（将每个神经元视为1×1空间大小的通道），我们将该方法与[35]在该数据集上进行比较</p><p><strong>4.2. Network Models</strong></p><p>On CIFAR and SVHN dataset, we evaluate our method on three popular network architectures: VGGNet[31], ResNet [14] and DenseNet [17]. The VGGNet is originally designed for ImageNet classiﬁcation. For our experiment a variation of the original VGGNet for CIFAR dataset is taken from [36]. For ResNet, a 164-layer pre-activation ResNet with bottleneck structure (ResNet-164) [15] is used. For DenseNet, we use a 40-layer DenseNet with growth rate 12 (DenseNet-40).</p><p>在CIFAR和SVHN数据集上，我们在三种流行的网络架构上评估我们的方法：VGGNet [31]，ResNet [14]和DenseNet [17]。 VGGNet最初是为ImageNet分类而设计的。 对于我们的实验，原始VGGNet的变体取自[36] 在CIFAR数据集的结果。 对于ResNet，使用具有瓶颈结构的164层预激活ResNet（ResNet-164）[15]。 对于DenseNet，我们使用40层DenseNet，增长率为12（DenseNet-40）。</p><p>On ImageNet dataset, we adopt the 11-layer (8-conv + 3 FC) “VGG-A” network [31] model with batch normalization from [4]. We remove the dropout layers since we use relatively heavy data augmentation. To prune the neurons in fully-connected layers, we treat them as convolutional channels with 1×1 spatial size.</p><p>在ImageNet数据集中，我们采用11层（8-conv + 3 FC）“VGG-A”网络[31]模型，并从[4]中进行批量归一化。 我们删除了dropout层，因为我们使用很多的数据扩充。 为了修剪完全连接层中的神经元，我们将它们视为具有1×1空间大小的卷积通道。</p><p>On MNIST dataset, we evaluate our method on the same 3-layer fully-connected network as in [35].</p><p>在MNIST数据集上，我们在与[35]中相同的3层全连接网络上评估我们的方法。</p><p><strong>4.3. Training, Pruning and Fine-tuning</strong></p><p><strong>Normal Training</strong>. We train all the networks normally from scratch as baselines. All the networks are trained using SGD. On CIFAR and SVHN datasets we train using minibatch size 64 for 160 and 20 epochs, respectively. The initial learning rate is set to 0.1, and is divided by 10 at 50% and 75% of the total number of training epochs. On ImageNet and MNIST datasets, we train our models for 60 and 30 epochs respectively, with a batch size of 256, and an initial learning rate of 0.1 which is divided by 10 after 1/3 and 2/3 fraction of training epochs. We use a weight decay of 10−4 and a Nesterov momentum [33] of 0.9 without dampening. The weight initialization introduced by [13] is adopted. Our optimization settings closely follow the original implementation at [10]. In all our experiments, we initialize all channel scaling factors to be 0.5, since this gives higher accuracy for the baseline models compared with default setting (all initialized to be 1) from [10].</p><p>正常训练。 我们通常从头开始训练所有网络作为基线。 所有网络都使用SGD进行训练。 在CIFAR和SVHN数据集上，我们分别使用尺寸为64的小批量训练160和20个epochs。 初始学习率设置为0.1，并且在训练epoch总数的50％和75％处除以10。 在ImageNet和MNIST数据集上，我们分别训练60和30个epochs的模型，批量大小为256，初始学习率为0.1，在1/3和2/3的训练epoch之后除以10。 我们使用10-4的重量衰减和0.9的Nesterov动量[33],不使用权重衰减（对么？）。 采用[13]引入的权重初始化。 我们的优化设置参考[10]原始实现。 在我们的所有实验中，我们将所有通道缩放因子初始化为0.5，因为与[10]中的默认设置（全部初始化为1）相比，这为基线模型提供了更高的准确性。</p><p><strong>Training with Sparsity.</strong> For CIFAR and SVHN datasets, when training with channel sparse regularization, the hyper parameter λ, which controls the tradeoff between empirical loss and sparsity, is determined by a grid search over 10−3, 10−4, 10−5 on CIFAR-10 validation set. For VGGNet we choose λ=10−4 and for ResNet and DenseNet λ=10−5. For VGG-A on ImageNet, we set λ=10−5. All other settings are kept the same as in normal training.</p><p>稀疏训练。 对于CIFAR和SVHN数据集，当使用通道稀疏正则化训练时，控制经验损失和稀疏度之间权衡的超参数λ由CIFAR-10上的10-3,10-4,10-5的网格搜索确定。 验证集。 对于VGGNet，我们选择λ= 10-4，对于ResNet和DenseNet，λ= 10-5。 对于ImageNet上的VGG-A，我们设置λ= 10-5。 所有其他设置保持与正常训练相同。</p><p><strong>Pruning</strong>. When we prune the channels of models trained with sparsity, a pruning threshold on the scaling factors needs to be determined. Unlike in [23] where different layers are pruned by different ratios, we use a global pruning threshold for simplicity. The pruning threshold is determined by a percentile among all scaling factors , e.g., 40% or 60% channels are pruned. The pruning process is implemented by building a new narrower model and copying the corresponding weights from the model trained with sparsity.</p><p>修剪。 当我们修剪用稀疏性训练的模型的通道时，需要确定缩放因子的修剪阈值。 与[23]不同，不同的层以不同的比例进行修剪，为简单起见，我们使用全局修剪阈值。 修剪阈值由所有缩放因子中的百分位数确定，例如，修剪40％或60％的通道。 修剪过程是通过构建一个新的较窄的模型并从稀疏训练的模型中复制相应的权重来实现的。</p><p><strong>Fine-tuning</strong>. After the pruning we obtain a narrower and more compact model, which is then ﬁne-tuned. On CIFAR, SVHN and MNIST datasets, the ﬁne-tuning uses the same optimization setting as in training. For ImageNet dataset, due to time constraint, we ﬁne-tune the pruned VGG-A with a learning rate of 10−3 for only 5 epochs.</p><p>微调。 在修剪之后，我们获得了一个更窄更紧凑的模型，然后进行了调整。 在CIFAR，SVHN和MNIST数据集上，微调使用与训练相同的优化设置。 对于ImageNet数据集，由于时间限制，我们仅在5个epochs内以10-3的学习速率调整修剪的VGG-A。</p><p><strong>4.4. Results</strong></p><p><strong>CIFAR and SVHN</strong> The results on CIFAR and SVHN are shown in Table 1. We mark all lowest test errors of a model in boldface.</p><p>CIFAR和SVHN  CIFAR和SVHN的结果如表1所示。我们用粗体标记模型的所有最低测试误差。</p><p><img src="/blog/blog/2022/04/29/learning-efficient-convolutional-networks-through-network-slimming/Learning-Efficient-Convolutional-Networks-through-Network-Slimming.assets/1571518-20220429211720975-1001210397.png"></p><p><strong>Parameter and FLOP reductions</strong>. The purpose of network slimming is to reduce the amount of computing resources needed. The last row of each model has ≥ 60% channels pruned while still maintaining similar accuracy to the baseline. The parameter saving can be up to 10×. The FLOP reductions are typically around 50%. To highlight network slimming’s efﬁciency, we plot the resource savings in Figure 3. It can be observed that VGGNet has a large amount of redundant parameters that can be pruned. On ResNet-164 the parameter and FLOP savings are relatively insigniﬁcant, we conjecture this is due to its “bottleneck” structure has already functioned as selecting channels. Also, on CIFAR-100 the reduction rate is typically slightly lower than CIFAR-10 and SVHN, which is possibly due to the fact that CIFAR-100 contains more classes.</p><p>参数和FLOP减少。 网络瘦身的目的是减少所需的计算资源量。 每个模型的最后一行修剪了≥60％的通道，同时仍然保持与基线相似的精度。 参数保存最高可达10倍。 FLOP减少量通常约为50％。 为了突出网络瘦身的效率，我们绘制了图3中的资源节省情况。可以观察到VGGNet具有大量可以修剪的冗余参数。 在ResNet-164上，参数和FLOP节省是相对不重要的，我们推测这是由于其“瓶颈”结构已经起到选择通道的作用。 此外，在CIFAR-100上，降低率通常略低于CIFAR-10和SVHN，这可能是由于CIFAR-100包含更多类。</p><p><img src="/blog/blog/2022/04/29/learning-efficient-convolutional-networks-through-network-slimming/Learning-Efficient-Convolutional-Networks-through-Network-Slimming.assets/1571518-20220429211657743-1630001899.png"></p><p><strong>Regularization Effect.</strong> From Table 1, we can observe that, on ResNet and DenseNet, typically when 40% channels are pruned, the ﬁne-tuned network can achieve a lower test error than the original models. For example, DenseNet-40 with 40% channels pruned achieve a test error of 5.19% on CIFAR-10, which is almost 1% lower than the original model. We hypothesize this is due to the regularization effect of L1 sparsity on channels, which naturally provides feature selection in intermediate layers of a network. We will analyze this effect in the next section.</p><p>正则化影响。 从表1中我们可以看出，在ResNet和DenseNet上，通常在修剪40％的通道时，经过调整的网络可以实现比原始模型更低的测试误差。 例如，具有40％通道修剪的DenseNet-40在CIFAR-10上实现了5.19％的测试误差，这比原始模型低近1％。 我们假设这是由于L1稀疏性对通道的正则化效应，这自然地在网络的中间层中提供特征选择。 我们将在下一节分析这种效果。</p><p><strong>ImageNet</strong>. The results for ImageNet dataset are summarized in Table 2. When 50% channels are pruned, the parameter saving is more than 5×, while the FLOP saving is only 30.4%. This is due to the fact that only 378 (out of 2752) channels from all the computation-intensive convolutional layers are pruned, while 5094 neurons (out of 8192) from the parameter-intensive fully-connected layers are pruned. It is worth noting that our method can achieve the savings with no accuracy loss on the 1000-class ImageNet dataset, where other methods for efﬁcient CNNs [2, 23, 35, 28] mostly report accuracy loss.</p><p>ImageNet。 ImageNet数据集的结果总结在表2中。当修剪50％通道时，参数节省超过5倍，而FLOP节省仅为30.4％。 这是因为所有计算密集型卷积层中仅有378个（2752个）通道被修剪，而来自参数密集型全连接层的5094个神经元（8192个）被修剪。 值得注意的是，我们的方法可以在1000个类ImageNet数据集上实现节省而没有精度损失，其中有效CNN的其他方法[2,23,35,28]都有报告精度损失。</p><p><img src="/blog/blog/2022/04/29/learning-efficient-convolutional-networks-through-network-slimming/Learning-Efficient-Convolutional-Networks-through-Network-Slimming.assets/1571518-20220429211631759-1655474150.png"></p><p><strong>MNIST</strong>. On MNIST dataset, we compare our method with the Structured Sparsity Learning (SSL) method [35] in Table 3. Despite our method is mainly designed to prune channels in convolutional layers, it also works well in pruning neurons in fully-connected layers. In this experiment, we observe that pruning with a global threshold sometimes completely removes a layer, thus we prune 80% of the neurons in each of the two intermediate layers. Our method slightly outperforms [35], in that a slightly lower test error is achieved while pruning more parameters.</p><p>MNIST。 在MNIST数据集上，我们将我们的方法与表3中的结构化稀疏度学习（SSL）方法[35]进行了比较。尽管我们的方法主要用于修剪卷积层中的通道，但它也可以很好地修剪完全连接层中的神经元。 在这个实验中，我们观察到用全局阈值修剪有时会完全去除一层，因此我们修剪了两个中间层中每个中80％的神经元。 我们的方法稍微优于[35]，因为在修剪更多参数时实现了略低的测试误差。</p><p><img src="/blog/blog/2022/04/29/learning-efficient-convolutional-networks-through-network-slimming/Learning-Efficient-Convolutional-Networks-through-Network-Slimming.assets/1571518-20220429211524871-1347192300.png"></p><p>We provide some additional experimental results in the supplementary materials, including (1) detailed structure of a compact VGGNet on CIFAR-10; (2) wall-clock time and run-time memory savings in practice. (3) comparison with a previous channel pruning method [23];</p><p>我们在补充材料中提供了一些额外的实验结果，包括（1）CIFAR-10上紧凑型VGGNet的详细结构; （2）在实践中节省挂钟时间（怎么翻译？加载时间？）和运行时间。 （3）与以前的通道修剪方法[23]进行比较;</p><p><strong>4.5. Results for Multi-pass Scheme</strong></p><p>We employ the multi-pass scheme on CIFAR datasets using VGGNet. Since there are no skip-connections, pruning away a whole layer will completely destroy the models. Thus, besides setting the percentile threshold as 50%, we also put a constraint that at each layer, at most 50% of channels can be pruned.</p><p>我们在使用VGGNet的CIFAR数据集上采用多程方案。 由于没有跳跃连接，修剪整个层将完全破坏模型。 因此，除了将百分位数阈值设置为50％之外，我们还设置了一个约束，即在每一层，最多可以修剪50％的通道。</p><p>The test errors of models in each iteration are shown in Table 4. As the pruning process goes, we obtain more and more compact models. On CIFAR-10, the trained model achieves the lowest test error in iteration 5. This model achieves 20× parameter reduction and 5× FLOP reduction, while still achieving lower test error. On CIFAR-100, after iteration 3, the test error begins to increase. This is possibly due to that it contains more classes than CIFAR-10, so pruning channels too agressively will inevitably hurt the performance. However, we can still prune near 90% parameters and near 70% FLOPs without notable accuracy loss.</p><p>每次迭代中模型的测试误差如表4所示。随着修剪过程的进行，我们获得了越来越紧凑的模型。 在CIFAR-10上，经过训练的模型在迭代5次实现了最低的测试误差。该模型实现了20倍的参数减少和5×FLOP减少，同时仍然实现了较低的测试误差。 在CIFAR-100上，在迭代3次之后，测试误差开始增加。 这可能是因为它包含比CIFAR-10更多的类，因此修剪频道过于激进将不可避免地损害性能。 然而，我们仍然可以修剪接近90％的参数和接近70％的FLOP而没有明显的精度损失。</p><p><img src="/blog/blog/2022/04/29/learning-efficient-convolutional-networks-through-network-slimming/Learning-Efficient-Convolutional-Networks-through-Network-Slimming.assets/1571518-20220429211506124-506708212.png"></p><p><strong>5. Analysis</strong></p><p>There are two crucial hyper-parameters in network slimming, the pruned percentage t and the coefﬁcient of the sparsity regularization term λ (see Equation 1). In this section, we analyze their effects in more detail.</p><p>网络瘦身有两个关键的超参数，修剪百分比t和稀疏正则化项λ的系数（见公式1）。 在本节中，我们将更详细地分析它们的影响。</p><p><strong>Effect of Pruned Percentage</strong>. Once we obtain a model trained with sparsity regularization, we need to decide what percentage of channels to prune from the model. If we prune too few channels, the resource saving can be very limited. However, it could be destructive to the model if we prune too many channels, and it may not be possible to recover the accuracy by ﬁne-tuning. We train a DenseNet40 model with λ=10−5 on CIFAR-10 to show the effect of pruning a varying percentage of channels. The results are summarized in Figure 5.</p><p>修剪百分比的影响。 一旦我们获得了通过稀疏正则化训练的模型，我们需要确定从模型中修剪的通道百分比。 如果我们修剪太少的频道，节省的资源可能会非常有限。 然而，如果我们修剪太多通道，它可能对模型具有破坏性，并且可能无法通过微调来恢复精度。 我们在CIFAR-10上训练一个λ= 10-5的DenseNet40模型，以显示修剪不同百分比通道的效果。 结果总结在图5中。</p><p><img src="/blog/blog/2022/04/29/learning-efficient-convolutional-networks-through-network-slimming/Learning-Efficient-Convolutional-Networks-through-Network-Slimming.assets/1571518-20220429211411580-807933228.png"></p><p>From Figure 5, it can be concluded that the classiﬁcation performance of the pruned or ﬁne-tuned models degrade only when the pruning ratio surpasses a threshold. The ﬁne tuning process can typically compensate the possible accuracy loss caused by pruning. Only when the threshold goes beyond 80%, the test error of ﬁne-tuned model falls behind the baseline model. Notably, when trained with sparsity, even without ﬁne-tuning, the model performs better than the original model. This is possibly due the the regularization effect of L1 sparsity on channel scaling factors.</p><p>从图5中可以得出结论，修剪或调整后的模型的分类性能仅在修剪比超过阈值时才会降低。 精细调整过程通常可以补偿由修剪引起的可能的精度损失。 只有当阈值超过80％时，精细模型的测试误差才会落后于基线模型。 值得注意的是，当训练有稀疏化时，即使没有进行微调，该模型也比原始模型表现更好。 这可能是由于L1稀疏化对信道缩放因子的正则化效应。</p><p>Channel Sparsity Regularization. The purpose of the L1 sparsity term is to force many of the scaling factors to be near zero. The parameter λ in Equation 1 controls its significance compared with the normal training loss. In Figure 4 we plot the distributions of scaling factors in the whole network with different λ values. For this experiment we use a VGGNet trained on CIFAR-10 dataset.</p><p>通道稀疏性正规化。 L1稀疏项的目的是迫使许多缩放因子接近零。 等式1中的参数λ控制其与正常训练损失相比的显着性。 在图4中，我们绘制了具有不同λ值的整个网络中缩放因子的分布。 对于本实验，我们使用在CIFAR-10数据集上训练的VGGNet。</p><p><img src="/blog/blog/2022/04/29/learning-efficient-convolutional-networks-through-network-slimming/Learning-Efficient-Convolutional-Networks-through-Network-Slimming.assets/1571518-20220429211354938-1212849920.png"></p><p>It can be observed that with the increase of λ, the scaling factors are more and more concentrated near zero. When λ=0, i.e., there’s no sparsity regularization, the distribution is relatively ﬂat. When λ=10−4, almost all scaling factors fall into a small region near zero. This process can be seen as a feature selection happening in intermediate layers of deep networks, where only channels with non-negligible scaling factors are chosen. We further visualize this process by a heatmap. Figure 6 shows the magnitude of scaling factors from one layer in VGGNet, along the training process. Each channel starts with equal weights; as the training progresses, some channels’ scaling factors become larger (brighter) while others become smaller (darker).</p><p>可以观察到，随着λ的增加，比例因子越来越集中在零附近。 当λ= 0时，即没有稀疏正则化时，分布相对较小。 当λ= 10-4时，几乎所有比例因子都落入接近零的小区域。 该过程可被视为在深度网络的中间层中发生的特征选择，其中仅选择具有不可忽略的缩放因子的通道。 我们通过热图进一步可视化此过程。 图6显示了VGGNet中沿着训练过程的一个层的缩放因子的大小。 每个通道以相同的权重开始; 随着训练的进行，一些频道的缩放因子变得更大（更亮），而其他频道变得更小（更暗）。</p><p><img src="/blog/blog/2022/04/29/learning-efficient-convolutional-networks-through-network-slimming/Learning-Efficient-Convolutional-Networks-through-Network-Slimming.assets/1571518-20220429211322683-625462196.png"></p><p><strong>6. Conclusion</strong></p><p>We proposed the network slimming technique to learn more compact CNNs. It directly imposes sparsity-induced regularization on the scaling factors in batch normalization layers, and unimportant channels can thus be automatically identiﬁed during training and then pruned. On multiple datasets, we have shown that the proposed method is able to signiﬁcantly decrease the computational cost (up to 20×) of state-of-the-art networks, with no accuracy loss. More importantly, the proposed method simultaneously reduces the model size, run-time memory, computing operations while introducing minimum overhead to the training process, and the resulting models require no special libraries/hardware for efﬁcient inference.</p><p>我们提出了网络瘦身技术来学习更紧凑的CNN。 它直接对批量归一化层中的缩放因子施加稀疏诱导的正则化，因此可以在训练期间自动识别不重要的通道，然后进行修剪。 在多个数据集上，我们已经表明，所提出的方法能够显着降低最先进网络的计算成本（高达20倍），没有精度损失。 更重要的是，所提出的方法同时减少了模型大小，运行时内存，计算操作，同时为训练过程引入了最小的开销，并且所得到的模型不需要特殊的库/硬件来进行有效的推理。</p><p><strong>References</strong></p><p>[1] B. Baker, O. Gupta, N. Naik, and R. Raskar. Designing neural network architectures using reinforcement learning. In ICLR, 2017. </p><p>[2] S. Changpinyo, M. Sandler, and A. Zhmoginov. The power of sparsity in convolutional neural networks. arXiv preprint arXiv:1702.06257, 2017. </p><p>[3] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen. Compressing neural networks with the hashing trick. In ICML, 2015. </p><p>[4] S. Chintala. Training an object classiﬁer in torch-7 on multiple gpus over imagenet. <a href="https://github.com/">https://github.com/</a> soumith/imagenet-multiGPU.torch. </p><p>[5] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine learning. In BigLearn, NIPS Workshop, number EPFL-CONF-192376, 2011. </p><p>[6] M. Courbariaux and Y. Bengio. Binarynet: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016. </p><p>[7] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Exploiting linear structure within convolutional networks for efﬁcient evaluation. In NIPS, 2014. </p><p>[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, pages 580–587, 2014. </p><p>[9] I. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks. In ICML, 2013. </p><p>[10] S. Gross and M. Wilber. Training and investigating residual nets. <a href="https://github.com/szagoruyko/cifar">https://github.com/szagoruyko/cifar</a>. torch. </p><p>[11] S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. In ICLR, 2016. </p><p>[12] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights and connections for efﬁcient neural network. In NIPS, pages 1135–1143, 2015. </p><p>[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In ICCV, 2015. </p><p>[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016. </p><p>[15] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, pages 630–645. Springer, 2016. </p><p>[16] G. Huang, D. Chen, T. Li, F. Wu, L. van der Maaten, and K. Q. Weinberger. Multi-scale dense convolutional networks for efﬁcient prediction. arXiv preprint arXiv:1703.09844, 2017. </p><p>[17] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten. Densely connected convolutional networks. In CVPR, 2017. </p><p>[18] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger. Deep networks with stochastic depth. In ECCV, 2016. </p><p>[19] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.</p><p>[20] J. Jin, Z. Yan, K. Fu, N. Jiang, and C. Zhang. Neural network architecture optimization through submodularity and supermodularity. arXiv preprint arXiv:1609.00074, 2016. </p><p>[21] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. In Tech Report, 2009.</p><p>[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In NIPS, pages 1097–1105, 2012. </p><p>[23] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. Pruning ﬁlters for efﬁcient convnets. arXiv preprint arXiv:1608.08710, 2016. </p><p>[24] M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR, 2014. </p><p>[25] B. Liu, M. Wang, H. Foroosh, M. Tappen, and M. Pensky. Sparse convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 806–814, 2015. </p><p>[26] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, pages 3431– 3440, 2015. </p><p>[27] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning, 2011. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. </p><p>[28] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnornet: Imagenet classiﬁcation using binary convolutional neural networks. In ECCV, 2016. </p><p>[29] S. Scardapane, D. Comminiello, A. Hussain, and A. Uncini. Group sparse regularization for deep neural networks. arXiv preprint arXiv:1607.00485, 2016. </p><p>[30] M. Schmidt, G. Fung, and R. Rosales. Fast optimization methods for l1 regularization: A comparative study and two new approaches. In ECML, pages 286–297, 2007. </p><p>[31] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. </p><p>[32] S. Srinivas, A. Subramanya, and R. V. Babu. Training sparse neural networks. CoRR, abs/1611.06694, 2016. </p><p>[33] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum in deep learning. In ICML, 2013. </p><p>[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, et al. Going deeper with convolutions. In CVPR, pages 1–9, 2015. </p><p>[35] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning structured sparsity in deep neural networks. In NIPS, 2016. </p><p>[36] S. Zagoruyko. 92.5% on cifar-10 in torch. https:// github.com/szagoruyko/cifar.torch. </p><p>[37] H.Zhou, J.M.Alvarez, andF.Porikli. Lessismore: Towards compact cnns. In ECCV, 2016. </p><p>[38] B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning. In ICLR, 2017.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper_Read </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AlexNet_ImageNet_Classification_with_Deep_Convolutional_Neural_Networks</title>
      <link href="/blog/2022/04/29/alexnet-imagenet-classification-with-deep-convolutional-neural-networks/"/>
      <url>/blog/2022/04/29/alexnet-imagenet-classification-with-deep-convolutional-neural-networks/</url>
      
        <content type="html"><![CDATA[<h1 id="ImageNet-Classification-with-Deep-Convolutional-Neural-Networks"><a href="#ImageNet-Classification-with-Deep-Convolutional-Neural-Networks" class="headerlink" title="ImageNet Classification with Deep Convolutional Neural Networks"></a>ImageNet Classification with Deep Convolutional Neural Networks</h1><p>Alex Krizhevsky University of Toronto <a href="mailto:&#107;&#114;&#x69;&#x7a;&#64;&#99;&#115;&#x2e;&#117;&#116;&#x6f;&#114;&#x6f;&#x6e;&#x74;&#111;&#x2e;&#x63;&#x61;">&#107;&#114;&#x69;&#x7a;&#64;&#99;&#115;&#x2e;&#117;&#116;&#x6f;&#114;&#x6f;&#x6e;&#x74;&#111;&#x2e;&#x63;&#x61;</a><br>Ilya Sutskever University of Toronto <a href="mailto:&#105;&#108;&#121;&#x61;&#x40;&#99;&#x73;&#x2e;&#x75;&#116;&#111;&#x72;&#x6f;&#x6e;&#x74;&#x6f;&#46;&#x63;&#x61;">&#105;&#108;&#121;&#x61;&#x40;&#99;&#x73;&#x2e;&#x75;&#116;&#111;&#x72;&#x6f;&#x6e;&#x74;&#x6f;&#46;&#x63;&#x61;</a><br>GeoffreyE. Hinton University of Toronto <a href="mailto:&#x68;&#x69;&#x6e;&#116;&#x6f;&#110;&#64;&#99;&#115;&#x2e;&#117;&#x74;&#x6f;&#x72;&#x6f;&#110;&#x74;&#111;&#46;&#99;&#x61;">&#x68;&#x69;&#x6e;&#116;&#x6f;&#110;&#64;&#99;&#115;&#x2e;&#117;&#x74;&#x6f;&#x72;&#x6f;&#110;&#x74;&#111;&#46;&#99;&#x61;</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.</p><p>本文训练了一个大规模的深度卷积神经网络来将ImageNet LSVRC-2010比赛中的包含120万幅高分辨率的图像数据集分为1000种不同类别。在测试集上，本文所得的top-1和top-5错误率分别为37.5%和17.0%，该测试结果大大优于当前的最佳水平。本文的神经网络包含6千万个参数和65万个神经元，包含了5个卷积层，其中有几层后面跟着最大池化（max-pooling）层，以及3个全连接层，最后还有一个1000路的softmax层。为了加快训练速度，本文使用了不饱和神经元以及一种高效的基于GPU的卷积运算方法。为了减少全连接层的过拟合，本文采用了最新的正则化方法“dropout”，该方法被证明非常有效。我们以该模型的变体参加了ILSVRC-2012比赛，相比第二名26.2%，我们以15.3%的top-5测试错误率获胜。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>Current approaches to object recognition make essential use of machine learning methods. To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing over ﬁtting. Until recently, datasets of labeled images were relatively small — on the order of tens of thousands of images (e.g., NORB [16], Caltech-101/256 [8, 9], and CIFAR-10/100 [12]). Simple recognition tasks can be solved quite well with datasets of this size, especially if they are augmented with label-preserving transformations. For example, the current best error rate on the MNIST digit-recognition task (&lt;0.3%) approaches human performance [4]. But objects in realistic settings exhibit considerable variability, so to learn to recognize them it is necessary to use much larger training sets. And indeed, the shortcomings of small image datasets have been widely recognized (e.g., Pinto et al [21]), but it has only recently become possible to collect labeled datasets with millions of images. The new larger datasets include LabelMe [23], which consists of hundreds of thousands of fully-segmented images, and ImageNet [6], which consists of over 15 million labeled high-resolution images in over 22,000 categories.</p><p>当前目标识别的方法基本都使用了机器学习的方法。为了提高这些方法的性能，我们可以收集更大的数据集，学习得到更加强大的模型，然后使用更好的方法防止过拟合。直到现在，相比于成千上百的图像，带标签的图像数据集相对较小（如NORB[16]，Caltech-101/256[8,9]，以及CIFAR-10/100[12]）。这种规模的数据集能使得简单的识别任务得到很好地解决，特别是如果他们进行带标签的转换来增广数据集。例如，当前MINIST数字识别任务最小的错误率（&lt;0.3% ）已经接近人类水平[4]。但是现实世界中的目标呈现出相当大的变化性，因此学习去识别它们就必须要使用更大的训练数据集。事实上，人们也已广泛地认识到小图像数据集的缺点（如Pinto等[21]），但直到最近，收集包含数百万图像的带标签数据集才成为可能。新的更大的数据集包括由数十万张全分割图像的LabelMe[23]和包含超过22000类的1500万张带标签高分辨率图像ImageNet[6]组成。</p><p>To learn about thousands of objects from millions of images, we need a model with a large learning capacity. However, the immense complexity of the object recognition task means that this problem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots of prior knowledge to compensate for all the data we don’t have. Convolutional neural networks (CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be controlled by varying their depth and breadth, and they also make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies). Thus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have much fewer connections and parameters and so they are easier to train, while their theoretically-best performance is likely to be only slightly worse.</p><p>为了从数以百万计的图像中学习出数千种的目标，我们需要一个具有很强学习能力的模型。然而，目标识别任务的巨大复杂性意味着，即使在ImageNet这样大的数据集也不能完成任务，因此我们的模型也要有许多先验知识来弥补所有我们没有的数据。卷积神经网络（CNNs）就形成了一种这样类别的模型[16，11,13,18,15,22,26]。可以通过改变网络的深度和广度控制CNN的学习能力，并且它们都能对图像的本质做出强大而又正确的判别（即统计的稳定性和像素位置的依赖性）。因此，相比于相似大小的标准前馈神经网络，CNNs的连接和参数更少，因此更易训练，尽管它们理论上的最优性能可能略差点。<br>Despite the attractive qualities of CNNs, and despite the relative efficiency of their local architecture, they have still been prohibitively expensive to apply in large scale to high-resolution images. Luckily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful enough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet contain enough labeled examples to train such models without severe overfitting.</p><p>尽管CNNs具有一些新颖的特性，和更有效率的局部结构，但大规模地应用于高分辨率图像消耗资源仍然过多。幸运的是，如今GPU以及高度优化的二维卷积计算，已经足够强大地去帮助大规模CNNs的训练，并且最新的数据集如ImageNet包含足够多的带标签样本，能够训练出不会严重过拟合的模型。<br>The specific contributions of this paper are as follows: we trained one of the largest convolutional neural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012 competitions [2] and achieved by far the best results ever reported on these datasets. We wrote a highly-optimized GPU implementation of 2D convolution and all the other operations inherent in training convolutional neural networks, which we make available publicly1. Our network contains a number of new and unusual features which improve its performance and reduce its training time, which are detailed in Section3. The size of our network made overfitting a significant problem, even with 1.2 million labeled training examples, so we used several effective techniques for preventing overfitting, which are described in Section 4. Our final network contains five convolutional and three fully-connected layers, and this depth seems to be important: we found that removing any convolutional layer (each of which contains no more than 1% of the model’s parameters) resulted in inferior performance.</p><p>本文具体贡献如下：基于ILSVRC-2010和ILSVRC-2012比赛中用到的ImageNet的子集本文训练出了至今为止一个最大的卷积神经网络[2]并且得到了迄今基于这些数据集最好的结果。本文实现了一种高度优化的二维卷积的GPU运算以及卷积神经网络训练中所有其他运算，这些都已公开提供；本文网络中包含了大量的不常见和新的特征来提升网络性能，减少训练时间，详见第三节；即使有120万带标签的训练样本，网络的大小使得过拟合仍成为一个严重的问题，因此本文使用了许多有效的防止过拟合的技术，详见第四节；本文最终的网络包含五层卷积层和三层全连接层，而这个深度似乎很重要：我们发现移除任何一层卷积层（每一层包含的参数个数不超过整个模型参数个数的1%）都会导致较差的结果。</p><p>In the end, the network’s size is limited mainly by the amount of memory available on current GPUs and by the amount of training time that we are willing to tolerate. Our network takes between five and six days to train on two GTX 580 3GB GPUs. All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available.</p><p>最后，网络的大小主要受限于GPU的内存大小和我们愿意忍受的训练时间长度。本文的网络在两个GTX 580 3GB GPU上训练了五到六天。本文所有的实验表明，如果有更快的GPU、更大的数据集，结果可以更好。</p><h2 id="2-The-Dataset"><a href="#2-The-Dataset" class="headerlink" title="2 The Dataset"></a>2 The Dataset</h2><p>ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.</p><p>ImageNet数据集包含有大概22000种类别共150多万带标签的高分辨率图像。这些图像是从网络上收集得来，由亚马逊的Mechanical Turkey的众包工具进行人工标记。从2010年开始，作为Pascal视觉目标挑战的一部分，ImageNet大规模视觉识别挑战（ImageNet Large-Scale Visual Recognition Challenge ，ILSVRC）比赛每年都会举行。ILSVRC采用ImageNet的子集，共包含一千个类别，每个类别包含大约1000幅图像。总的来说，大约有120万张训练图像，5万张验证图像以及15万张测试图像。</p><p>ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is the version on which we performed most of our experiments. Since we also entered our model in the ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as well, for which test set labels are unavailable. On ImageNet, it is customary to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model.</p><p>ILSVRC-2010是ILSVRC唯一一个测试集标签公开的版本，因此这个版本就是本文大部分实验采用的数据集。由于我们也以我们的模型参加了ILSVRC-2012的比赛，在第6节本文也会列出在这个数据集上的结果，该测试集标签不可获取。ImageNet通常使用两种错误率：top-1和top-5，其中top-5错误率是指正确标签不在模型认为最有可能的前五个标签中的测试图像的百分数。</p><p>ImageNet consists of variable-resolution images, while our system requires a constant input dimensionality. Therefore, we down-sampled the images to a fixed resolution of 256 × 256. Given a rectangular image, we first rescaled the image such that the shorter side was of length 256, and then cropped out the central 256×256 patch from the resulting image. We did not pre-process the images in any other way, except for subtracting the mean activity over the training set from each pixel. So we trained our network on the (centered) raw RGB values of the pixels.</p><p>ImageNet包含不同分辨率的图像，但是本文的模型要求固定的输入维度。因此，本文将这些图像下采样为256x256 。给定一幅矩形图像，本文采用的方法是首先重新调整图像使得短边长度为256，然后裁剪出中央256x256 的区域。除了将图像减去训练集的图像均值(训练集和测试集都减去训练集的图像均值)，本文不做任何其他图像预处理。因此本文直接在每个像素的原始RGB值上进行训练。</p><h2 id="3-The-Architecture"><a href="#3-The-Architecture" class="headerlink" title="3 The Architecture"></a>3 The Architecture</h2><p>The architecture of our network is summarized in Figure 2. It contains eight learned layers — ve convolutional and three fully-connected. Below, we describe some of the novel or unusual features of our network’s architecture. Sections 3.1-3.4 are sorted according to our estimation of their importance, with the most important first.<br>本文网络结构详见图2。它包含8层学习层——5层卷积层和三层全连接层。下面将描述该网络结构的一些创新和新的特征。3.1节至3.4节根据他们的重要性从大到小排序。</p><p><img src="/blog/blog/2022/04/29/alexnet-imagenet-classification-with-deep-convolutional-neural-networks/AlexNet-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks.assets/1571518-20220429205402634-16533092.png"></p><p>Figure 2: An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilities between the two GPUs. One GPU runs the layer-parts at the top of the ﬁgure while the other runs the layer-parts at the bottom. The GPUs communicate only at certain layers. The network’s inputis150,528-dimensional, and the number of neurons in the network’s remaining layers is given by 253,440–186,624–64,896–64,896–43,264– 4096–4096–1000.</p><p>图2 本文CNN的结构图示，明确地描述了两个GPU之间的职责。一个GPU运行图上方的层，另一个运行图下方的层。两个GPU只在特定的层通信。网络的输入是150,528维的，网络剩余层中的神经元数目分别是253440，186624，64896，64896，43264，4096，4096，1000</p><h3 id="3-1-ReLU-Nonlinearity"><a href="#3-1-ReLU-Nonlinearity" class="headerlink" title="3.1 ReLU Nonlinearity"></a>3.1 ReLU Nonlinearity</h3><p>The standard way to model a neuron’s output f as a function of its input x is with f(x) = tanh(x) or f(x) = (1 + e−x)−1. In terms of training time with gradient descent, these saturating nonlinearities are much slower than the non-saturating nonlinearity f(x) = max(0,x). Following Nair and Hinton [20], we refer to neurons with this nonlinearity as Rectified Linear Units (ReLUs). Deep convolutional neural networks with ReLUs train several times faster than their equivalents with tanh units. This is demonstrated in Figure 1, which shows the number of iterations required to reach 25% training error on the CIFAR-10 dataset for a particular four-layer convolutional network. This plot shows that we would not have been able to experiment with such large neural networks for this work if we had used traditional saturating neuron models.</p><p>通常使用一个关于输入x的函数模拟神经元的输出f，这种标准函数是f(x)=tanh(x)或者f(x)=(1+e−x)-1。在梯度下降训练时间上，这些饱和的非线性函数比不饱和非线性函数f(x)=max(0,x)f(x)=max(0,x)更慢。根据Nair和Hinton[20]，本文将具有这种非线性特征的神经元称为修正线性单元（ReLUs: Rectified Linear Units）。使用ReLUs的深度卷积神经网络训练速度比同样情况下使用tanh单元的速度快好几倍。图1表示使用特定的四层卷积网络在数据集CIFAR-10上达到25%错误率所需的迭代次数。这个图表明如果使用传统的饱和神经元模型我们不可能利用这么大规模的神经网络对本文工作进行试验。</p><p><img src="/blog/blog/2022/04/29/alexnet-imagenet-classification-with-deep-convolutional-neural-networks/AlexNet-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks.assets/1571518-20220429205531032-963928677.png"></p><p>Figure 1: A four-layer convolutional neural network with ReLUs(solid line) reaches a 25% training error rate onCIFAR-10 six times faster than an equivalent network with tanh neurons (dashed line). The learning rates for each network were chosen in dependently to make training as fast as possible. No regularization of any kind was employed. The magnitude of the effect demonstrated here varies with network architecture, but networks with ReLUs consistently learn several times faster than equivalents with saturating neurons.</p><p>图1 使用ReLUs（实线）的四层卷积神经网络在CIFAR-10数据集上达到25%训练错误率比同等条件下使用tanh神经元（虚线）快6倍。为了尽可能使得训练速度快，每一个网络的学习速率都是独立选择的。任何一种都没有经过正则化。这里展示的效果的量级随着网络的结构而变化，但是利用ReLUs的网络始终比同等情况下使用饱和神经元的学习速度快很多倍。</p><p>We are not the first to consider alternatives to traditional neuron models in CNNs. For example, Jarrett et al.[11] claim that the nonlinearity f(x) = |tanh(x)| works particularly well with their type of contrast normalization followed by local average pooling on the Caltech-101 dataset. However, on this dataset the primary concern is preventing overfitting, so the effect they are observing is different from the accelerated ability to fit the training set which we report when using ReLUs. Faster learning has a great influence on the performance of large models trained on large datasets.</p><p>本文不是第一个考虑在CNNs中寻找传统神经模型替代方案的。例如，Jarrett等[11]考虑使用非线性函数f(x)=|tanh(x)|，在数据集Caltech-101上，与基于局部平均池化的对比归一化结合取得了很好地效果。但是，在这个数据集上他们主要关心的就是防止过拟合，而本文用ReLUs主要是对训练集的拟合进行加速。快速学习对由大规模数据集上训练出大模型的性能有相当大的影响。</p><h3 id="3-2-Training-on-Multiple-GPUs"><a href="#3-2-Training-on-Multiple-GPUs" class="headerlink" title="3.2 Training on Multiple GPUs"></a>3.2 Training on Multiple GPUs</h3><p>A single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the networks that can be trained on it. It turns out that 1.2 million training examples are enough to train networks which are too big to fit on one GPU. Therefore we spread the net across two GPUs. Current GPUs are particularly well-suited to cross-GPU parallelization, as they are able to read from and write to one another’s memory directly, without going through host machine memory. The parallelization scheme that we employ essentially puts half of the kernels (or neurons) on each GPU, with one additional trick: the GPUs communicate only in certain layers. This means that, for example, the kernels of layer 3 take input from all kernel maps in layer 2. However, kernels in layer 4 take input only from those kernel maps in layer 3 which reside on the same GPU. Choosing the pattern of connectivity is a problem for cross-validation, but this allows us to precisely tune the amount of communication until it is an acceptable fraction of the amount of computation.</p><p>单个GTX 580 GPU只有3GB的内存，从而限制了能由它训练出的网络的最大规模。实验表明使用120万训练样本训练网络已足够，但是这个任务对一个GPU来说太大了。因此，本文中的网络使用两个GPU。当前的GPU都能很方便地进行交叉GPU并行，因为它们可以直接相互读写内存，而不用经过主机内存。我们采用的并行模式本质上来说就是在每一个GPU上放二分之一的核(或者神经元)，我们还使用了另一个技巧：只有某些层才能进行GPU之间的通信。这就意味着，例如第三层的输入为第二层的所有特征图。但是，第四层的输入仅仅是第三层在同一GPU上的特征图。在交叉验证时，连接模式的选择是一个问题，而这个也恰好允许我们精确地调整通信的数量，直到他占计算数量的一个合理比例。</p><p>The resultant architecture is somewhat similar to that of the “columnar” CNN employed by Cires¸an et al.[5],except that our columns are not independent(seeFigure2). This scheme reduces our top-1 and top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as many kernels in each convolutional layer trained on one GPU. The two-GPU net takes slightly less time to train than the one-GPU net2.</p><p>最终的结构有点像Ciresan等[5]采用的柱状卷积神经网络，但是本文的列不是独立的（见图2）。与每个卷积层拥有本文一半的核,并且在一个GPU上训练的网络相比，这种组合让本文的top-1和top-5错误率分别下降了1.7%和1.2%。本文的2-GPU网络训练时间比一个GPU的时间都要略少。</p><h3 id="3-3-Local-Response-Normalization"><a href="#3-3-Local-Response-Normalization" class="headerlink" title="3.3 Local Response Normalization"></a>3.3 Local Response Normalization</h3><p>ReLUs have the desirable property that they do not require input normalization to prevent them from saturating. If at least some training examples produce a positive input to a ReLU, learning will happen in that neuron. However, we still find that the following local normalization scheme aids generalization. Denoting by aix,y the activity of a neuron computed by applying kernel i at position (x,y) and then applying the ReLU nonlinearity, the response-normalized activity bix,y is given by the expression</p><p>ReLUs具有符合本文要求的一个性质：它不需要对输入进行归一化来防止饱和。只要一些训练样本产生一个正输入给一个ReLU，那么在那个神经元中学习就会开始。但是，我们还是发现如下的局部标准化方案有助于增加泛化性能。aix,y表示使用核i作用于(x,y)然后再采用ReLU非线性函数计算得到的活跃度，那么响应标准化活跃bix,y由以下公式计算出.</p><p><img src="/blog/blog/2022/04/29/alexnet-imagenet-classification-with-deep-convolutional-neural-networks/AlexNet-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks.assets/1571518-20220429205612275-1333567698.png"></p><p>where the sum runs over n “adjacent” kernel maps at the same spatial position, and N is the total number of kernels in the layer. The ordering of the kernel maps is of course arbitrary and determined before training begins. This sort of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activities amongst neuron outputs computed using different kernels. The constants k,n,α, and β are hyper-parameters whose values are determined using a validation set; we used k = 2, n = 5, α = 10−4, and β = 0.75. We applied this normalization after applying the ReLU nonlinearity in certain layers (see Section 3.5).</p><p>这里，对同一个空间位置的n个邻接核特征图（kernel maps）求和，N是该层的核的总数目。核特征图的顺序显然是任意的，并且在训练之前就已决定了的。这种响应归一化实现了侧抑制的一种形式，侧抑制受启发于一种在真实神经中发现的形式，对利用不同核计算得到的神经输出之间的大的活跃度生成竞争。常数k,n,α,β是超参数，它们的值使用一个验证集来确定。本文使用k=2,n=5,α=10−4,β=0.75。本文在某些特定的层中，采用ReLUs非线性函数后应用了该归一化（见3.5节）。</p><p>This scheme bears some resemblance to the local contrast normalization scheme of Jarrett et al.[11], but ours would be more correctly termed “brightness normalization”, since we do not subtract the mean activity. Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%, respectively. We also verified the effectiveness of this scheme on the CIFAR-10 dataset: a four-layer CNN achieved a 13% test error rate without normalization and 11% with normalization3.</p><p>这个方案与Jarrett等[11]的局部对比度归一化方案有些相似，但本文更加准确的称呼为“亮度归一化”(brightness normalization)，因为本文没有减去平均活跃度。响应归一化将top-1和top-5的错误率分别降低了1.4%和1.2%。本文也在CIFAR-10数据集上验证了这个方案的有效性：一个四层的CNN网络在未归一化的情况下错误率是13%，在归一化的情况下是11%。</p><h3 id="3-4-Overlapping-Pooling"><a href="#3-4-Overlapping-Pooling" class="headerlink" title="3.4 Overlapping Pooling"></a>3.4 Overlapping Pooling</h3><p>Pooling layers in CNNs summarize the outputs of neighboring groups of neurons in the same kernel map. Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g., [17, 11, 4]). To be more precise, a pooling layer can be thought of as consisting of a grid of pooling units spaced s pixels apart, each summarizing a neighborhood of size z×z centered at the location of the pooling unit. If we set s = z, we obtain traditional local pooling as commonly employed in CNNs. If we set s &lt; z, we obtain overlapping pooling. This is what we use throughout our network, with s = 2 and z = 3. This scheme reduces the top-1 and top-5 error rates by 0.4% and 0.3%, respectively, as compared with the non-overlapping scheme s = 2, z = 2, which produces output of equivalent dimensions. We generally observe during training that models with overlapping pooling find it slightly more difficult to overfit.</p><p>CNNs中的池化层归纳了同一个核特征图中的相邻神经元组的输出。通常，由邻接池化单元归纳的邻域并不重叠（例如，[17,11,4]）。更确切地说，一个池化层可以被看作是包含了每间隔S个像素的池化单元的栅格组成，每一个都归纳了以池化单元为中心大小为Z x Z的邻域。如果令S=Z，将会得到CNNs通常采用的局部池化。如果我们设置s &lt;z，我们获得重叠池化。 这是我们在整个网络中使用的，s = 2和z = 3.与非重叠方案s =2, z= 2相比，该方案分别将top-1和top-5错误率分别降低0.4％和0.3％，产生等效尺寸的输出。 我们在训练期间观察到具有重叠池化的模型通常过度拟合稍微困难一些。</p><h3 id="3-5-Overall-Architecture"><a href="#3-5-Overall-Architecture" class="headerlink" title="3.5 Overall Architecture"></a>3.5 Overall Architecture</h3><p>Now we are ready to describe the overall architecture of our CNN. As depicted in Figure 2, the net contains eight layers with weights; the first five are convolutional and the remaining three are fully connected. The output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels. Our network maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average across training cases of the log-probability of the correct label under the prediction distribution.</p><p>现在我们可以来描述本文CNN的整体结构。正如图2所示，这个网络包含八个有权值的层：前五层是卷积层，剩下的三层是全连接层。最后一个全连接层的输出传递给一个1000路的softmax层，这个softmax产生一个对1000类标签的分布。本文的网络最大化多项Logistic回归结果，也就是最大化训练集预测正确的标签的对数概率。</p><p>The kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel maps in the previous layer which reside on the same GPU (see Figure 2). The kernels of the third convolutional layer are connected to all kernel maps in the second layer. The neurons in the fully connected layers are connected to all neurons in the previous layer. Response-normalization layers follow the first and second convolutional layers. Max-pooling layers, of the kind described in Section 3.4, follow both response-normalization layers as well as the fifth convolutional layer. The ReLU non-linearity is applied to the output of every convolutional and fully-connected layer.</p><p>第二、四、五层卷积层的核只和同一个GPU上的前层的核特征图相连（见图2）。第三层卷积层和第二层所有的核特征图相连接。全连接层中的神经元和前一层中的所有神经元相连接。响应归一化层跟着第一和第二层卷积层。最大池化层，3.4节中有所描述，既跟着响应归一化层也跟着第五层卷积层。ReLU非线性变换应用于每一个卷积和全连接层的输出。</p><p>The first convolutional layer filters the 224×224×3  input image with 96 kernels of size 11×11×3 with a stride of 4 pixels (this is the distance between the receptive field centers of neighboring neurons in a kernel map). The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5×5×48. The third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers. The third convolutional layer has 384 kernels of size 3 × 3 × 256 connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth convolutional layer has 384 kernels of size 3×3×192 , and the fifth convolutional layer has 256 kernels of size 3×3×192. The fully-connected layers have 4096 neurons each.</p><p>第一层卷积层使用96个大小为11x11x3的卷积核对224x224x3的输入图像以4个像素为步长（这是核特征图中相邻神经元感受域中心之间的距离）进行滤波。第二层卷积层将第一层卷积层的输出（经过响应归一化和池化）作为输入，并使用256个大小为5x5x48的核对它进行滤波。第三层、第四层和第五层的卷积层在没有任何池化或者归一化层介于其中的情况下相互连接。第三层卷积层有384个大小为3x3x256的核与第二层卷积层的输出（已归一化和池化）相连。第四层卷积层有384个大小为3x3x192的核，第五层卷积层有256个大小为 的核。每个全连接层有4096个神经元。</p><h2 id="4-Reducing-Overfitting"><a href="#4-Reducing-Overfitting" class="headerlink" title="4 Reducing Overfitting"></a>4 Reducing Overfitting</h2><p>Our neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRC make each training example impose 10 bits of constraint on the mapping from image to label, this turns out to be insufficient to learn so many parameters without considerable overfitting. Below, we describe the two primary ways in which we combat overfitting.<br>本文的神经网络结构有6千万个参数。尽管ILSVRC的1000个类别使得每一个训练样本利用10bit的数据就可以将图像映射到标签上，但是如果没有大量的过拟合，是不足以学习这么多参数的。接下来，本文描述了两种对抗过拟合的主要的方法。</p><h3 id="4-1-Data-Augmentation"><a href="#4-1-Data-Augmentation" class="headerlink" title="4.1 Data Augmentation"></a>4.1 Data Augmentation</h3><p>The easiest and most common method to reduce overfitting on image data is to artificially enlarge the dataset using label-preserving transformations (e.g., [25, 4, 5]). We employ two distinct forms of data augmentation, both of which allow transformed images to be produced from the original images with very little computation, so the transformed images do not need to be stored on disk. In our implementation, the transformed images are generated in Python code on the CPU while the GPU is training on the previous batch of images. So these data augmentation schemes are, in effect, computationally free.<br>降低图像数据过拟合的最简单常见的方法就是利用标签转换人为地增大数据集（例如，[25,4,5]）。本文采取两种不同的数据增强方式，这两种方式只需要少量的计算就可以从原图中产生转换图像，因此转换图像不需要存入磁盘。本文中利用GPU训练先前一批图像的同时，使用CPU运行Python代码生成转换图像。因此这些数据增强方法实际上是不用消耗计算资源的。</p><p>The first form of data augmentation consists of generating image translations and horizontal reflections. We do this by extracting random 224×224 patches(and their horizontal reflections) from the 256×256 images and training our network on these extracted patches 4. This increases the size of our training set by a factor of 2048, though the resulting training examples are, of course, highly interdependent. Without this scheme, our network suffers from substantial overfitting, which would have forced us to use much smaller networks. At test time, the network makes a prediction by extracting five 224 × 224 patches (the four corner patches and the center patch) as well as their horizontal reflections (hence ten patches in all), and averaging the predictions made by the network’s softmax layer on the ten patches.</p><p>第一种数据增强的形式包括生成平移图像和水平翻转图像。做法就是从256x256的图像中提取随机的224x224大小的块（以及它们的水平翻转），然后基于这些提取的块训练网络。这个让我们的训练集增大了2048倍（(256-224)2*2=2048），尽管产生的这些训练样本显然是高度相互依赖的。如果不使用这个方法，本文的网络会有大量的过拟合，这将会迫使我们使用更小的网络。在测试时，网络通过提取5个224x224块（四个边角块和一个中心块）以及它们的水平翻转（因此共十个块）做预测，然后网络的softmax层对这十个块做出的预测取均值。</p><p>The second form of data augmentation consists of altering the intensities of the RGB channels in training images. Specifically, we perform PCA on the set of RGB pixel values throughout the ImageNet training set. To each training image, we add multiples of the found principal components, with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from a Gaussian with mean zero and standard deviation 0.1. Therefore to each RGB image pixel Ixy = [IRxy,IGxy,IBxy]T we add the following quantity:</p><p><img src="/blog/blog/2022/04/29/alexnet-imagenet-classification-with-deep-convolutional-neural-networks/AlexNet-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks.assets/1571518-20220429210008964-34214913.png"></p><p>where pi and λi are ith eigenvector and eigenvalue of the 3 × 3 covariance matrix of RGB pixel values, respectively, and αi is the aforementioned random variable. Each αi is drawn only once for all the pixels of a particular training image until that image is used for training again, at which point it is re-drawn. This scheme approximately captures an important property of natural images, namely, that object identity is invariant to changes in the intensity and color of the illumination. This scheme reduces the top-1 error rate by over 1%.</p><p>第二种数据增强的形式包括改变训练图像的RGB通道的强度。特别的，本文对整个ImageNet训练集的RGB像素值进行了PCA。对每一幅训练图像，本文加上多倍的主成分，倍数的值为相应的特征值乘以一个均值为0标准差为0.1的高斯函数产生的随机变量。因此对每一个RGB图像像素Ixy=[IRxy,IGxy,IBxy]T加上如下的量</p><p>[P1,P2,P3][α1λ1,α2λ2,α3λ3]T</p><p>这里Pi,λi分别是RGB像素值的3x3协方差矩阵的第i个特征向量和特征值，αi是上述的随机变量。每一个αi的值对一幅特定的训练图像的所有像素是不变的，直到这幅图像再次用于训练，此时才又赋予αi新的值。这个方案得到了自然图像的一个重要的性质，也就是，改变光照的颜色和强度，目标的特性是不变的。这个方案将top-1错误率降低了1%。</p><h3 id="4-2-Dropout"><a href="#4-2-Dropout" class="headerlink" title="4.2 Dropout"></a>4.2 Dropout</h3><p>Combining the predictions of many different models is a very successful way to reduce test errors [1, 3], but it appears to be too expensive for big neural networks that already take several days to train. There is, however, a very efficient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called “dropout” [10], consists of setting to zero the output of each hidden neuron with probability 0.5. The neurons which are “dropped out” in this way do not contribute to the forward pass and do not participate in backpropagation. So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights. This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks.</p><p>结合多种不同模型的预测结果是一种可以降低测试误差的非常成功的方法[1,3]，但是这对于已经要花很多天来训练的大规模神经网络来说显得太耗费时间了。但是，有一种非常有效的模型结合的方法，训练时间只需要原先的两倍。最新研究的技术，叫做“dropout”[10]，它将每一个隐藏神经元的输出以50%的概率设为0。这些以这种方式被“踢出”的神经元不会参加前向传递，也不会加入反向传播。因此每次有输入时，神经网络采样一个不同的结构，但是所有这些结构都共享权值。这个技术降低了神经元之间复杂的联合适应性，因为一个神经元不是依赖于另一个特定的神经元的存在的。因此迫使要学到在连接其他神经元的多个不同随机子集的时候更鲁棒性的特征。在测试时，本文使用所有的神经元，但对其输出都乘以了0.5，对采用多指数dropout网络生成的预测分布的几何平均数来说这是一个合理的近似。</p><p>We use dropout in the first two fully-connected layers of Figure2. Without dropout, our network exhibits substantial overfitting. Dropout roughly doubles the number of iterations required to converge.<br>本文在图2中的前两个全连接层使用dropout。如果不采用dropout，本文的网络将会出现大量的过拟合。Dropout大致地使达到收敛的迭代次数增加了一倍。</p><h2 id="5-Details-of-learning"><a href="#5-Details-of-learning" class="headerlink" title="5 Details of learning"></a>5 Details of learning</h2><p>We trained our models using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005. We found that this small amount of weight decay was important for the model to learn. In other words, weight decay here is not merely a regularizer: it reduces the model’s training error. The update rule for weight w was</p><p><img src="/blog/blog/2022/04/29/alexnet-imagenet-classification-with-deep-convolutional-neural-networks/AlexNet-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks.assets/1571518-20220429210052541-1918180964.png"></p><p>where i is the iteration index,  is the momentum variable,  is the learning rate, and  is the average over the ith batch Di of the derivative of the objective with respect to w, evaluated at wi.<br>本文使用随机梯度下降来训练模型，同时设置batch size大小为128，0.9倍动量以及0.0005的权值衰减。我们发现这个很小的权值衰减对模型的学习很重要。换句话说，这里的权值衰减不只是一个正则化矩阵：它降低了模型的训练错误率。权重ω更新规则是</p><p><img src="/blog/blog/2022/04/29/alexnet-imagenet-classification-with-deep-convolutional-neural-networks/AlexNet-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks.assets/1571518-20220429210109409-948521449.png"></p><p>这里i是迭代索引，v是动量变量，ε是学习速率，是在点ωi，目标对ω求得的导数的第i个batch:Di的均值。</p><p>We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. We initialized the neuron biases in the second, fourth, and ﬁfth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates the early stages of learning by providing the ReLUs with positive inputs. We initialized the neuron biases in the remaining layers with the constant 0.<br>本文对每一层的权值使用均值为0、标准差为0.01的高斯分布进行初始化。对第二层、第四层、第五层卷积层以及全连接的隐藏层使用常数1初始化神经元偏置项。这个初始化通过给ReLUs提供正输入加快了学习的初始阶段。本文对剩余的层使用常数0初始化神经元偏置项。</p><p>We used an equal learning rate for all layers, which we adjusted manually throughout training. The heuristic which we followed was to divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and reduced three times prior to termination. We trained the network for roughly 90 cycles through the training set of 1.2 million images, which took five to six days on two NVIDIA GTX580 3GB GPUs.<br>本文对所有层使用相同的学习速率，这个由在整个学习过程中手动地调整得到。我们采用启发式算法:当验证错误率停止降低就将当前学习速率除以10。本文的学习速率初始值设为0.01，在终止之前减小了三次。本文训练该网络对120万的图像训练集大约进行了90个周期，使用了两个NVIDIA GTX 580 3GB GPU，花费了5到6天的时间。</p><h2 id="6-Results"><a href="#6-Results" class="headerlink" title="6 Results"></a>6 Results</h2><p>Our results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0%5. The best performance achieved during the ILSVRC2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].<br>本文的在ILSVRC-2010上的结果见表1。本文网络的测试集top-1和top-5的错误率分别为37.5%和17.0%。在ILSVRC-2010比赛中最好的结果是47.1%和28.2%，采用的方法是对六个基于不同特征训练得到的稀疏编码模型的预测结果求平均数[2]，此后最好的结果是45.7%和25.7%，采用的方法是对基于从两种密集采样特征计算得到的Fisher向量（FVs），训练得到两个分类器，所得的预测结果求平均数[24]。</p><p><img src="/blog/blog/2022/04/29/alexnet-imagenet-classification-with-deep-convolutional-neural-networks/AlexNet-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks.assets/1571518-20220429210147793-472769871.png"></p><p>Table 1: Comparison of results on ILSVRC2010 test set. In italics are best results achieved by others.<br>表1 基于ILSVRC-2010测试集的结果对比。斜体字是其他方法获得的最好结果</p><p>We also entered our model in the ILSVRC-2012 competition and report our results in Table 2. Since the ILSVRC-2012 test set labels are not publicly available, we cannot report test error rates for all the models that we tried. In the remainder of this paragraph, we use validation and test error rates interchangeably because in our experience they do not differ by more than 0.1% (seeTable2). The CNN described in this paper achieves a top-5 error rate of 18.2%. Averaging the predictions of five similar CNNs gives an error rate of 16.4%. Training one CNN, with an extra sixth convolutional layer over the last pooling layer, to classify the entire ImageNet Fall 2011 release (15M images, 22K categories), and then “ﬁne-tuning” it on ILSVRC-2012 gives an error rate of 16.6%. Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 release with the aforementioned five CNNs gives an error rate of 15.3%. The second-best contest entry achieved an error rate of 26.2% with an approach that averages the predictions of several classifiers trained on FVs computed from different types of densely-sampled features [7].<br>我们也以我们的模型参加了ILSVRC-2012比赛，表2中是我们的结果。由于ILSVRC-2012测试集标签不是公开的，我们不能报告我们尝试的所有模型的测试错误率。在本段接下来的部分，我们交换着使用验证错误率和测试错误率因为根据我们的经验，他们不会有超过0.1%的不同（见表2）。本文中所描述的CNN的top-5错误率是18.2%。五个相似的CNN的平均预测结果的错误率是16.4%。在最后一个池化层上增加第六个卷积层，使用整个ImageNet Fall 2011的数据（15M图像，22000种类别）作为分类数据预训练得到的一个CNN，再经过微调，用ILSVRC-2012对该CNN进行测试得到的错误率为16.6%。对上述的五个在整个Fall 2011数据集上预训练过的CNN，得到的预测求平均得到的错误率结果为15.3%。当时第二的队伍得到的错误率为26.2%，使用的方法是对基于从多种密集采样特征计算得到的FVs，训练得到多个分类器的预测值求平均[7]。</p><p><img src="/blog/blog/2022/04/29/alexnet-imagenet-classification-with-deep-convolutional-neural-networks/AlexNet-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks.assets/1571518-20220429210214639-871863637.png"></p><p>Table 2: Comparison of error rates on ILSVRC-2012 validation and test sets. In italics are best results achieved by others. Models with an asterisk* were “pre-trained” to classify the entire ImageNet 2011 Fall release. See Section 6 for details.<br>表2 基于ILSVRC-2012的验证集和测试集的错误率比较。斜体字是其他方法取得的最好结果。带星号*的模型预先用整个ImageNet 2011 Fall数据集训练过。详见第6节。</p><p>Finally, we also report our error rates on the Fall 2009 version of ImageNet with 10,184 categories and 8.9 million images. On this dataset we follow the convention in the literature of using half of the images for training and half for testing. Since there is no established test set, our split necessarily differs from the splits used by previous authors, but this does not affect the results appreciably. Our top-1 and top-5 error rates on this dataset are 67.4% and 40.9%, attained by the net described above but with an additional, sixth convolutional layer over the last pooling layer. The best published results on this dataset are 78.1% and 60.9% [19].<br>最后，我们也汇报了我们的基于包含10184钟类别890万张图像的Fall 2009的错误率。对于该数据集我们遵从文献中的约定：一半为训练集一半为测试集。由于没有确定的测试集，我们的分割必然与前面作者使用的不一样，但是这并不会明显地影响结果。我们在该数据集上得到top-1和top-5错误率分别为67.4%和40.9%，这个结果是由在上述网络的最后一个池化层加了第六层卷积层所得到的。之前在这个数据集上最好的结果是78.1%和60.9%[19]。</p><h3 id="6-1-Qualitative-Evaluations"><a href="#6-1-Qualitative-Evaluations" class="headerlink" title="6.1 Qualitative Evaluations"></a>6.1 Qualitative Evaluations</h3><p>Figure 3 shows the convolutional kernels learned by the network’s two data-connected layers. The network has learned a variety of frequency-and orientation-selective kernels, as well as various colored blobs. Notice the specialization exhibited by the two GPUs, a result of the restricted connectivity described in Section 3.5. The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific. This kind of specialization occurs during every run and is independent of any particular random weight initialization (modulo a renumbering of the GPUs).<br>图3表示由网络的两个数据连接层学习到的卷积核。该网络已经学习到了各种各样的具有频率、方向选择性的核以及多种着色斑块。注意到两个GPU展现出的特殊化，这是3.5节描述的限制连接的结果。GPU1上的核大部分颜色不可知，而GPU2上的核大部分有颜色。这种特殊化每一次运行时都会发生，并且独立于任何特定随机权值初始化（模除GPU的重编号）。</p><p><img src="/blog/blog/2022/04/29/alexnet-imagenet-classification-with-deep-convolutional-neural-networks/AlexNet-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks.assets/1571518-20220429210307163-83883366.png"></p><p>Figure 3: 96 convolutional kernels of size 11×11×3 learned by the first convolutional layer on the 224×224×3 input images. The top48 kernels were learned on GPU1 while the bottom 48 kernels were learned on GPU 2. See Section 6.1 for details.</p><p>图3 第一层卷积层对224x224x3的输入图像使用96个大小为11x11x3的卷积核学习得到的特征图。上面的48个卷积核在GPU1上学习，下面的48个卷积核在GPU2上学习。详见6.1节<br>In the left panel of Figure 4 we qualitatively assess what the network has learned by computing its top-5 predictions on eight test images. Notice that even off-center objects, such as the mite in the top-left, can be recognized by the net. Most of the top-5 labels appear reasonable. For example, only other types of cat are considered plausible labels for the leopard. In some cases (grille, cherry) there is genuine ambiguity about the intended focus of the photograph.<br>图4的左边我们通过计算8幅测试图像的top-5预测结果定性地评估了网络学习到了什么。注意到即使偏离中心的目标，例如左上方的小虫，也可以被网络识别。大多数top-5标签都显得很合理。例如，只有别的种类的猫被似是而非贴上豹子的标签。在一些情况下（窗格、樱桃）会存在对照片的意图的判断的含糊不清。</p><p><img src="/blog/blog/2022/04/29/alexnet-imagenet-classification-with-deep-convolutional-neural-networks/AlexNet-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks.assets/1571518-20220429210334291-2036815356.png"></p><p>Figure 4: (Left) Eight ILSVRC-2010 test images and the ﬁve labels considered most probable by our model. The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar(if it happens to be in the top5). (Right) Five ILSVRC-2010 test images in the ﬁrst column. The remaining columns show the six training images that produce feature vectors in the last hidden layer with the smallest Euclidean distance from the feature vector for the test image.<br>图4 （左）8幅ILSVRC-2010测试图像和五个根据本文模型得到的最可能的标签。每一幅图像下方写着正确的标签，正确标签的可能性大小也用红色条表示了出来（如果其恰巧在前5个）。（右）第一列是五幅ILSVRC-2010测试图像。剩余列表示6幅训练图像，这些训练图像在最后一层隐藏层得到的特征向量与测试图像的特征向量有最小的欧式距离。<br>Another way to probe the network’s visual knowledge is to consider the feature activations induced by an image at the last, 4096-dimensional hidden layer. If two images produce feature activation vectors with a small Euclidean separation, we can say that the higher levels of the neural network consider them to be similar. Figure 4 shows five images from the test set and the six images from the training set that are most similar to each of them according to this measure. Notice that at the pixel level, the retrieved training images are generally not close in L2 to the query images in the first column. For example, the retrieved dogs and elephants appear in a variety of poses. We present the results for many more test images in the supplementary material.<br>另一种探讨网络的视觉知识的方法就是考虑最终图像在最后4096维隐藏层产生的特征激活度。如果两幅图像产生的特征激活向量的欧氏距离很小，我们就可以说神经网络的更高层认为它们是相似的（根据了特征激活向量的欧式距离，这种测度跟视觉感官上的相似度是不同的）。图4显示了根据这种测度下的五幅测试集图像和六幅跟他们最相似的训练集图像。注意到在像素水平，第二列中检索到的训练图像一般地不会和第一列的查询图像相近。例如，检索到的狗和大象以多种姿势出现。我们在补充材料中展示更多测试图像的结果。<br>Computing similarity by using Euclidean distance between two 4096-dimensional, real-valued vectors is inefficient, but it could be made efficient by training an auto-encoder to compress these vectors to short binary codes. This should produce a much better image retrieval method than applying auto encoders to the raw pixels [14], which does not make use of image labels and hence has a tendency to retrieve images with similar patterns of edges, whether or not they are semantically similar.<br>使用欧氏距离计算4096维、实值向量之间的相似度效率较低，但是可以通过训练一个自动编码器来将这些向量压缩为短的二进制编码而提高效率。 这个相比将自动编码器直接应用到原始像素上，是一个更加好的图像检索方法[14]，前者没有利用图像的标签，因此会倾向于检索到有相似边界模式的图像，而不论他们语义上是否相似。</p><h2 id="7-Discussion"><a href="#7-Discussion" class="headerlink" title="7 Discussion"></a>7 Discussion</h2><p>Our results show that a large, deep convolutional neural network is capable of achieving record breaking results on a highly challenging dataset using purely supervised learning. It is notable that our network’s performance degrades if a single convolutional layer is removed. For example, removing any of the middle layers results in a loss of about 2% for the top-1 performance of the network. So the depth really is important for achieving our results.<br>本文的结果表明一个大规模深度卷积神经网络在具有高度挑战性的数据集上仅用监督学习就能够获得破纪录的好结果。值得注意的是如果一个卷积层被移除则本文的网络性能会降低。例如，移除任一个中间层，网络的top-1性能会降低大约2%。因此深度对本文的结果真的很重要。</p><p>To simplify our experiments, we did not use any unsupervised pre-training even though we expect that it will help, especially if we obtain enough computational power to significantly increase the size of the network without obtaining a corresponding increase in the amount of labeled data. Thus far, our results have improved as we have made our network larger and trained it longer but we still have many orders of magnitude to go in order to match the infero-temporal pathway of the human visual system. Ultimately we would like to use very large and deep convolutional nets on video sequences where the temporal structure provides very helpful information that is missing or far less obvious in static images.<br>为了简化本文的实验，我们没有使用任何非监督预训练即使我们认为它会起作用，尤其是我们可以在标签数据没有发生相应增长的情况下，获得足够的计算资源来增大我们网络的大小，能够有足够的计算能力去显著地增加网络的大小。迄今，由于我们使用了更大的网络，训练了更长的时间，本文的结果已经有所提高，但我们仍然有很多需求来进行时空下人类视觉系统的研究。最终我们想要将非常大规模地深度卷积网络应用于视频序列的处理，视频序列中的时间结构提供了许多有用的信息，而这些信息在静态图中丢失了或者不是很明显。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] R.M.BellandY.Koren.Lessons from the net ﬂixprize challenge. ACM SIG KDD Explorations News letter, 9(2):75–79, 2007.<br>[2] A. Berg, J. Deng, and L. Fei-Fei. Large scale visual recognition challenge 2010. <a href="http://www.imagenet.org/challenges">www.imagenet.org/challenges</a>. 2010.<br>[3] L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.<br>[4] D. Cires¸an, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation. Arxiv preprint arXiv:1202.2745, 2012.<br>[5] D.C. Cires¸an, U. Meier, J. Masci, L.M. Gambardella, and J. Schmidhuber. High-performance neural networks for visual object classiﬁcation. Arxiv preprint arXiv:1102.0183, 2011.<br>[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.<br>[7] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-Fei. ILSVRC-2012, 2012. URL <a href="http://www.image-net.org/challenges/LSVRC/2012/">http://www.image-net.org/challenges/LSVRC/2012/</a>.<br>[8] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesi an approach tested on 101 object categories. Computer Vision and Image Understanding, 106(1):59–70, 2007.<br>[9] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical Report 7694, California Institute of Technology, 2007. URL <a href="http://authors.library.caltech.edu/7694">http://authors.library.caltech.edu/7694</a>.<br>[10] G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R.R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.<br>[11] K. Jarrett, K. Kavukcuoglu, M. A. Ranzato, and Y. LeCun. What is the best multi-stage architecture for object recognition? In International Conference on Computer Vision, pages 2146–2153. IEEE, 2009.<br>[12] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Department of Computer Science, University of Toronto, 2009.<br>[13] A. Krizhevsky. Convolutional deep belief networks on cifar-10. Unpublished manuscript, 2010.<br>[14] A. Krizhevsky and G.E. Hinton. Using very deep autoencoders for content-based image retrieval. In ESANN, 2011.<br>[15] Y. Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, et al. Hand written digit recognition with a back-propagation network. In Advances in neural information processing systems, 1990.<br>[16] Y. LeCun, F.J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, volume 2, pages II–97. IEEE, 2004.<br>[17] Y. LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications in vision. In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on, pages 253–256. IEEE, 2010.<br>[18] H. Lee, R. Grosse, R. Ranganath, and A.Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hier archical representations. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 609–616. ACM, 2009.<br>[19] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric Learning for Large Scale Image Classiﬁcation: Generalizing to New Classes at Near-Zero Cost. In ECCV - European Conference on Computer Vision, Florence, Italy, October 2012.<br>[20] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In Proc. 27th International Conference on Machine Learning, 2010.<br>[21] N. Pinto, D.D. Cox, and J.J. DiCarlo. Why is real-world visual object recognition hard? PLoS computational biology, 4(1):e27, 2008.<br>[22] N. Pinto, D. Doukhan, J.J. DiCarlo, and D.D. Cox. A high-throughput screening approach to discovering good forms of biologically inspired visual representation. PLoS computational biology, 5(11):e1000579, 2009.<br>[23] B.C. Russell, A. Torralba, K.P. Murphy, and W.T. Freeman. Labelme: a database and web-based tool for image annotation. International journal of computer vision, 77(1):157–173, 2008.<br>[24] J.SánchezandF.Perronnin. High-dimensionalsignaturecompressionforlarge-scaleimageclassiﬁcation. InComputerVisionandPatternRecognition(CVPR),2011IEEEConferenceon,pages1665–1672.IEEE, 2011.<br>[25] P.Y. Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to visualdocumentanalysis. InProceedingsoftheSeventhInternationalConferenceonDocumentAnalysis and Recognition, volume 2, pages 958–962, 2003.<br>[26] S.C.Turaga,J.F.Murray,V.Jain,F.Roth,M.Helmstaedter,K.Briggman,W.Denk,andH.S.Seung. Convolutional networks can learn to generate afﬁnity graphs for image segmentation. Neural Computation, 22(2):511–538, 2010.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper_Read </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Model_PlayGround</title>
      <link href="/blog/2022/03/05/model-playground/"/>
      <url>/blog/2022/03/05/model-playground/</url>
      
        <content type="html"><![CDATA[<h1 id="可视化经典模型的对比实验总结"><a href="#可视化经典模型的对比实验总结" class="headerlink" title="可视化经典模型的对比实验总结"></a>可视化经典模型的对比实验总结</h1><hr><h2 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h2><h3 id="安装OpenGL"><a href="#安装OpenGL" class="headerlink" title="安装OpenGL"></a>安装OpenGL</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">sudo apt-get install -y build-essential  libxmu-dev libgl1-mesa-glx libglu1-mesa-dev libgl1-mesa-dev freeglut3-dev libglew-dev libsdl2-dev libsdl2-image-dev libglm-dev libfreetype6-dev<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="安装Netron"><a href="#安装Netron" class="headerlink" title="安装Netron"></a>安装Netron</h3><p>网页版：<a href="https://netron.app/">https://netron.app/</a><br>官方下载：<a href="https://github.com/lutzroeder/netron">https://github.com/lutzroeder/netron</a></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220305162352499-1591018073.png"></p><h3 id="安装zetane"><a href="#安装zetane" class="headerlink" title="安装zetane"></a>安装zetane</h3><p>官方链接“”<a href="https://zetane.com/">https://zetane.com/</a><br>GitHub链接：<a href="https://github.com/zetane/viewer">https://github.com/zetane/viewer</a></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220305162248318-2047619956.png"></p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>Alexnet 是一个图像分类模型，其中输入是 1000 个不同类别（例如猫、狗等）之一的图像，输出是 1000 个数字的向量。<br>输出向量的第i个元素是输入图像属于第i类的概率；因此，输出向量的所有元素之和为 1。<br>AlexNet 的输入是大小为 224x224 像素的 RGB 图像。</p><p>模型设计图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225145203604-1693667163.png"></p><p>Netron结构图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225140315239-1252251100.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225141625919-478564144.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225141939074-1964146127.png"></p><p>weights</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225142000979-1208083108.png"></p><p>bias</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225142035006-104454797.png"></p><p>input (1,3,224,224)</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225140516780-477155284.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225141720885-482648097.png"></p><p>Conv</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225141904046-443173747.png"></p><p>Feature Maps<br><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225142131191-1941806955.png"></p><p>ReLU</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225142609149-973728261.png"></p><p>MaxPool</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225142756207-1018614640.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225142834485-331509093.png"></p><p>整体结构图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225143409901-468288485.png"></p><p>feature0</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225143636108-341920803.png"></p><p>feature3</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225143740378-1592067968.png"></p><p>feature6</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225143856590-1693357601.png"></p><p>feature8</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225143934716-1747820310.png"></p><p>feature10</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225144020506-1695211690.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225144059763-138027249.png"></p><p>classify</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225144237338-1898633771.png"></p><h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><p>模型设计图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225145046001-1458327826.png"></p><p>Netron结构图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225150537049-891493968.png"></p><p>整体结构图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225152559322-1149730495.png"></p><p>feature0</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225152750409-187000821.png"></p><p>feature2</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225152941281-1204442755.png"></p><p>feature5</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225153132935-1871481424.png"></p><p>feature7</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225153259364-1250930943.png"></p><p>feature10</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225153348113-302309405.png"></p><p>feature12</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225153448549-640621181.png"></p><p>feature14</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225153539550-1806783753.png"></p><p>feature17</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225153629625-1468152986.png"></p><p>feature19</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225153720713-877509722.png"></p><p>feature21</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225160526229-1428599420.png"></p><p>feature24</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225160748772-21524161.png"></p><p>feature26</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225160836318-841285763.png"></p><p>feature28</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225162311365-98380387.png"></p><p>classifier</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225162355145-1901323229.png"></p><h2 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h2><p>模型设计图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225163302824-1362493528.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225163312657-1912740755.png"></p><p>Netron网络图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225163511480-1300588968.png"></p><p>zetane整体结构图<br><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225163816353-215415042.png"></p><p>详细可视化<br>1</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225164027522-1510122691.png"></p><p>2</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225164245123-1068736293.png"></p><p>3<br><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225164437341-1049144409.png"></p><p>分支</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225165506711-848751439.png"></p><p>m_1</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225164621508-1358903517.png"></p><p>m_2</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225164708591-1897267092.png"></p><p>m_3</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225164803107-1594251840.png"></p><p>m_4</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225164847846-254496265.png"></p><p>for_epoch 9次<br><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225165802140-1803925215.png"></p><p>output_merge</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225165057783-43982196.png"></p><p>end</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225165201855-291482507.png"></p><h2 id="Inception-v3"><a href="#Inception-v3" class="headerlink" title="Inception_v3"></a>Inception_v3</h2><p>也称为 GoogleNetv3，2015 年开始在 Imagenet 上训练的著名 ConvNet。</p><p>模型设计图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225170441225-1444254505.png"></p><p>Netron结构图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225171828164-1619770749.png"></p><p>zetane整体结构图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220225172157106-916922756.png"></p><p>conv</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228105357984-1173663008.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228105509847-564145604.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228105632743-1014896237.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228105715764-994351996.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228105824822-141628719.png"></p><p>分支</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228105917475-1909633004.png"></p><p>m_1</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228110017466-411973851.png"></p><p>m_2</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228110059059-648280204.png"></p><p>m_3</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228110546231-2034790858.png"></p><p>m_4</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228110609834-1005388752.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228110655723-930615372.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228110724170-1612694160.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228110800529-589118568.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228110820825-1761940179.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228110855861-531054803.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228110935025-418365837.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228111043287-708269569.png"></p><p>全局图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228114216656-219182460.png"></p><h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><p>密集卷积网络（DenseNet）以前馈方式将每一层连接到其他每一层。</p><p>网络设计图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228115120387-933369143.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228115127046-647785944.png"></p><p>Netron网络图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228153703321-1269014383.png"></p><p>这个部分可以看做是一个基础组件的结构，后面大量嵌套并循环使用<br><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228165814936-588490293.png"></p><p>局部图<br><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228165820358-1673461436.png"></p><p>第一部分</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228170026670-92347754.png"></p><p>第二部分</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228170144694-1084057811.png"></p><p>第三部分-头</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228170500118-2068008562.png"></p><p>第三部分-尾</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228170505684-1699916486.png"></p><p>第四部分 </p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228170553452-1448792945.png"></p><p>zetane网络图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228174721633-1978608516.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228174820241-19661652.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228174829836-1146331656.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228174835976-1789254247.png"></p><h2 id="SqueezeNet"><a href="#SqueezeNet" class="headerlink" title="SqueezeNet"></a>SqueezeNet</h2><p>参数减少 50 倍的 Alexnet 级精度。</p><p>网络设计图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228203721301-183649768.png"></p><p>Netron网络图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228203814545-1020463711.png"></p><p>Zetene整体图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228204053197-2025424060.png"></p><p>细节</p><p>features.0 </p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228204603726-1521186485.png"></p><p>features.3.squeeze</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228205445501-2130145544.png"></p><p>features.3.expand3x3</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301090947542-723517316.png"></p><p>features.4</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301091037116-1887713548.png"></p><p>features.4.expand1x1</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301091458576-108253404.png"></p><p>features.5.squeeze</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301091653726-1473094623.png"></p><p>features.5.expand3x3</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301091808363-262417800.png"></p><p>features.7.squeeze</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301091910881-216017219.png"></p><p>features.7.expand </p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301092610963-360997827.png"></p><p>features.8.squeeze</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301092706685-449851702.png"></p><p>features.8.expand </p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301092753515-269603352.png"></p><p>features.9.squeeze</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301092853535-1453649827.png"></p><p>features.10.expand</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301092955198-271839240.png"></p><p>features.10.squeeze</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301093047107-867498964.png"></p><p>features.10.expand1x1</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301093208817-1347431653.png"></p><p>features.12.squeeze</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301093321486-1887295047.png"></p><p>features.12.expand.3x3</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301093648575-957121320.png"></p><p>classifer.1</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301094639842-1748417057.png"></p><p>全局图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301094840108-387938584.png"></p><p>output</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301095022112-1958225398.png"></p><h2 id="ShuffleNet"><a href="#ShuffleNet" class="headerlink" title="ShuffleNet"></a>ShuffleNet</h2><p>一个针对速度和内存进行了优化的高效 ConvNet，在 Imagenet 上进行了预训练。</p><p>所有预训练模型都期望输入图像以相同的方式归一化，即形状为 的 3 通道 RGB 图像的小批量<code>(3 x H x W)</code>，其中<code>H</code>和<code>W</code>预计至少为<code>224</code>。必须将图像加载到 的范围内，然后使用 和<code>[0, 1]</code>进行归一化。`mean = [0.485, 0.456, 0.406]``std = [0.229, 0.224, 0.225]。</p><p>网络设计图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301154747781-434373750.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301154753666-1022040968.png"></p><p>Netron网络图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301160822766-236083582.png"></p><p>Zetane整体图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301160951118-740286363.png"></p><p>p1</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301161228919-1274638524.png"></p><p>这里同样可以将几个重复的结构作为单独模块来理解分析，大致可分为两个组件和一个连接件。</p><p>组件一：包含两个输出层，输出层一包含三个Conv卷积和两个ReLu，输出层二包含两个Conv卷积和一个ReLu。</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301161740668-1364149764.png"></p><p>组件二：包含两个输出层，输出层一无其他操作直接输出，输出层二包含三个Conv卷积和两个ReLu。</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301161852527-1338204606.png"></p><p>连接件：这部分主要是用来连接组件模块设计的，包含Constant、Concat、Transpose、Reshape、Split。</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301162001255-274904922.png"></p><p>先来预览下组件模块的效果</p><p>组件一</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301162211491-950958721.png"></p><p>组件二</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301162249016-216836367.png"></p><p>连接件：</p><p>concat</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301163617583-361559307.png"></p><p>input_1 (24,28,28)</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301163322827-713482907.png"></p><p>input_2 (24,28,28)</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301163402051-1585287036.png"></p><p>output (48,28,28)</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301163449137-30117470.png"></p><p>Constant</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301163738944-698473299.png"></p><p>Reshape</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301163833650-1901190548.png"></p><p>Transpose</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301164658138-1571132414.png"></p><p>Split</p><p>input (48,28,28)</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301164919027-1068410671.png"></p><p>output_1 (24,28,28)</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301165002776-347972027.png"></p><p>output_2 (24,28,28)</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301165119464-538866478.png"></p><p>模型组网</p><p>组网方式一</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301165504155-1361769752.png"></p><p>组网方式二</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301165526279-524827084.png"></p><p>end</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301165346065-1267305526.png"></p><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>在 ImageNet 上预训练的深度残差网络</p><p>网络设计图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228180734045-2069384407.png"></p><p>Netron网络图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228180902400-860347899.png"></p><p>Zetane整体图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228181119496-1007069512.png"></p><p>局部细节</p><p>p1</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228181337402-1725386448.png"></p><p>p2</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228181421463-717846787.png"></p><p>p3</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228182015625-1377034284.png"></p><p>p4</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228182059442-801591851.png"></p><p>p5</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228182154960-1947470775.png"></p><p>p6</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228182235419-927467848.png"></p><p>p7</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228182319768-706186423.png"></p><p>p8</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228182403517-657090797.png"></p><p>p9</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228182516426-1153405849.png"></p><p>p10</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228182715376-396622561.png"></p><p>整体可视化</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228182931000-1708426110.png"></p><p>output </p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220228181237953-201412330.png"></p><h2 id="ResNext"><a href="#ResNext" class="headerlink" title="ResNext"></a>ResNext</h2><p>下一代 ResNet，更高效、更准确</p><p>网络设计图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301095542036-1728996924.png"></p><p>Netron结构图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301095746489-1691647850.png"></p><p>Zetane整体图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301095849857-179924785.png"></p><p>局部细节图</p><p>下面是一个基本的结构，共包括两个输出层，其中一个输出层包含三个Conv卷积和两个ReLu，另一个输出层只包含一个Conv卷积。<br>最后合并两个输出通道的结果，输入下一层进行计算。</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301153844378-1978488961.png"></p><p>简单来看下效果</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301153932368-1388850870.png"></p><p>接下来是对上面基本结构的一个交叉循环，共计16个。</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301154203692-1453068831.png"></p><p>最终输出</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301154259564-1791948757.png"></p><p>output结果</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301100302416-343142941.png"></p><h2 id="Wide-ResNet"><a href="#Wide-ResNet" class="headerlink" title="Wide_ResNet"></a>Wide_ResNet</h2><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301170110398-1298360538.png"></p><p>Netron网络图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301181316458-2035910976.png"></p><p>Zetane整体结构</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220301202937407-1847245913.png"></p><p>p1</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220302085641382-65047282.png"></p><p>两个基本结构</p><p>结构一</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220302085305787-31303168.png"></p><p>结构二</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220302085410404-757800926.png"></p><p>下面分别来看下两个结构的可视化</p><p>结构一，分为两个输出层，一层仅包含一个Conv卷积，另外一层是包含三个Conv卷积和两个ReLu。</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220302103556123-880678666.png"></p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220302103647756-126045488.png"></p><p>结构二，分为两个输出层，其中一层不包含操作，另外一层与结构一的第二层一样，包含三个Conv卷积和两个ReLu。</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220302103922631-958348776.png"></p><p>中间连接部分，主要是承上启下，将两个输出通道的输出结果进行叠加融合，重新进行ReLu传递给下面。</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220302104101642-1146895737.png"></p><h2 id="RegNet"><a href="#RegNet" class="headerlink" title="RegNet"></a>RegNet</h2><p>Netron网络图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220302152004006-386926191.png"></p><p>Zetane整体图</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220302152941039-787593432.png"></p><p>两个基本结构</p><p>结构一</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220302182900281-2093413226.png"></p><p>结构二</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220302182934830-797769578.png"></p><p>结构一</p><p>局部细节一</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220302195917740-2116583784.png"></p><p>局部细节二</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220303091849311-1418801263.png"></p><p>结构二</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220303094321949-81552848.png"></p><p>整体效果</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220303174323452-612469439.png"></p><p>局部细节</p><p>细节一</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220303183044820-999853294.png"></p><p>细节二</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220303183116488-1746000920.png"></p><p>连接部分</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220303094156638-756170570.png"></p><p>结尾部分</p><p><img src="/blog/blog/2022/03/05/model-playground/Model-PlayGround/1571518-20220304203727276-1961049293.png"></p>]]></content>
      
      
      <categories>
          
          <category> 算法实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SwinTranformer_Det</title>
      <link href="/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/"/>
      <url>/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="基于SwinTransformer的目标检测训练模型学习总结"><a href="#基于SwinTransformer的目标检测训练模型学习总结" class="headerlink" title="基于SwinTransformer的目标检测训练模型学习总结"></a>基于SwinTransformer的目标检测训练模型学习总结</h1><hr><h2 id="一、简要介绍"><a href="#一、简要介绍" class="headerlink" title="一、简要介绍"></a>一、简要介绍</h2><p>Swin Transformer是2021年提出的，是一种基于Transformer的一种深度学习网络结构，在目标检测、实例分割等计算机视觉任务上均取得了SOTA的性能。同时这篇论文也获得了ICCV2021年的Best Paper。</p><h3 id="1-1-Transformer的关键里程碑"><a href="#1-1-Transformer的关键里程碑" class="headerlink" title="1.1 Transformer的关键里程碑"></a>1.1 Transformer的关键里程碑</h3><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210161725464-1470625546.png"></p><ul><li><strong>Tranformer</strong>: 在2017年6月，仅基于注意力机制的Transformer首次由谷歌提出，应用于NLP自然语言处理的任务上表现出了良好的性能。</li><li><strong>BERT</strong>: 在2018年10月，预训练Transformer模型开始在NLP领域中占主导地位。</li><li><strong>GPT-3</strong>: 在2020年5月，提出一个带有170亿参数的大型Transformer，向通用NLP模型迈出了一大步。</li><li><strong>DETR</strong>:  在2020年5月，提出一个简单而有效的高层视觉框架，将目标检测视为一个直接集预测问题。</li><li><strong>iGPT</strong>: 在2020年7月，针对NLP的模型也可以用于图像预训练。</li><li><strong>VIT</strong>: 在2020年10月，纯Transformer架构适用于可视化识别</li><li><strong>IPT</strong>: 在2020年12月，第一个结合多任务的低层视觉变换Transformer模型</li><li><strong>VIT Variants</strong>： 在2021年，提出基于VIT模型的几种变体，例如DeiT、PVT、TNT、Swin。</li></ul><hr><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210161812539-389872081.png"></p><p>Swin Transformer是由微软亚洲研究院提出的一种Transformer结构模型，它取得了多个计算机视觉任务和数据集上的SOTA性能，这些任务包括目标检测、实例分割和语义分割，图像分类。它可以用作计算机视觉的通用backbone。在NLP和CV两个领域之间的差异，例如视觉实体尺度的巨大差异以及与文字中的单词相比，图像中像素的高分辨率，带来了使Transformer从语言适应视觉方面的挑战。</p><h3 id="1-2-Transformer的网络结构"><a href="#1-2-Transformer的网络结构" class="headerlink" title="1.2 Transformer的网络结构"></a>1.2 Transformer的网络结构</h3><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210162050922-1231652849.png"></p><p>和大多数seq2seq模型一样，transformer的结构也是由encoder和decoder组成。<br>实际上，Transformer是基于Encoder和Decoder的网络架构。最初是被用来解决NLP自然语言处理来提出的，主要是用来的处理seq2seq的任务，例如语言的翻译。它的主要特点是采用了Multi-Head Attention的注意力机制以及Encoder和Decoder之间的Cross相互的注意力机制。</p><h3 id="1-3-Swin-Transformer的网络架构"><a href="#1-3-Swin-Transformer的网络架构" class="headerlink" title="1.3 Swin Transformer的网络架构"></a>1.3 Swin Transformer的网络架构</h3><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210162121601-411564667.png"></p><p>Swin Transformer的优势在于在ViT的基础上将<strong>hierarchy</strong>(层次性)、<strong>locality</strong>(局部性)、<strong>translation invariance</strong>(平移不变性)等先验引入Transformer网络结构设计从而能在视觉任务中取得更好的性能。</p><p>下面分别从几个特性来介绍：</p><ul><li><p><strong>层次性</strong>：从上图可以看到，Image经过Patch Partition传入，经过多个stage，其中每个stage中分别对其进行Patch Mergeing和Swin Transformer Block的处理，在此过程中Size的大小不断减少，同时通道数不断地增加，这就是该结构体现的层次性。</p></li><li><p><strong>局部性</strong>：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</p></li><li><p><strong>平移不变性</strong>：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。卷积神经网络具有平移不变性，而图网络不具有该性质，因此不能直接做卷积操作。平移不变性是一个很有用的性质，尤其是当我们关心某个特征<strong>是否出现</strong>而不关心它出现的具体位置时。简单地说，卷积+最大池化约等于平移不变性。</p></li></ul><h3 id="1-4-核心创新点"><a href="#1-4-核心创新点" class="headerlink" title="1.4 核心创新点"></a>1.4 核心创新点</h3><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210162232434-376099605.png"></p><p>每个Swin Transformer模块都由两个Block组成。</p><p>(1)自注意力的计算在局部的非重叠窗口内进行。<br>(2)在前后两层的Transformer模块中，非重叠窗口的配置相比前一层做了半个窗口的移位，使得上一层中不同窗户的信息进行了交换。</p><hr><h2 id="二、目标检测相关基础"><a href="#二、目标检测相关基础" class="headerlink" title="二、目标检测相关基础"></a>二、目标检测相关基础</h2><h3 id="2-1-目标检测任务理解与总结"><a href="#2-1-目标检测任务理解与总结" class="headerlink" title="2.1 目标检测任务理解与总结"></a>2.1 目标检测任务理解与总结</h3><p><a href="https://www.cnblogs.com/isLinXu/p/15893539.html">目标检测任务理解与总结</a></p><h3 id="2-2-目标检测之常用数据集"><a href="#2-2-目标检测之常用数据集" class="headerlink" title="2.2 目标检测之常用数据集"></a>2.2 目标检测之常用数据集</h3><p><a href="https://www.cnblogs.com/isLinXu/p/15893506.html">目标检测之常用数据集</a></p><h3 id="2-3-目标检测之性能指标"><a href="#2-3-目标检测之性能指标" class="headerlink" title="2.3 目标检测之性能指标"></a>2.3 目标检测之性能指标</h3><p><a href="https://www.cnblogs.com/isLinXu/p/15893489.html">目标检测之性能指标</a></p><hr><h2 id="三、Transformer的介绍及在CV领域内的应用"><a href="#三、Transformer的介绍及在CV领域内的应用" class="headerlink" title="三、Transformer的介绍及在CV领域内的应用"></a>三、Transformer的介绍及在CV领域内的应用</h2><p>解码器与编码器</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210162844379-199629236.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210162921607-176483924.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210163022263-1886794222.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210163108987-1746119168.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210163505453-375872420.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210165319524-1993358431.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210165339611-2051991970.png"></p><p>Positional Encoding</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210165356427-1800991937.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210171934227-768569020.png"></p><p>Decoder<br><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210182837840-1132583376.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210182902982-1229229011.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210182914285-289123855.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210182931664-1888912096.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210191719401-127928081.png"></p><p>DETR<br>End to End Object Detection with Transformers<br><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210191812683-1919983590.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210191940817-1931228032.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220210192009448-314402354.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211101815077-1760211366.jpg"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211101853987-1261890538.png"></p><h2 id="四、Swin-Transformer的原理"><a href="#四、Swin-Transformer的原理" class="headerlink" title="四、Swin Transformer的原理"></a>四、Swin Transformer的原理</h2><p>论文</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211102404861-1638762083.png"><br>Ze Liu Yutong Lin Yue Cao Han Hu Yixuan Wei Zheng Zhang Stephen Lin Baining Guo<br>Swin Transformer : Hierarchical Vision Transformer using Shifted Windows. ICCV 2021<br><a href="https://arxiv.org/abs/2103.14030">https://arxiv.org/abs/2103.14030</a></p><p>SOTA<br><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211102434243-1041859823.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211102530289-1102382243.png"></p><p>Problems of ViT<br>. Does not consider the difference between textual and visual signals<br>. Mainly for image classification</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211102909420-1770397249.png"></p><p>Key tech innovation : locality by Shifted windows<br>• Non-overlapped windows (faster real speed than sliding windows)<br>• Windows are shifted in the next layer</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211103106841-2069297650.png"></p><p>Hierarchical feature maps<br>• Windows Multi-Head Self-Attention (W-MSA)<br>• Shifted Windows Multi-Head Self-Attention (SW-MSA)</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211103153207-1928167935.png"></p><p>1）自注意的计算在局部的非重叠窗口内进行。不同query会共享同样的key集合，从而对硬件友好<br>2）在前后两层的Transformer模块中，非重叠窗口的配置相比前一层做了半个窗口的移位，使得上一层中不<br>同窗口的信息进行了交换。</p><p>Self-attention in non-overlapped windows<br>For efficient modeling, we propose to compute self attention within local windows.<br>The windows are arranged to evenly partition the image in a non-overlapping manner.<br>Supposing each window contains M × M patches, the computational complexity of a global<br>MSA module and a window based one on an image of h × w patches are :</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211103231770-1344195193.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211103647672-1998938594.png"></p><p>The architecture of a Swin Transformer (Swin-T)<br><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211103723480-917256578.png"></p><p>Patch Merging<br>经过Patch Merging后，feature map的高和宽会减半，深度会加倍<br><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211103820414-1456521674.png"></p><p>Swin Transformer block<br><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211103857370-849387511.png"></p><p>Swin Transformer is built by replacing the standard multi-head<br>self attention (MSA) module in a Transformer block by a module<br>based on shifted windows, with other layers kept the same.<br>A Swin Transformer block consists of a shifted window based MSA<br>module, followed by a 2-layer MLP with GELU nonlinearity in<br>between.<br>A LayerNorm (LN) layer is applied before each MSA module and<br>each MLP, and a residual connection is applied after each module.</p><p>Shifted window partitioning in successive blocks<br><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211103916799-1005212465.png"></p><p>Swin T(Tiny)，S(Small)，B(Base)，L(Large)<br><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211103956361-1294854273.png"><br>• win. sz. 7x7表示使用的窗口（Windows）的大小<br>• dim表示feature map的channel深度）<br>• head表示多头注意力模块中head的个数</p><p>Architecture Variants</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211104127694-1279887825.png"></p><p>where C is the channel number of the hidden layers in the first stage.<br>Architecture Variants<br>The window size is set to M = 7 by default. The query dimension of each<br>head is d = 32, and the expansion layer of each MLP is α = 4.</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211104146423-1069831464.png"></p><p>Illustration of an efficient batch computation approach for self-attention in shifted window partitioning</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211104213844-333037308.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211104234515-1167429771.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211104443788-310599498.png"></p><p>Experimental Designs<br>3 datasets to cover various recognition tasks of different granularities<br>• lmage-level lmageNet-1K classification (1.28M images; 1000 classes).<br>• Region-level coco object detection (115K images; 80 classes).<br>• Pixel-level ADE20K semantic segmentation (20K images; 150 classes)</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211104601939-965284686.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211105009208-1472070156.png"></p><p>Results on COCO object detection and instance segmentation<br><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211105045787-1471956532.png"></p><p>Results of semantic segmentation on the ADE20K val and test set<br><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211105116539-862749492.png"></p><h2 id="五、Swin-Transformer的环境部署和训练相关"><a href="#五、Swin-Transformer的环境部署和训练相关" class="headerlink" title="五、Swin Transformer的环境部署和训练相关"></a>五、Swin Transformer的环境部署和训练相关</h2><h3 id="5-1-软件安装"><a href="#5-1-软件安装" class="headerlink" title="5.1 软件安装"></a>5.1 软件安装</h3><p>1） 安装Anaconda<br>Anaconda 是一个用于科学计算的 Python 发行版，支持 Linux, Mac, Windows, 包含了众多流行的科学<br>计算、数据分析的 Python 包。</p><ol><li><p>先去官方地址下载好对应的安装包<br>下载地址:<a href="https://www.anaconda.com/download/#linux">https://www.anaconda.com/download/#linux</a></p></li><li><p>然后安装anaconda</p></li></ol>  <pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">bash ~&#x2F;Downloads&#x2F;Anaconda3-2021.05-Linux-x86_64.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>  anaconda会自动将环境变量添加到PATH里面，如果后面你发现输入conda提示没有该命令，那么<br>  你需要执行命令 source <del>/.bashrc 更新环境变量，就可以正常使用了。<br>  如果发现这样还是没用，那么需要添加环境变量。<br>  编辑</del>/.bashrc 文件，在最后面加上  </p>  <pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">export PATH&#x3D;&#x2F;home&#x2F;bai&#x2F;anaconda3&#x2F;bin:$PATH<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>  注意：路径应改为自己机器上的路径<br>  保存退出后执行： source ~/.bashrc<br>  再次输入 conda list 测试看看，应该没有问题。</p><p>添加Aanaconda国内镜像配置<br>清华TUNA提供了 Anaconda 仓库的镜像，运行以下三个命令:</p>  <pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F;conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main&#x2F;conda config --set show_channel_urls yes<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>  2） 安装pytorch<br>  首先创建一个anaconda虚拟环境，环境名字可自己确定，这里本人使用mypytorch作为环境名:</p>  <pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda create -n mypytorch python&#x3D;3.8<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>   安装成功后激活mypytorch环境：</p>  <pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda activate mypytorch<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>  注意：10.2处应为cuda的安装版本号<br>  编辑~/.bashrc 文件，设置使用mypytorch环境下的python3.8</p>  <pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">alias python&#x3D;&#39;&#x2F;home&#x2F;linxu&#x2F;anaconda3&#x2F;envs&#x2F;mypytorch&#x2F;bin&#x2F;python3.8&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>  注意：python路径应改为自己机器上的路径<br>  保存退出后执行： source ~/.bashrc<br>  该命令将自动回到base环境，再执行 conda activate mypytorch 到pytorch环境。</p><h3 id="5-2-Swin-Transformer项目安装"><a href="#5-2-Swin-Transformer项目安装" class="headerlink" title="5.2 Swin Transformer项目安装"></a>5.2 Swin Transformer项目安装</h3><p>1） 安装mmcv<br>克隆mmcv项目到本地</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git clone -b v1.3.1 https:&#x2F;&#x2F;github.com&#x2F;open-mmlab&#x2F;mmcv.git<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cd mmcvMMCV_WITH_OPS&#x3D;1 pip install -e .<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>检查</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">pip list<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2） 安装mmdetection<br>参考：<a href="https://mmdetection.readthedocs.io/zh_CN/v2.19.1/get_started.html">https://mmdetection.readthedocs.io/zh_CN/v2.19.1/get_started.html</a></p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git clone -b v2.11.0 https:&#x2F;&#x2F;github.com&#x2F;open-mmlab&#x2F;mmdetection.gitcd mmdetectionpip install -r requirements&#x2F;build.txtpip install -v -e . # or &quot;python setup.py<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ol start="3"><li>安装apex</li></ol><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git clone https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;apex<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cd apex<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python setup.py install<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意：cuda的版本应和cudatoolkit一致</p><p>4）安装Swin-Transformer-Object-Detection</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git clone https:&#x2F;&#x2F;github.com&#x2F;SwinTransformer&#x2F;Swin-Transformer-ObjectDetection.git<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>执行</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cd Swin-Transformer-Object-Detectionpython setup.py develop <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>下载预训练权重文件<br>下载mask_rcnn_swin_tiny_patch4_window7_1x.pth权重文件，并放置在Swin-Transformer-ObjectDetection文件夹下</p><p>百度网盘下载链接：</p><pre class="line-numbers language-txt" data-language="txt"><code class="language-txt">链接：https://pan.baidu.com/s/1cO3ln3fokP3cazveC-bjuw 提取码：uzzk <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>测试命令：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python demo&#x2F;image_demo.py demo&#x2F;demo.jpg configs&#x2F;swin&#x2F;mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco.py mask_rcnn_swin_tiny_patch4_window7_1x.pth<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220221161406131-1868308853.jpg"></p><h3 id="5-3-标注自己的数据集"><a href="#5-3-标注自己的数据集" class="headerlink" title="5.3.标注自己的数据集"></a>5.3.标注自己的数据集</h3><ol><li>安装图像标注工具labelImg<br>克隆labelImg</li></ol><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git clone https:&#x2F;&#x2F;github.com&#x2F;tzutalin&#x2F;labelImg.git<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>使用Anaconda安装<br>到labelImg路径下执行命令</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda install pyqt&#x3D;5pip install lxmlpyrcc5 -o libs&#x2F;resources.py resources.qrcpython labelImg.py<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>添加自定义类别<br>修改文件labelImg/data/predefined_classes.txt</li></ol><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">ballmessitrophy<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>3）使用labelImg进行图像标注<br>用labelImg标注生成PASCAL VOC格式的xml标记文件。例如：<br><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211110447357-1157834920.png"><br>width =1000<br>height = 654<br>PASCAL VOC标记文件如下：</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220211110617429-745834298.png"></p><h3 id="5-4-准备自己的数据集"><a href="#5-4-准备自己的数据集" class="headerlink" title="5.4 准备自己的数据集"></a>5.4 准备自己的数据集</h3><ol><li>下载项目文件<br>从百度网盘下载<br>VOCdevkit_bm.zip (下载到并解压)<br>prepare_mydata.py<br>pascal_voc.py<br>testfiles.zip (下载到Swin-Transformer-Object-Detection目录下并解压)</li></ol><pre class="line-numbers language-txt" data-language="txt"><code class="language-txt">链接：https://pan.baidu.com/s/1cO3ln3fokP3cazveC-bjuw 提取码：uzzk <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>2）解压建立或自行建立数据集<br> 使用PASCAL VOC数据集的目录结构:<br> 建立文件夹层次为 VOCdevkit / VOC2007<br> VOC2007下面建立三个文件夹：Annotations，JPEGImages和ImageSets/Main<br>JPEGImages放所有的数据集图片；Annotations放所有的xml标记文件；ImageSets/Main下存放训练<br>集、验证集、测试集划分文件（目前为空）</p><p>执行python脚本：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python prepare_mydata.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意根据自己的数据集修改其中的classes=[“ball”,”messi”]</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220221160711109-1162722623.png"></p><p>ImageSets/Main目录下可以看到生成四个文件</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220221160855808-178554785.png"></p><ul><li>train.txt给出了训练集图片文件的列表（不含文件名后缀）</li><li>val.txt给出了验证集图片文件的列表</li><li>test.txt给出了测试集图片文件的列表</li><li>trainval.txt给出了训练集和验证集图片文件的列表</li></ul><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220221160351812-1085984169.png"></p><p>4）PASCAL VOC数据格式转换成COCO数据格式<br>执行python脚本</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python pascal_voc.py VOCdevkit --out-dir mycoco --out-format coco<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意根据自己的数据集修改其中的voc_classes = [“ball”, “messi”]<br>然后，把转成的COCO数据格式的数据的目录结构准备成COCO目录结构格式。</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220221160543785-1818576242.png"></p><p>在Swin-Transformer-Object-Detection根目录下面，创建目录结构如下：</p><pre class="line-numbers language-tree" data-language="tree"><code class="language-tree">└── data ├── coco ├── annotations ├── instances_train2017.json └── instances_val2017.json ├── train2017 └── val2017<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中：<br>train2017放置voc-images/train2017中的图片<br>val2017放置voc-images/val2017中的图片<br>voc07_train.json改名为instances_train2017.json<br>voc07_val.json改名为instances_val2017.json</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220221160225351-1394774337.png"></p><h3 id="5-5-修改配置文件"><a href="#5-5-修改配置文件" class="headerlink" title="5.5 修改配置文件"></a>5.5 修改配置文件</h3><h4 id="5-5-1-step1"><a href="#5-5-1-step1" class="headerlink" title="5.5.1 step1"></a>5.5.1 step1</h4><p>1） 修改changemaskrcnn.py中num_class并执行该脚本, 产生新的权重文件</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python changemaskrcnn.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220221161808004-2104884261.png"></p><h4 id="5-5-2-step2"><a href="#5-5-2-step2" class="headerlink" title="5.5.2 step2"></a>5.5.2 step2</h4><p>2）修改 <code>configs/_base_/models/mask_rcnn_swin_fpn.py</code> 中num_classes， 共两处</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220221162317995-234270455.png"></p><h4 id="5-5-3-step3"><a href="#5-5-3-step3" class="headerlink" title="5.5.3 step3"></a>5.5.3 step3</h4><p>3）修改 <code>configs/_base_/default_runtime.py</code> 中interval, load_from</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220221162621446-1280058361.png"></p><h4 id="5-5-4-step4"><a href="#5-5-4-step4" class="headerlink" title="5.5.4 step4"></a>5.5.4 step4</h4><p>4）修改 <code>configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco.py</code> 中的 <code>_base_</code>,<code>max_epochs</code>, <code>lr</code></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220221163511423-14259905.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220221163400517-1059097252.png"></p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220221163441270-986859479.png"></p><h4 id="5-5-5-step5"><a href="#5-5-5-step5" class="headerlink" title="5.5.5 step5"></a>5.5.5 step5</h4><p>5） 修改 <code>configs/_base_/datasets/coco_detection.py</code> 中的<br>samples_per_gpu=2,<br>workers_per_gpu=2,</p><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220221162928881-1731921953.png"></p><h4 id="5-5-6-step6"><a href="#5-5-6-step6" class="headerlink" title="5.5.6 step6"></a>5.5.6 step6</h4><p>6）修改mmdet\datasets\coco.py中的CLASSES<br>另外，如果在训练时出现错误：IndexError: list index out of range<br>注释掉coco.py中_segm2json(self, results)函数中的以下语句 （268~271行）</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#if isinstance(segms[i]['counts'], bytes):</span><span class="token comment">#   segms[i]['counts'] = segms[i]['counts'].decode()</span><span class="token comment">#data['segmentation'] = segms[i]</span><span class="token comment">#segm_json_results.append(data)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220221163827563-602364166.png"></p><h4 id="5-5-7-step7"><a href="#5-5-7-step7" class="headerlink" title="5.5.7 step7"></a>5.5.7 step7</h4><p>为了保持类别和类名一致，需要修改./mmdet/core/evaluation/class_names.py中的内容<br><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220221172214704-169784419.png"></p><p>同时还需要修改.\mmdet\datasets\coco.py中的内容<br><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220221172233392-1299562339.png"></p><p>最后为了避免第三方库受之前环境配置的影响，重新编译一遍源码。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python setup.py install<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="5-6-训练自己的数据集"><a href="#5-6-训练自己的数据集" class="headerlink" title="5.6 训练自己的数据集"></a>5.6 训练自己的数据集</h3><ol><li>训练命令<br>在Swin-Transformer-Object-Detection路径下执行：</li></ol><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python tools&#x2F;train.py configs&#x2F;swin&#x2F;mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/blog/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/SwinTranformer-Det/1571518-20220221170031396-2137249070.png"></p><ol start="2"><li>训练结果的查看<br>查看Swin-Transformer-Object-Detection/work_dirs目录下的文件</li></ol><h3 id="5-7-测试训练出的网络模型"><a href="#5-7-测试训练出的网络模型" class="headerlink" title="5.7 测试训练出的网络模型"></a>5.7 测试训练出的网络模型</h3><ol><li>测试图片</li></ol><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python demo&#x2F;image_demo.py testfiles&#x2F;img1.jpg configs&#x2F;swin&#x2F;mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco.py work_dirs&#x2F;mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco&#x2F;latest.pth --score-thr 0.5<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>2）测试视频</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python demo&#x2F;video_demo.py testfiles&#x2F;messi.mp4 configs&#x2F;swin&#x2F;mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco.py work_dirs&#x2F;mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco&#x2F;latest.pth --score-thr 0.5 --show<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>3）性能统计</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">python tools<span class="token operator">/</span>test<span class="token punctuation">.</span>py configs<span class="token operator">/</span>swin<span class="token operator">/</span>mask_rcnn_swin_tiny_patch4_window7_mstrain_480<span class="token operator">-</span>800_adamw_1x_coco<span class="token punctuation">.</span>py work_dirs<span class="token operator">/</span>mask_rcnn_swin_tiny_patch4_window7_mstrain_480<span class="token operator">-</span>800_adamw_1x_coco<span class="token operator">/</span>latest<span class="token punctuation">.</span>pth <span class="token operator">-</span><span class="token operator">-</span><span class="token builtin">eval</span> bbox<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>4）日志分析</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python tools&#x2F;analysis_tools&#x2F;analyze_logs.py plot_curve work_dirs&#x2F;mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco&#x2F;20211225_030436.log.json<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 炼丹术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于COCO数据集验证的目标检测算法天梯排行榜</title>
      <link href="/blog/2022/02/16/ji-yu-coco-shu-ju-ji-yan-zheng-de-mu-biao-jian-ce-suan-fa-tian-ti-pai-xing-bang/"/>
      <url>/blog/2022/02/16/ji-yu-coco-shu-ju-ji-yan-zheng-de-mu-biao-jian-ce-suan-fa-tian-ti-pai-xing-bang/</url>
      
        <content type="html"><![CDATA[<h1 id="基于COCO数据集验证的目标检测算法天梯排行榜"><a href="#基于COCO数据集验证的目标检测算法天梯排行榜" class="headerlink" title="基于COCO数据集验证的目标检测算法天梯排行榜"></a>基于COCO数据集验证的目标检测算法天梯排行榜</h1><hr><h2 id="AP50"><a href="#AP50" class="headerlink" title="AP50"></a>AP50</h2><p><img src="/blog/blog/2022/02/16/ji-yu-coco-shu-ju-ji-yan-zheng-de-mu-biao-jian-ce-suan-fa-tian-ti-pai-xing-bang/%E5%9F%BA%E4%BA%8ECOCO%E6%95%B0%E6%8D%AE%E9%9B%86%E9%AA%8C%E8%AF%81%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E5%A4%A9%E6%A2%AF%E6%8E%92%E8%A1%8C%E6%A6%9C/1571518-20220216175113001-141667941.jpg"></p><table><thead><tr><th align="center">Rank</th><th>Model</th><th><strong>box AP</strong></th><th>AP50</th><th>Paper</th><th>Code</th><th>Result</th><th>Year</th><th>Tags</th></tr></thead><tbody><tr><td align="center">1</td><td><a href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and">SwinV2-G (HTC++)</a></td><td>63.1</td><td></td><td><a href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and">Swin Transformer V2: Scaling Up Capacity and Resolution</a></td><td><a href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and#code">Link</a></td><td></td><td>2021</td><td><strong>Swin-Transformer</strong></td></tr><tr><td align="center">2</td><td><a href="https://paperswithcode.com/paper/florence-a-new-foundation-model-for-computer">Florence-CoSwin-H</a></td><td>62.4</td><td></td><td><a href="https://paperswithcode.com/paper/florence-a-new-foundation-model-for-computer">Florence: A New Foundation Model for Computer Vision</a></td><td></td><td></td><td>2021</td><td><strong>Swin-Transformer</strong></td></tr><tr><td align="center">3</td><td><a href="https://paperswithcode.com/paper/grounded-language-image-pre-training">GLIP (Swin-L, multi-scale)</a></td><td>61.5</td><td>79.5</td><td><a href="https://paperswithcode.com/paper/grounded-language-image-pre-training">Grounded Language-Image Pre-training</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong>;<br>Vision Language**;<br><strong>Dynamic Head</strong>;<br>**BERT-Base</td></tr><tr><td align="center">4</td><td><a href="https://paperswithcode.com/paper/end-to-end-semi-supervised-object-detection">Soft Teacher + Swin-L (HTC++, multi-scale)</a></td><td>61.3</td><td></td><td><a href="https://paperswithcode.com/paper/end-to-end-semi-supervised-object-detection">End-to-End Semi-Supervised Object Detection with Soft Teacher</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong>;<br>Swin-Transformer</td></tr><tr><td align="center">5</td><td><a href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">DyHead (Swin-L, multi scale, self-training)</a></td><td>60.6</td><td>78.5</td><td><a href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">Dynamic Head: Unifying Object Detection Heads with Attentions</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong>;<br>Swin-Transformer</td></tr><tr><td align="center">6</td><td><a href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">Dual-Swin-L (HTC, multi-scale)</a></td><td>60.1</td><td></td><td><a href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">CBNetV2: A Composite Backbone Network Architecture for Object Detection</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br>Swin-Transformer</td></tr><tr><td align="center">7</td><td><a href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">Dual-Swin-L (HTC, single-scale)</a></td><td>59.4</td><td></td><td><a href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">CBNetV2: A Composite Backbone Network Architecture for Object Detection</a></td><td></td><td></td><td>2021</td><td><strong>Swin-Transformer</strong></td></tr><tr><td align="center">8</td><td><a href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal-L (DyHead, multi-scale)</a></td><td>58.9</td><td></td><td><a href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal Self-attention for Local-Global Interactions in Vision Transformers</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br>Focal-Transformer</td></tr><tr><td align="center">9</td><td><a href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">DyHead (Swin-L, multi scale)</a></td><td>58.7</td><td>77.1</td><td><a href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">Dynamic Head: Unifying Object Detection Heads with Attentions</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br>Swin-Transformer</td></tr><tr><td align="center">10</td><td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin-L (HTC++, multi scale)</a></td><td>58.7</td><td></td><td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br><strong>Swin-Transformer</strong></td></tr><tr><td align="center">11</td><td><a href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal-L (HTC++, multi-scale)</a></td><td>58.4</td><td></td><td><a href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal Self-attention for Local-Global Interactions in Vision Transformers</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong></td></tr><tr><td align="center">12</td><td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin-L (HTC++, single scale)</a></td><td>57.7</td><td></td><td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>Swin-Transformer</strong></td></tr><tr><td align="center">13</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-D6 (1280, single-scale, 34 fps)</a></td><td>57.3</td><td>75.0</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br>YOLO</td></tr><tr><td align="center">14</td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (Swin-L, single)</a></td><td>56.5</td><td></td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td><td></td><td></td><td>2021</td><td><strong>Transformer</strong><br><strong>single scale</strong></td></tr><tr><td align="center">15</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-E6 (1280, single-scale, 45 fps)</a></td><td>56.4</td><td>74.1</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">16</td><td><a href="https://paperswithcode.com/paper/probabilistic-two-stage-detection">CenterNet2 (Res2Net-101-DCN-BiFPN, self-training, 1560 single-scale)</a></td><td>56.4</td><td>74.0</td><td><a href="https://paperswithcode.com/paper/probabilistic-two-stage-detection">Probabilistic two-stage detection</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>FPN</strong><br><strong>DCN</strong></td></tr><tr><td align="center">17</td><td><a href="https://paperswithcode.com/paper/queryinst-parallelly-supervised-mask-query">QueryInst (single-scale)</a></td><td>56.1</td><td>75.9</td><td><a href="https://paperswithcode.com/paper/queryinst-parallelly-supervised-mask-query">Instances as Queries</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">18</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P7 with TTA</a></td><td>55.8</td><td>73.2</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">19</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-64x4d, multi-scale)</a></td><td>55.7</td><td>74.2</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">20</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-W6 (1280, single-scale, 66 fps)</a></td><td>55.5</td><td>73.2</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">21</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P7 CSP-P7 (single-scale, 16 fps)</a></td><td>55.4</td><td>73.3</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">22</td><td><a href="https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural">CSP-p6 + Mish (multi-scale)</a></td><td>55.2</td><td>72.9</td><td><a href="https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural">Mish: A Self Regularized Non-Monotonic Activation Function</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">23</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P6 with TTA</a></td><td>54.9</td><td>72.6</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">24</td><td><a href="https://paperswithcode.com/paper/simple-copy-paste-is-a-strong-data">Cascade Eff-B7 NAS-FPN (1280)</a></td><td>54.8</td><td></td><td><a href="https://paperswithcode.com/paper/simple-copy-paste-is-a-strong-data">Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>NAS-FPN</strong></td></tr><tr><td align="center">25</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-32x4d, multi-scale)</a></td><td>54.7</td><td>73.5</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">26</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P6 CSP-P6 (single-scale, 32 fps)</a></td><td>54.3</td><td>72.3</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">27</td><td><a href="https://paperswithcode.com/paper/rethinking-pre-training-and-self-training">SpineNet-190 (1280, with Self-training on OpenImages, single-scale)</a></td><td>54.3</td><td></td><td><a href="https://paperswithcode.com/paper/rethinking-pre-training-and-self-training">Rethinking Pre-training and Self-training</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong></td></tr><tr><td align="center">28</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08d (Res2Net-101, DCN, multi-scale)</a></td><td>54.1</td><td>71.6</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">29</td><td><a href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet-D7 (single-scale)</a></td><td>53.7</td><td>72.4</td><td><a href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet: Scalable and Efficient Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">30</td><td><a href="https://paperswithcode.com/paper/probabilistic-anchor-assignment-with-iou">PAA (ResNext-152-32x8d + DCN, multi-scale)</a></td><td>53.5</td><td>71.6</td><td><a href="https://paperswithcode.com/paper/probabilistic-anchor-assignment-with-iou">Probabilistic Anchor Assignment with IoU Prediction for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">31</td><td><a href="https://paperswithcode.com/paper/location-sensitive-visual-recognition-with">LSNet (Res2Net-101+ DCN, multi-scale)</a></td><td>53.5</td><td>71.1</td><td><a href="https://paperswithcode.com/paper/location-sensitive-visual-recognition-with">Location-Sensitive Visual Recognition with Cross-IOU Loss</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">32</td><td><a href="https://paperswithcode.com/paper/resnest-split-attention-networks">ResNeSt-200 (multi-scale)</a></td><td>53.3</td><td>72.0</td><td><a href="https://paperswithcode.com/paper/resnest-split-attention-networks">ResNeSt: Split-Attention Networks</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong></td></tr><tr><td align="center">33</td><td><a href="https://paperswithcode.com/paper/cbnet-a-novel-composite-backbone-network">Cascade Mask R-CNN (Triple-ResNeXt152, multi-scale)</a></td><td>53.3</td><td>71.9</td><td><a href="https://paperswithcode.com/paper/cbnet-a-novel-composite-backbone-network">CBNet: A Novel Composite Backbone Network Architecture for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">34</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-32x4d, single-scale)</a></td><td>53.3</td><td>71.6</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>single scale</strong></td></tr><tr><td align="center">35</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (Res2Net-101, DCN, multiscale)</a></td><td>53.3</td><td>70.9</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">36</td><td><a href="https://paperswithcode.com/paper/relationnet-bridging-visual-representations">RelationNet++ (ResNeXt-64x4d-101-DCN)</a></td><td>52.7</td><td></td><td><a href="https://paperswithcode.com/paper/relationnet-bridging-visual-representations">RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">37</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P5 with TTA</a></td><td>52.5</td><td>70.3</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">38</td><td><a href="https://paperswithcode.com/paper/deformable-detr-deformable-transformers-for-1">Deformable DETR (ResNeXt-101+DCN)</a></td><td>52.3</td><td>71.9</td><td><a href="https://paperswithcode.com/paper/deformable-detr-deformable-transformers-for-1">Deformable DETR: Deformable Transformers for End-to-End Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">39</td><td><a href="https://paperswithcode.com/paper/global-context-networks">GCNet (ResNeXt-101 + DCN + cascade + GC r4)</a></td><td>52.3</td><td>70.9</td><td><a href="https://paperswithcode.com/paper/global-context-networks">Global Context Networks</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong><br><strong>GCN</strong></td></tr><tr><td align="center">40</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-190, 1280x1280)</a></td><td>52.1</td><td>71.8</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">41</td><td><a href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints v2 (ResNeXt-101, DCN, multi-scale)</a></td><td>52.1</td><td>70.1</td><td><a href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints V2: Verification Meets Regression for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong>;<br><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td align="center"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td align="center">42</td><td><a href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">AC-FPN Cascade R-CNN (X-152-32x8d-FPN-IN5k, multi scale, only CEM)</a></td><td>51.9</td><td>70.4</td><td><a href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">Attention-guided Context Feature Pyramid Network for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>FPN</strong></td></tr><tr><td align="center">43</td><td><a href="https://paperswithcode.com/paper/ota-optimal-transport-assignment-for-object">OTA (ResNeXt-101+DCN, multiscale)</a></td><td>51.5</td><td>68.6</td><td><a href="https://paperswithcode.com/paper/ota-optimal-transport-assignment-for-object">OTA: Optimal Transport Assignment for Object Detection</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">44</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08d (Res2Net-101, DCN, single-scale)</a></td><td>51.3</td><td>70.0</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">45</td><td><a href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">TSD (SENet154-DCN,multi-scale)</a></td><td>51.2</td><td>71.9</td><td><a href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">Revisiting the Sibling Head in Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">46</td><td><a href="https://paperswithcode.com/paper/yolox-exceeding-yolo-series-in-2021">YOLOX-X (Modified CSP v5)</a></td><td>51.2</td><td>69.6</td><td><a href="https://paperswithcode.com/paper/yolox-exceeding-yolo-series-in-2021">YOLOX: Exceeding YOLO Series in 2021</a></td><td></td><td></td><td>2021</td><td><strong>YOLO</strong></td></tr><tr><td align="center">47</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-143, 1280x1280)</a></td><td>50.7</td><td>70.4</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">48</td><td><a href="https://paperswithcode.com/paper/bridging-the-gap-between-anchor-based-and">ATSS (ResNetXt-64x4d-101+DCN,multi-scale)</a></td><td>50.7</td><td>68.9</td><td><a href="https://paperswithcode.com/paper/bridging-the-gap-between-anchor-based-and">Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">49</td><td><a href="https://paperswithcode.com/paper/learning-data-augmentation-strategies-for">NAS-FPN (AmoebaNet-D, learned aug)</a></td><td>50.7</td><td></td><td><a href="https://paperswithcode.com/paper/learning-data-augmentation-strategies-for">Learning Data Augmentation Strategies for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">50</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (Res2Net-101, DCN)</a></td><td>50.6</td><td>69</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>DCN</strong></td></tr><tr><td align="center">51</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, DCN, multiscale test)</a></td><td>50.2</td><td>70.3</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">52</td><td><a href="https://paperswithcode.com/paper/scale-equalizing-pyramid-convolution-for">FreeAnchor + SEPC (DCN, ResNext-101-64x4d)</a></td><td>50.1</td><td>69.8</td><td><a href="https://paperswithcode.com/paper/scale-equalizing-pyramid-convolution-for">Scale-Equalizing Pyramid Convolution for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">53</td><td><a href="https://paperswithcode.com/paper/d2det-towards-high-quality-object-detection">D2Det (ResNet-101-DCN, multi-scale test)</a></td><td>50.1</td><td>69.4</td><td><a href="https://paperswithcode.com/paper/d2det-towards-high-quality-object-detection">D2Det: Towards High Quality Object Detection and Instance Segmentation</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">54</td><td><a href="https://paperswithcode.com/paper/dynamic-r-cnn-towards-high-quality-object">Dynamic R-CNN (ResNet-101-DCN, multi-scale)</a></td><td>50.1</td><td>68.3</td><td><a href="https://paperswithcode.com/paper/dynamic-r-cnn-towards-high-quality-object">Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">55</td><td><a href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">TSD (ResNet-101-Deformable, Image Pyramid)</a></td><td>49.4</td><td>69.6</td><td><a href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">Revisiting the Sibling Head in Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>ResNet</strong></td></tr><tr><td align="center">56</td><td><a href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints v2 (ResNeXt-101, DCN)</a></td><td>49.4</td><td>68.9</td><td><a href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints V2: Verification Meets Regression for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">57</td><td><a href="https://paperswithcode.com/paper/corner-proposal-network-for-anchor-free-two">CPNDet (Hourglass-104, multi-scale)</a></td><td>49.2</td><td>67.3</td><td><a href="https://paperswithcode.com/paper/corner-proposal-network-for-anchor-free-two">Corner Proposal Network for Anchor-free, Two-stage Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong></td></tr><tr><td align="center">58</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNeXt-101, 32x4d, DCN)</a></td><td>49</td><td>67.6</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">59</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, DCN, single scale)</a></td><td>48.9</td><td>69.3</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>single scale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">60</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08 (Res2Net-50, DCN, single-scale)</a></td><td>48.8</td><td>67.5</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">61</td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (ResNet101, single scale)</a></td><td>48.7</td><td></td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td><td></td><td></td><td>2021</td><td><strong>Transformer</strong><br><strong>single scale</strong></td></tr><tr><td align="center">62</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-96, 1024x1024)</a></td><td>48.6</td><td>68.4</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">63</td><td><a href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">TridentNet (ResNet-101-Deformable, Image Pyramid)</a></td><td>48.4</td><td>69.7</td><td><a href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">Scale-Aware Trident Networks for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">64</td><td><a href="https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze">GCNet (ResNeXt-101 + DCN + cascade + GC r4)</a></td><td>48.4</td><td>67.6</td><td><a href="https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>DCN</strong><br><strong>GCN</strong></td></tr><tr><td align="center">65</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-101-DCN)</a></td><td>48.3</td><td>66.5</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">66</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-learning-qualified-and">GFL (X-101-32x4d-DCN, single-scale)</a></td><td>48.2</td><td>67.4</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-learning-qualified-and">Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>single scale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">67</td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet101-FPN-3x, single-scale)</a></td><td>48.1</td><td></td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">68</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, single scale)</a></td><td>47.8</td><td>68.4</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>single scale</strong></td></tr><tr><td align="center">69</td><td><a href="https://paperswithcode.com/paper/matrix-nets-a-new-deep-architecture-for">MatrixNet Corners (ResNet-152, multi-scale)</a></td><td>47.8</td><td>66.2</td><td><a href="https://paperswithcode.com/paper/matrix-nets-a-new-deep-architecture-for">Matrix Nets: A New Deep Architecture for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">70</td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (ResNet50, single scale)</a></td><td>47.8</td><td></td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td><td></td><td></td><td>2021</td><td><strong>Transformer</strong><br><strong>single scale</strong></td></tr><tr><td align="center">71</td><td><a href="https://paperswithcode.com/paper/soft-anchor-point-object-detection">SAPD (ResNeXt-101, single-scale)</a></td><td>47.4</td><td>67.4</td><td><a href="https://paperswithcode.com/paper/soft-anchor-point-object-detection">Soft Anchor-Point Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>single scale</strong></td></tr><tr><td align="center">72</td><td><a href="https://paperswithcode.com/paper/path-aggregation-network-for-instance">PANet (ResNeXt-101, multi-scale)</a></td><td>47.4</td><td>67.2</td><td><a href="https://paperswithcode.com/paper/path-aggregation-network-for-instance">Path Aggregation Network for Instance Segmentation</a></td><td></td><td></td><td>2018</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">73</td><td><a href="https://paperswithcode.com/paper/190807919">HTC (HRNetV2p-W48)</a></td><td>47.3</td><td>65.9</td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">74</td><td><a href="https://paperswithcode.com/paper/hybrid-task-cascade-for-instance-segmentation">HTC (ResNeXt-101-FPN)</a></td><td>47.1</td><td>63.9</td><td><a href="https://paperswithcode.com/paper/hybrid-task-cascade-for-instance-segmentation">Hybrid Task Cascade for Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">75</td><td><a href="https://paperswithcode.com/paper/centernet-object-detection-with-keypoint">CenterNet511 (Hourglass-104, multi-scale)</a></td><td>47.0</td><td>64.5</td><td><a href="https://paperswithcode.com/paper/centernet-object-detection-with-keypoint">CenterNet: Keypoint Triplets for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">76</td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNeXt101, multi-scale)</a></td><td>47.0</td><td></td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">77</td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet50-FPN-3x)</a></td><td>46.8</td><td></td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td><td></td><td></td><td>2021</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">78</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49, 896x896)</a></td><td>46.7</td><td>66.3</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">79</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101-DCN, multi-scale)</a></td><td>46.5</td><td>67.4</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">80</td><td><a href="https://paperswithcode.com/paper/houghnet-integrating-near-and-long-range">HoughNet (MS)</a></td><td>46.4</td><td>65.1</td><td><a href="https://paperswithcode.com/paper/houghnet-integrating-near-and-long-range">HoughNet: Integrating near and long-range evidence for bottom-up object detection</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong></td></tr><tr><td align="center">81</td><td><a href="https://paperswithcode.com/paper/reducing-label-noise-in-anchor-free-object">PPDet (ResNeXt-101-FPN, multiscale)</a></td><td>46.3</td><td>64.8</td><td><a href="https://paperswithcode.com/paper/reducing-label-noise-in-anchor-free-object">Reducing Label Noise in Anchor-Free Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>FPN</strong></td></tr><tr><td align="center">82</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-101)</a></td><td>46.2</td><td>64.3</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNet</strong></td></tr><tr><td align="center">83</td><td><a href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER (ResNet-101)</a></td><td>46.1</td><td>67.0</td><td><a href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER: Efficient Multi-Scale Training</a></td><td></td><td></td><td>2018</td><td><strong>ResNet</strong></td></tr><tr><td align="center">84</td><td><a href="https://paperswithcode.com/paper/190807919">Mask R-CNN (HRNetV2p-W48 + cascade)</a></td><td>46.1</td><td>64.0</td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">85</td><td><a href="https://paperswithcode.com/paper/deformable-convnets-v2-more-deformable-better">DCNv2 (ResNet-101, multi-scale)</a></td><td>46.0</td><td>67.9</td><td><a href="https://paperswithcode.com/paper/deformable-convnets-v2-more-deformable-better">Deformable ConvNets v2: More Deformable, Better Results</a></td><td></td><td></td><td>2018</td><td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">86</td><td><a href="https://paperswithcode.com/paper/localization-uncertainty-estimation-for">Gaussian-FCOS</a></td><td>46</td><td></td><td><a href="https://paperswithcode.com/paper/localization-uncertainty-estimation-for">Localization Uncertainty Estimation for Anchor-Free Object Detection</a></td><td></td><td></td><td>2020</td><td></td></tr><tr><td align="center">87</td><td><a href="https://paperswithcode.com/paper/instaboost-boosting-instance-segmentation-via">Cascade R-CNN-FPN (ResNet-101, map-guided)</a></td><td>45.9</td><td>64.2</td><td><a href="https://paperswithcode.com/paper/instaboost-boosting-instance-segmentation-via">InstaBoost: Boosting Instance Segmentation via Probability Map Guided Copy-Pasting</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">88</td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNeXt101, single-scale)</a></td><td>45.9</td><td></td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>single scale</strong></td></tr><tr><td align="center">89</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask+VoVNetV2-99 (single-scale)</a></td><td>45.8</td><td>64.5</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">90</td><td><a href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">D-RFCN + SNIP (DPN-98 with flip, multi-scale)</a></td><td>45.7</td><td>67.3</td><td><a href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">An Analysis of Scale Invariance in Object Detection - SNIP</a></td><td></td><td></td><td>2017</td><td><strong>multiscale</strong></td></tr><tr><td align="center">91</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4 (CD53)</a></td><td>45.5</td><td>64.1</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">92</td><td><a href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO (608x608)</a></td><td>45.2</td><td>65.2</td><td><a href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>YOLO</strong></td></tr><tr><td align="center">93</td><td><a href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">AC-FPN Cascade R-CNN (ResNet-101, single scale)</a></td><td>45</td><td>64.4</td><td><a href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">Attention-guided Context Feature Pyramid Network for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong><br><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">94</td><td><a href="https://paperswithcode.com/paper/freeanchor-learning-to-match-anchors-for">FreeAnchor (ResNeXt-101)</a></td><td>44.8</td><td>64.3</td><td><a href="https://paperswithcode.com/paper/freeanchor-learning-to-match-anchors-for">FreeAnchor: Learning to Match Anchors for Visual Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">95</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-64x4d-101-FPN 4 + improvements)</a></td><td>44.7</td><td>64.1</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">96</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask+VoVNet2-57 (single-scale)</a></td><td>44.7</td><td>63.1</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">97</td><td><a href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">FSAF (ResNeXt-101, multi-scale)</a></td><td>44.6</td><td>65.2</td><td><a href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">98</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101, DCN, 500 scale)</a></td><td>44.6</td><td>65.0</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">99</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask + X-101-32x8d (single-scale)</a></td><td>44.6</td><td>63.4</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">100</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49, 640x640)</a></td><td>44.3</td><td>63.8</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">101</td><td><a href="https://paperswithcode.com/paper/you-only-look-one-level-feature">YOLOF-DC5</a></td><td>44.3</td><td>62.9</td><td><a href="https://paperswithcode.com/paper/you-only-look-one-level-feature">You Only Look One-level Feature</a></td><td></td><td></td><td>2021</td><td><strong>YOLO</strong></td></tr><tr><td align="center">102</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-50)</a></td><td>44.3</td><td>62.3</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNet</strong></td></tr><tr><td align="center">103</td><td><a href="https://paperswithcode.com/paper/feature-intertwiner-for-object-detection-1">InterNet (ResNet-101-FPN, multi-scale)</a></td><td>44.2</td><td>67.5</td><td><a href="https://paperswithcode.com/paper/feature-intertwiner-for-object-detection-1">Feature Intertwiner for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong><br><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">104</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (VGG-16, multi-scale)</a></td><td>44.2</td><td>64.6</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td><td></td><td></td><td>2018</td><td><strong>multiscale</strong></td></tr><tr><td align="center">105</td><td><a href="https://paperswithcode.com/paper/lip-local-importance-based-pooling">Faster R-CNN (LIP-ResNet-101-MD w FPN)</a></td><td>43.9</td><td>65.7</td><td><a href="https://paperswithcode.com/paper/lip-local-importance-based-pooling">LIP: Local Importance-based Pooling</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">106</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (ResNet-101, multi-scale)</a></td><td>43.9</td><td>64.4</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td><td></td><td></td><td>2018</td><td><strong>multiscale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">107</td><td><a href="https://paperswithcode.com/paper/learning-spatial-fusion-for-single-shot">YOLOv3 @800 + ASFF* (Darknet-53)</a></td><td>43.9</td><td>64.1</td><td><a href="https://paperswithcode.com/paper/learning-spatial-fusion-for-single-shot">Learning Spatial Fusion for Single-Shot Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>YOLO</strong></td></tr><tr><td align="center">108</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td><td>43.9</td><td>63.5</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">109</td><td><a href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">ExtremeNet (Hourglass-104, multi-scale)</a></td><td>43.7</td><td>60.5</td><td><a href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">Bottom-up Object Detection by Grouping Extreme and Center Points</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">110</td><td><a href="https://paperswithcode.com/paper/yolov4-optimal-speed-and-accuracy-of-object">YOLOv4-608</a></td><td>43.5</td><td>65.7</td><td><a href="https://paperswithcode.com/paper/yolov4-optimal-speed-and-accuracy-of-object">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">111</td><td><a href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER (ResNet-50)</a></td><td>43.5</td><td>65.0</td><td><a href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER: Efficient Multi-Scale Training</a></td><td></td><td></td><td>2018</td><td><strong>ResNet</strong></td></tr><tr><td align="center">112</td><td><a href="https://paperswithcode.com/paper/190807919">CenterNet (HRNetV2-W48)</a></td><td>43.5</td><td></td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">113</td><td><a href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">D-RFCN + SNIP (ResNet-101, multi-scale)</a></td><td>43.4</td><td>65.5</td><td><a href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">An Analysis of Scale Invariance in Object Detection - SNIP</a></td><td></td><td></td><td>2017</td><td><strong>multiscale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">114</td><td><a href="https://paperswithcode.com/paper/grid-r-cnn">Grid R-CNN (ResNeXt-101-FPN)</a></td><td>43.2</td><td>63.0</td><td><a href="https://paperswithcode.com/paper/grid-r-cnn">Grid R-CNN</a></td><td></td><td></td><td>2018</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">115</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-101-64x4d-FPN)</a></td><td>43.2</td><td>62.8</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">116</td><td><a href="https://paperswithcode.com/paper/190408900">CornerNet-Saccade (Hourglass-104, multi-scale)</a></td><td>43.2</td><td></td><td><a href="https://paperswithcode.com/paper/190408900">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">117</td><td><a href="https://paperswithcode.com/paper/libra-r-cnn-towards-balanced-learning-for">Libra R-CNN (ResNeXt-101-FPN)</a></td><td>43.0</td><td>64</td><td><a href="https://paperswithcode.com/paper/libra-r-cnn-towards-balanced-learning-for">Libra R-CNN: Towards Balanced Learning for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">118</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101-DCN)</a></td><td>42.8</td><td>65.0</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">119</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet-49 (640, RetinaNet, single-scale)</a></td><td>42.8</td><td>62.3</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">120</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-101-FPN+, cascade)</a></td><td>42.8</td><td>62.1</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">121</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-high-quality-object-detection">Cascade R-CNN</a></td><td>42.8</td><td>62.1</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-high-quality-object-detection">Cascade R-CNN: High Quality Object Detection and Instance Segmentation</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">122</td><td><a href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">TridentNet (ResNet-101)</a></td><td>42.7</td><td>63.6</td><td><a href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">Scale-Aware Trident Networks for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">123</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-32x8d-101-FPN)</a></td><td>42.7</td><td>62.2</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">124</td><td><a href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask (ResNeXt-101-FPN-GN)</a></td><td>42.6</td><td>62.5</td><td><a href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">125</td><td><a href="https://paperswithcode.com/paper/tood-task-aligned-one-stage-object-detection">TAL + TAP</a></td><td>42.5</td><td>60.3</td><td><a href="https://paperswithcode.com/paper/tood-task-aligned-one-stage-object-detection">TOOD: Task-aligned One-stage Object Detection</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">126</td><td><a href="https://paperswithcode.com/paper/190807919">Faster R-CNN (HRNetV2p-W48)</a></td><td>42.4</td><td>63.6</td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">127</td><td><a href="https://paperswithcode.com/paper/hierarchical-shot-detector">HSD (Rest101, 768x768, single-scale test)</a></td><td>42.3</td><td>61.2</td><td><a href="https://paperswithcode.com/paper/hierarchical-shot-detector">Hierarchical Shot Detector</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">128</td><td><a href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet511 (Hourglass-104, multi-scale)</a></td><td>42.1</td><td>57.8</td><td><a href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet: Detecting Objects as Paired Keypoints</a></td><td></td><td></td><td>2018</td><td><strong>multiscale</strong></td></tr><tr><td align="center">129</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td><td>42.1</td><td></td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">130</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (HRNet-W32-5l)</a></td><td>42.0</td><td>60.4</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">131</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512+ (ResNet-101)</a></td><td>41.8</td><td>62.9</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>ResNet</strong></td></tr><tr><td align="center">132</td><td><a href="https://paperswithcode.com/paper/gradient-harmonized-single-stage-detector">GHM-C + GHM-R (RetinaNet-FPN-ResNeXt-101)</a></td><td>41.6</td><td>62.8</td><td><a href="https://paperswithcode.com/paper/gradient-harmonized-single-stage-detector">Gradient Harmonized Single-stage Detector</a></td><td></td><td></td><td>2018</td><td><strong>FPN</strong></td></tr><tr><td align="center">133</td><td><a href="https://paperswithcode.com/paper/objects-as-points">CenterNet-DLA (DLA-34, multi-scale)</a></td><td>41.6</td><td></td><td><a href="https://paperswithcode.com/paper/objects-as-points">Objects as Points</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">134</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49S, 640x640)</a></td><td>41.5</td><td>60.5</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">135</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101)</a></td><td>41</td><td>62.9</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">136</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (VGG-16, single-scale)</a></td><td>41.0</td><td>59.7</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td><td></td><td></td><td>2018</td><td><strong>single scale</strong></td></tr><tr><td align="center">137</td><td><a href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">FSAF (ResNet-101, single-scale)</a></td><td>40.9</td><td>61.5</td><td><a href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">138</td><td><a href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">RetinaNet (ResNeXt-101-FPN)</a></td><td>40.8</td><td>61.1</td><td><a href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">Focal Loss for Dense Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">139</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-50-FPN+, cascade)</a></td><td>40.6</td><td>59.9</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">140</td><td><a href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Faster R-CNN (Cascade RPN)</a></td><td>40.6</td><td>58.9</td><td><a href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">141</td><td><a href="https://paperswithcode.com/paper/deformable-kernels-adapting-effective">ResNet-50-DW-DPN (Deformable Kernels)</a></td><td>40.6</td><td></td><td><a href="https://paperswithcode.com/paper/deformable-kernels-adapting-effective">Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">142</td><td><a href="https://paperswithcode.com/paper/acquisition-of-localization-confidence-for">IoU-Net</a></td><td>40.6</td><td></td><td><a href="https://paperswithcode.com/paper/acquisition-of-localization-confidence-for">Acquisition of Localization Confidence for Accurate Object Detection</a></td><td></td><td></td><td>2018</td><td></td></tr><tr><td align="center">143</td><td><a href="https://paperswithcode.com/paper/190807919">FCOS (HRNetV2p-W48)</a></td><td>40.5</td><td>59.3</td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">144</td><td><a href="https://paperswithcode.com/paper/softer-nms-rethinking-bounding-box-regression">ResNet-50-FPN Mask R-CNN + KL Loss + var voting + soft-NMS</a></td><td>40.4</td><td></td><td><a href="https://paperswithcode.com/paper/softer-nms-rethinking-bounding-box-regression">Bounding Box Regression with Uncertainty for Accurate Object Detection</a></td><td></td><td></td><td>2018</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">145</td><td><a href="https://paperswithcode.com/paper/rdsnet-a-new-deep-architecture-for-reciprocal">RDSNet (ResNet-101, RetinaNet, mask, MBRM)</a></td><td>40.3</td><td>60.1</td><td><a href="https://paperswithcode.com/paper/rdsnet-a-new-deep-architecture-for-reciprocal">RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">146</td><td><a href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">ExtremeNet (Hourglass-104, single-scale)</a></td><td>40.2</td><td>55.5</td><td><a href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">Bottom-up Object Detection by Grouping Extreme and Center Points</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">147</td><td><a href="https://paperswithcode.com/paper/cross-iteration-batch-normalization">Mask R-CNN (ResNet-101-FPN, CBN)</a></td><td>40.1</td><td>60.5</td><td><a href="https://paperswithcode.com/paper/cross-iteration-batch-normalization">Cross-Iteration Batch Normalization</a></td><td></td><td></td><td>2020</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">148</td><td><a href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Fast R-CNN (Cascade RPN)</a></td><td>40.1</td><td>59.4</td><td><a href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">149</td><td><a href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN (ResNeXt-101-FPN)</a></td><td>39.8</td><td>62.3</td><td><a href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN</a></td><td></td><td></td><td>2017</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">150</td><td><a href="https://paperswithcode.com/paper/region-proposal-by-guided-anchoring">GA-Faster-RCNN</a></td><td>39.8</td><td>59.2</td><td><a href="https://paperswithcode.com/paper/region-proposal-by-guided-anchoring">Region Proposal by Guided Anchoring</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">151</td><td><a href="https://paperswithcode.com/paper/chainercv-a-library-for-deep-learning-in">FPN (ResNet101 backbone)</a></td><td>39.5</td><td></td><td><a href="https://paperswithcode.com/paper/chainercv-a-library-for-deep-learning-in">ChainerCV: a Library for Deep Learning in Computer Vision</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">152</td><td><a href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask (ResNet-50-FPN)</a></td><td>39.4</td><td>58.6</td><td><a href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">153</td><td><a href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO (320x320)</a></td><td>39.3</td><td>59.3</td><td><a href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>YOLO</strong></td></tr><tr><td align="center">154</td><td><a href="https://paperswithcode.com/paper/190409925">AA-ResNet-10 + RetinaNet</a></td><td>39.2</td><td></td><td><a href="https://paperswithcode.com/paper/190409925">Attention Augmented Convolutional Networks</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">155</td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNet50, single-scale)</a></td><td>39.2</td><td></td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">156</td><td><a href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">RetinaNet (ResNet-101-FPN)</a></td><td>39.1</td><td>59.1</td><td><a href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">Focal Loss for Dense Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">157</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-101-FPN+)</a></td><td>38.8</td><td>61.1</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">158</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (ResNet-101, single-scale)</a></td><td>38.8</td><td>59.4</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td><td></td><td></td><td>2018</td><td><strong>single scale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">159</td><td><a href="https://paperswithcode.com/paper/saccadenet-a-fast-and-accurate-object">SaccadeNet (DLA-34-DCN)</a></td><td>38.5</td><td>55.6</td><td><a href="https://paperswithcode.com/paper/saccadenet-a-fast-and-accurate-object">SaccadeNet: A Fast and Accurate Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>DCN</strong></td></tr><tr><td align="center">160</td><td><a href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN (ResNet-101-FPN)</a></td><td>38.2</td><td>60.3</td><td><a href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">161</td><td><a href="https://paperswithcode.com/paper/segmentation-is-all-you-need">WSMA-Seg</a></td><td>38.1</td><td></td><td><a href="https://paperswithcode.com/paper/segmentation-is-all-you-need">Segmentation is All You Need</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">162</td><td><a href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Faster R-CNN + FPN + CGD</a></td><td>37.9</td><td></td><td><a href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Compact Global Descriptor for Neural Networks</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">163</td><td><a href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet511 (Hourglass-52, single-scale)</a></td><td>37.8</td><td>53.7</td><td><a href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet: Detecting Objects as Paired Keypoints</a></td><td></td><td></td><td>2018</td><td><strong>single scale</strong></td></tr><tr><td align="center">164</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512+ (VGG-16)</a></td><td>37.6</td><td>58.7</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">165</td><td><a href="https://paperswithcode.com/paper/deformable-convolutional-networks">DeformConv-R-FCN (Aligned-Inception-ResNet)</a></td><td>37.5</td><td>58.0</td><td><a href="https://paperswithcode.com/paper/deformable-convolutional-networks">Deformable Convolutional Networks</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">166</td><td><a href="https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data">Faster R-CNN (ImageNet+300M)</a></td><td>37.4</td><td>58</td><td><a href="https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">167</td><td><a href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">Mask R-CNN (Bottleneck-injected ResNet-50, FPN)</a></td><td>36.9</td><td></td><td><a href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation</a></td><td></td><td></td><td>2020</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">168</td><td><a href="https://paperswithcode.com/paper/beyond-skip-connections-top-down-modulation">Faster R-CNN + TDM</a></td><td>36.8</td><td></td><td><a href="https://paperswithcode.com/paper/beyond-skip-connections-top-down-modulation">Beyond Skip Connections: Top-Down Modulation for Object Detection</a></td><td></td><td></td><td>2016</td><td></td></tr><tr><td align="center">169</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-50-FPN+)</a></td><td>36.5</td><td>59</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong>;<br><strong>ResNet</strong></td></tr><tr><td align="center">170</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512 (ResNet-101)</a></td><td>36.4</td><td>57.5</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>ResNet</strong></td></tr><tr><td align="center">171</td><td><a href="https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection">Faster R-CNN + FPN</a></td><td>36.2</td><td></td><td><a href="https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection">Feature Pyramid Networks for Object Detection</a></td><td></td><td></td><td>2016</td><td><strong>FPN</strong></td></tr><tr><td align="center">172</td><td><a href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">Faster R-CNN (Bottleneck-injected ResNet-50 and FPN)</a></td><td>35.9</td><td></td><td><a href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation</a></td><td></td><td></td><td>2020</td><td><strong>FPN</strong>;<br><strong>ResNet</strong></td></tr><tr><td align="center">173</td><td><a href="https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition">Faster R-CNN (box refinement, context, multi-scale testing)</a></td><td>34.9</td><td></td><td><a href="https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition">Deep Residual Learning for Image Recognition</a></td><td></td><td></td><td>2015</td><td><strong>multiscale</strong></td></tr><tr><td align="center">174</td><td><a href="https://paperswithcode.com/paper/speedaccuracy-trade-offs-for-modern">Faster R-CNN</a></td><td>34.7</td><td></td><td><a href="https://paperswithcode.com/paper/speedaccuracy-trade-offs-for-modern">Speed/accuracy trade-offs for modern convolutional object detectors</a></td><td></td><td></td><td>2016</td><td></td></tr><tr><td align="center">175</td><td><a href="https://paperswithcode.com/paper/190408900">CornerNet-Squeeze</a></td><td>34.4</td><td></td><td><a href="https://paperswithcode.com/paper/190408900">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">176</td><td><a href="https://paperswithcode.com/paper/a-multipath-network-for-object-detection">MultiPath Network</a></td><td>33.2</td><td></td><td><a href="https://paperswithcode.com/paper/a-multipath-network-for-object-detection">A MultiPath Network for Object Detection</a></td><td></td><td></td><td>2016</td><td></td></tr><tr><td align="center">177</td><td><a href="https://paperswithcode.com/paper/inside-outside-net-detecting-objects-in">ION</a></td><td>33.1</td><td>55.7</td><td><a href="https://paperswithcode.com/paper/inside-outside-net-detecting-objects-in">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</a></td><td></td><td></td><td>2015</td><td></td></tr><tr><td align="center">178</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512 (VGG-16)</a></td><td>33</td><td>54.5</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">179</td><td><a href="https://paperswithcode.com/paper/yolov3-an-incremental-improvement">YOLOv3 + Darknet-53</a></td><td>33.0</td><td></td><td><a href="https://paperswithcode.com/paper/yolov3-an-incremental-improvement">YOLOv3: An Incremental Improvement</a></td><td></td><td></td><td>2018</td><td><strong>YOLO</strong></td></tr><tr><td align="center">180</td><td><a href="https://paperswithcode.com/paper/ssd-single-shot-multibox-detector">SSD512</a></td><td>28.8</td><td>48.5</td><td><a href="https://paperswithcode.com/paper/ssd-single-shot-multibox-detector">SSD: Single Shot MultiBox Detector</a></td><td></td><td></td><td>2015</td><td></td></tr><tr><td align="center">181</td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MobileNetV2)</a></td><td>26.1</td><td></td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">182</td><td><a href="https://paperswithcode.com/paper/espnetv2-a-light-weight-power-efficient-and">ESPNetv2-512</a></td><td>26.0</td><td></td><td><a href="https://paperswithcode.com/paper/espnetv2-a-light-weight-power-efficient-and">ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network</a></td><td></td><td></td><td>2018</td><td></td></tr><tr><td align="center">183</td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MobileNetV3)</a></td><td>25.5</td><td></td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">184</td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MNASNet-B1)</a></td><td>24.6</td><td></td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">185</td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN x0.7 (MobileNetV2)</a></td><td>23.8</td><td></td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">186</td><td><a href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">MobielNet-v1-SSD-300x300+CGD</a></td><td>21.4</td><td></td><td><a href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Compact Global Descriptor for Neural Networks</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">187</td><td><a href="https://paperswithcode.com/paper/fast-r-cnn">Fast-RCNN</a></td><td>19.7</td><td></td><td><a href="https://paperswithcode.com/paper/fast-r-cnn">Fast R-CNN</a></td><td></td><td></td><td>2015</td><td></td></tr><tr><td align="center">188</td><td><a href="https://paperswithcode.com/paper/mobilenets-efficient-convolutional-neural">MobileNet</a></td><td>19.3</td><td></td><td><a href="https://paperswithcode.com/paper/mobilenets-efficient-convolutional-neural">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">189</td><td><a href="https://paperswithcode.com/paper/vision-transformer-with-deformable-attention">DAT-S (RetinaNet)</a></td><td></td><td>69.6</td><td><a href="https://paperswithcode.com/paper/vision-transformer-with-deformable-attention">Vision Transformer with Deformable Attention</a></td><td></td><td></td><td>2022</td><td></td></tr><tr><td align="center">190</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask-VoVNet99 (multi-scale)</a></td><td></td><td>68.3</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">191</td><td><a href="https://paperswithcode.com/paper/190807919">Mask R-CNN (HRNetV2p-W32 + cascade)</a></td><td></td><td>62.5</td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">192</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td><td></td><td>61.9</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">193</td><td><a href="https://paperswithcode.com/paper/virtex-learning-visual-representations-from">VirTex Mask R-CNN (ResNet-50-FPN)</a></td><td></td><td>61.7</td><td><a href="https://paperswithcode.com/paper/virtex-learning-visual-representations-from">VirTex: Learning Visual Representations from Textual Annotations</a></td><td></td><td></td><td>2020</td><td><strong>FPN</strong>;<br><strong>ResNet</strong></td></tr><tr><td align="center">194</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">Centermask + ResNet101</a></td><td></td><td>61.6</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">195</td><td><a href="https://paperswithcode.com/paper/pafnet-an-efficient-anchor-free-object">PAFNet (ResNet50-vd)</a></td><td></td><td>59.8</td><td><a href="https://paperswithcode.com/paper/pafnet-an-efficient-anchor-free-object">PAFNet: An Efficient Anchor-Free Object Detector Guidance</a></td><td></td><td></td><td>2021</td><td><strong>ResNet</strong></td></tr><tr><td align="center">196</td><td><a href="https://paperswithcode.com/paper/dctd-deep-conditional-target-densities-for">IoU-Net+EnergyRegression</a></td><td></td><td>58.5</td><td><a href="https://paperswithcode.com/paper/dctd-deep-conditional-target-densities-for">Energy-Based Models for Deep Probabilistic Regression</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">197</td><td><a href="https://paperswithcode.com/paper/190807919">Cascade R-CNN (HRNetV2p-W48)</a></td><td></td><td></td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">198</td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet50-FPN-3x, single-scale)</a></td><td></td><td></td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">199</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td><td></td><td></td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">200</td><td><a href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet-D7x (single-scale)</a></td><td></td><td></td><td><a href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet: Scalable and Efficient Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr></tbody></table><h2 id="AP75"><a href="#AP75" class="headerlink" title="AP75"></a>AP75</h2><p><img src="/blog/blog/2022/02/16/ji-yu-coco-shu-ju-ji-yan-zheng-de-mu-biao-jian-ce-suan-fa-tian-ti-pai-xing-bang/%E5%9F%BA%E4%BA%8ECOCO%E6%95%B0%E6%8D%AE%E9%9B%86%E9%AA%8C%E8%AF%81%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E5%A4%A9%E6%A2%AF%E6%8E%92%E8%A1%8C%E6%A6%9C/1571518-20220216180708632-632288615.jpg"></p><table><thead><tr><th align="center">Rank</th><th>Model</th><th><strong>box AP</strong></th><th>AP75</th><th>Paper</th><th>Code</th><th>Result</th><th>Year</th><th>Tags</th></tr></thead><tbody><tr><td align="center">1</td><td><a href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and">SwinV2-G (HTC++)</a></td><td>63.1</td><td></td><td><a href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and">Swin Transformer V2: Scaling Up Capacity and Resolution</a></td><td><a href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and#code">Link</a></td><td></td><td>2021</td><td><strong>Swin-Transformer</strong></td></tr><tr><td align="center">2</td><td><a href="https://paperswithcode.com/paper/florence-a-new-foundation-model-for-computer">Florence-CoSwin-H</a></td><td>62.4</td><td></td><td><a href="https://paperswithcode.com/paper/florence-a-new-foundation-model-for-computer">Florence: A New Foundation Model for Computer Vision</a></td><td></td><td></td><td>2021</td><td><strong>Swin-Transformer</strong></td></tr><tr><td align="center">3</td><td><a href="https://paperswithcode.com/paper/grounded-language-image-pre-training">GLIP (Swin-L, multi-scale)</a></td><td>61.5</td><td>67.7</td><td><a href="https://paperswithcode.com/paper/grounded-language-image-pre-training">Grounded Language-Image Pre-training</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong>;<br>Vision Language**;<br><strong>Dynamic Head</strong>;<br>**BERT-Base</td></tr><tr><td align="center">4</td><td><a href="https://paperswithcode.com/paper/end-to-end-semi-supervised-object-detection">Soft Teacher + Swin-L (HTC++, multi-scale)</a></td><td>61.3</td><td></td><td><a href="https://paperswithcode.com/paper/end-to-end-semi-supervised-object-detection">End-to-End Semi-Supervised Object Detection with Soft Teacher</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong>;<br>Swin-Transformer</td></tr><tr><td align="center">5</td><td><a href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">DyHead (Swin-L, multi scale, self-training)</a></td><td>60.6</td><td>66.6</td><td><a href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">Dynamic Head: Unifying Object Detection Heads with Attentions</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong>;<br>Swin-Transformer</td></tr><tr><td align="center">6</td><td><a href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">Dual-Swin-L (HTC, multi-scale)</a></td><td>60.1</td><td></td><td><a href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">CBNetV2: A Composite Backbone Network Architecture for Object Detection</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br>Swin-Transformer</td></tr><tr><td align="center">7</td><td><a href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">Dual-Swin-L (HTC, single-scale)</a></td><td>59.4</td><td></td><td><a href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">CBNetV2: A Composite Backbone Network Architecture for Object Detection</a></td><td></td><td></td><td>2021</td><td><strong>Swin-Transformer</strong></td></tr><tr><td align="center">8</td><td><a href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal-L (DyHead, multi-scale)</a></td><td>58.9</td><td></td><td><a href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal Self-attention for Local-Global Interactions in Vision Transformers</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br>Focal-Transformer</td></tr><tr><td align="center">9</td><td><a href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">DyHead (Swin-L, multi scale)</a></td><td>58.7</td><td>64.5</td><td><a href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">Dynamic Head: Unifying Object Detection Heads with Attentions</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br>Swin-Transformer</td></tr><tr><td align="center">10</td><td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin-L (HTC++, multi scale)</a></td><td>58.7</td><td></td><td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br><strong>Swin-Transformer</strong></td></tr><tr><td align="center">11</td><td><a href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal-L (HTC++, multi-scale)</a></td><td>58.4</td><td></td><td><a href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal Self-attention for Local-Global Interactions in Vision Transformers</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong></td></tr><tr><td align="center">12</td><td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin-L (HTC++, single scale)</a></td><td>57.7</td><td></td><td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>Swin-Transformer</strong></td></tr><tr><td align="center">13</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-D6 (1280, single-scale, 34 fps)</a></td><td>57.3</td><td>62.7</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br>YOLO</td></tr><tr><td align="center">14</td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (Swin-L, single)</a></td><td>56.5</td><td></td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td><td></td><td></td><td>2021</td><td><strong>Transformer</strong><br><strong>single scale</strong></td></tr><tr><td align="center">15</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-E6 (1280, single-scale, 45 fps)</a></td><td>56.4</td><td>61.6</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">16</td><td><a href="https://paperswithcode.com/paper/probabilistic-two-stage-detection">CenterNet2 (Res2Net-101-DCN-BiFPN, self-training, 1560 single-scale)</a></td><td>56.4</td><td>61.6</td><td><a href="https://paperswithcode.com/paper/probabilistic-two-stage-detection">Probabilistic two-stage detection</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>FPN</strong><br><strong>DCN</strong></td></tr><tr><td align="center">17</td><td><a href="https://paperswithcode.com/paper/queryinst-parallelly-supervised-mask-query">QueryInst (single-scale)</a></td><td>56.1</td><td>61.9</td><td><a href="https://paperswithcode.com/paper/queryinst-parallelly-supervised-mask-query">Instances as Queries</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">18</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P7 with TTA</a></td><td>55.8</td><td>61.2</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">19</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-64x4d, multi-scale)</a></td><td>55.7</td><td>61.1</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">20</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-W6 (1280, single-scale, 66 fps)</a></td><td>55.5</td><td>60.6</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">21</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P7 CSP-P7 (single-scale, 16 fps)</a></td><td>55.4</td><td>60.7</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">22</td><td><a href="https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural">CSP-p6 + Mish (multi-scale)</a></td><td>55.2</td><td>60.5</td><td><a href="https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural">Mish: A Self Regularized Non-Monotonic Activation Function</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">23</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P6 with TTA</a></td><td>54.9</td><td>60.2</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">24</td><td><a href="https://paperswithcode.com/paper/simple-copy-paste-is-a-strong-data">Cascade Eff-B7 NAS-FPN (1280)</a></td><td>54.8</td><td></td><td><a href="https://paperswithcode.com/paper/simple-copy-paste-is-a-strong-data">Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>NAS-FPN</strong></td></tr><tr><td align="center">25</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-32x4d, multi-scale)</a></td><td>54.7</td><td>60.1</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">26</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P6 CSP-P6 (single-scale, 32 fps)</a></td><td>54.3</td><td>59.5</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">27</td><td><a href="https://paperswithcode.com/paper/rethinking-pre-training-and-self-training">SpineNet-190 (1280, with Self-training on OpenImages, single-scale)</a></td><td>54.3</td><td></td><td><a href="https://paperswithcode.com/paper/rethinking-pre-training-and-self-training">Rethinking Pre-training and Self-training</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong></td></tr><tr><td align="center">28</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08d (Res2Net-101, DCN, multi-scale)</a></td><td>54.1</td><td>59.9</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">29</td><td><a href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet-D7 (single-scale)</a></td><td>53.7</td><td></td><td><a href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet: Scalable and Efficient Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">30</td><td><a href="https://paperswithcode.com/paper/probabilistic-anchor-assignment-with-iou">PAA (ResNext-152-32x8d + DCN, multi-scale)</a></td><td>53.5</td><td>59.1</td><td><a href="https://paperswithcode.com/paper/probabilistic-anchor-assignment-with-iou">Probabilistic Anchor Assignment with IoU Prediction for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">31</td><td><a href="https://paperswithcode.com/paper/location-sensitive-visual-recognition-with">LSNet (Res2Net-101+ DCN, multi-scale)</a></td><td>53.5</td><td>59.2</td><td><a href="https://paperswithcode.com/paper/location-sensitive-visual-recognition-with">Location-Sensitive Visual Recognition with Cross-IOU Loss</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">32</td><td><a href="https://paperswithcode.com/paper/resnest-split-attention-networks">ResNeSt-200 (multi-scale)</a></td><td>53.3</td><td>58.0</td><td><a href="https://paperswithcode.com/paper/resnest-split-attention-networks">ResNeSt: Split-Attention Networks</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong></td></tr><tr><td align="center">33</td><td><a href="https://paperswithcode.com/paper/cbnet-a-novel-composite-backbone-network">Cascade Mask R-CNN (Triple-ResNeXt152, multi-scale)</a></td><td>53.3</td><td>58.5</td><td><a href="https://paperswithcode.com/paper/cbnet-a-novel-composite-backbone-network">CBNet: A Novel Composite Backbone Network Architecture for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">34</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-32x4d, single-scale)</a></td><td>53.3</td><td>58.5</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>single scale</strong></td></tr><tr><td align="center">35</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (Res2Net-101, DCN, multiscale)</a></td><td>53.3</td><td>59.2</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">36</td><td><a href="https://paperswithcode.com/paper/relationnet-bridging-visual-representations">RelationNet++ (ResNeXt-64x4d-101-DCN)</a></td><td>52.7</td><td></td><td><a href="https://paperswithcode.com/paper/relationnet-bridging-visual-representations">RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">37</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P5 with TTA</a></td><td>52.5</td><td>58</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">38</td><td><a href="https://paperswithcode.com/paper/deformable-detr-deformable-transformers-for-1">Deformable DETR (ResNeXt-101+DCN)</a></td><td>52.3</td><td>58.1</td><td><a href="https://paperswithcode.com/paper/deformable-detr-deformable-transformers-for-1">Deformable DETR: Deformable Transformers for End-to-End Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">39</td><td><a href="https://paperswithcode.com/paper/global-context-networks">GCNet (ResNeXt-101 + DCN + cascade + GC r4)</a></td><td>52.3</td><td>56.9</td><td><a href="https://paperswithcode.com/paper/global-context-networks">Global Context Networks</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong><br><strong>GCN</strong></td></tr><tr><td align="center">40</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-190, 1280x1280)</a></td><td>52.1</td><td>56.5</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">41</td><td><a href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints v2 (ResNeXt-101, DCN, multi-scale)</a></td><td>52.1</td><td>57.5</td><td><a href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints V2: Verification Meets Regression for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong>;<br><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td align="center"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td align="center">42</td><td><a href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">AC-FPN Cascade R-CNN (X-152-32x8d-FPN-IN5k, multi scale, only CEM)</a></td><td>51.9</td><td>57</td><td><a href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">Attention-guided Context Feature Pyramid Network for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>FPN</strong></td></tr><tr><td align="center">43</td><td><a href="https://paperswithcode.com/paper/ota-optimal-transport-assignment-for-object">OTA (ResNeXt-101+DCN, multiscale)</a></td><td>51.5</td><td>57.1</td><td><a href="https://paperswithcode.com/paper/ota-optimal-transport-assignment-for-object">OTA: Optimal Transport Assignment for Object Detection</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">44</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08d (Res2Net-101, DCN, single-scale)</a></td><td>51.3</td><td>55.8</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">45</td><td><a href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">TSD (SENet154-DCN,multi-scale)</a></td><td>51.2</td><td>56.0</td><td><a href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">Revisiting the Sibling Head in Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">46</td><td><a href="https://paperswithcode.com/paper/yolox-exceeding-yolo-series-in-2021">YOLOX-X (Modified CSP v5)</a></td><td>51.2</td><td>55.7</td><td><a href="https://paperswithcode.com/paper/yolox-exceeding-yolo-series-in-2021">YOLOX: Exceeding YOLO Series in 2021</a></td><td></td><td></td><td>2021</td><td><strong>YOLO</strong></td></tr><tr><td align="center">47</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-143, 1280x1280)</a></td><td>50.7</td><td>54.9</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">48</td><td><a href="https://paperswithcode.com/paper/bridging-the-gap-between-anchor-based-and">ATSS (ResNetXt-64x4d-101+DCN,multi-scale)</a></td><td>50.7</td><td>56.3</td><td><a href="https://paperswithcode.com/paper/bridging-the-gap-between-anchor-based-and">Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">49</td><td><a href="https://paperswithcode.com/paper/learning-data-augmentation-strategies-for">NAS-FPN (AmoebaNet-D, learned aug)</a></td><td>50.7</td><td></td><td><a href="https://paperswithcode.com/paper/learning-data-augmentation-strategies-for">Learning Data Augmentation Strategies for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">50</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (Res2Net-101, DCN)</a></td><td>50.6</td><td>55.3</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>DCN</strong></td></tr><tr><td align="center">51</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, DCN, multiscale test)</a></td><td>50.2</td><td>53.9</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">52</td><td><a href="https://paperswithcode.com/paper/scale-equalizing-pyramid-convolution-for">FreeAnchor + SEPC (DCN, ResNext-101-64x4d)</a></td><td>50.1</td><td>54.3</td><td><a href="https://paperswithcode.com/paper/scale-equalizing-pyramid-convolution-for">Scale-Equalizing Pyramid Convolution for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">53</td><td><a href="https://paperswithcode.com/paper/d2det-towards-high-quality-object-detection">D2Det (ResNet-101-DCN, multi-scale test)</a></td><td>50.1</td><td>54.9</td><td><a href="https://paperswithcode.com/paper/d2det-towards-high-quality-object-detection">D2Det: Towards High Quality Object Detection and Instance Segmentation</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">54</td><td><a href="https://paperswithcode.com/paper/dynamic-r-cnn-towards-high-quality-object">Dynamic R-CNN (ResNet-101-DCN, multi-scale)</a></td><td>50.1</td><td>55.6</td><td><a href="https://paperswithcode.com/paper/dynamic-r-cnn-towards-high-quality-object">Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">55</td><td><a href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">TSD (ResNet-101-Deformable, Image Pyramid)</a></td><td>49.4</td><td>54.4</td><td><a href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">Revisiting the Sibling Head in Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>ResNet</strong></td></tr><tr><td align="center">56</td><td><a href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints v2 (ResNeXt-101, DCN)</a></td><td>49.4</td><td>53.4</td><td><a href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints V2: Verification Meets Regression for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">57</td><td><a href="https://paperswithcode.com/paper/corner-proposal-network-for-anchor-free-two">CPNDet (Hourglass-104, multi-scale)</a></td><td>49.2</td><td>53.7</td><td><a href="https://paperswithcode.com/paper/corner-proposal-network-for-anchor-free-two">Corner Proposal Network for Anchor-free, Two-stage Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong></td></tr><tr><td align="center">58</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNeXt-101, 32x4d, DCN)</a></td><td>49</td><td>53.5</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">59</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, DCN, single scale)</a></td><td>48.9</td><td>52.5</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>single scale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">60</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08 (Res2Net-50, DCN, single-scale)</a></td><td>48.8</td><td>53.0</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">61</td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (ResNet101, single scale)</a></td><td>48.7</td><td></td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td><td></td><td></td><td>2021</td><td><strong>Transformer</strong><br><strong>single scale</strong></td></tr><tr><td align="center">62</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-96, 1024x1024)</a></td><td>48.6</td><td>52.5</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">63</td><td><a href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">TridentNet (ResNet-101-Deformable, Image Pyramid)</a></td><td>48.4</td><td>53.5</td><td><a href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">Scale-Aware Trident Networks for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">64</td><td><a href="https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze">GCNet (ResNeXt-101 + DCN + cascade + GC r4)</a></td><td>48.4</td><td>52.7</td><td><a href="https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>DCN</strong><br><strong>GCN</strong></td></tr><tr><td align="center">65</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-101-DCN)</a></td><td>48.3</td><td>52.8</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">66</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-learning-qualified-and">GFL (X-101-32x4d-DCN, single-scale)</a></td><td>48.2</td><td>52.6</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-learning-qualified-and">Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>single scale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">67</td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet101-FPN-3x, single-scale)</a></td><td>48.1</td><td></td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">68</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, single scale)</a></td><td>47.8</td><td>51.1</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>single scale</strong></td></tr><tr><td align="center">69</td><td><a href="https://paperswithcode.com/paper/matrix-nets-a-new-deep-architecture-for">MatrixNet Corners (ResNet-152, multi-scale)</a></td><td>47.8</td><td>52.3</td><td><a href="https://paperswithcode.com/paper/matrix-nets-a-new-deep-architecture-for">Matrix Nets: A New Deep Architecture for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">70</td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (ResNet50, single scale)</a></td><td>47.8</td><td></td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td><td></td><td></td><td>2021</td><td><strong>Transformer</strong><br><strong>single scale</strong></td></tr><tr><td align="center">71</td><td><a href="https://paperswithcode.com/paper/soft-anchor-point-object-detection">SAPD (ResNeXt-101, single-scale)</a></td><td>47.4</td><td>51.1</td><td><a href="https://paperswithcode.com/paper/soft-anchor-point-object-detection">Soft Anchor-Point Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>single scale</strong></td></tr><tr><td align="center">72</td><td><a href="https://paperswithcode.com/paper/path-aggregation-network-for-instance">PANet (ResNeXt-101, multi-scale)</a></td><td>47.4</td><td>51.8</td><td><a href="https://paperswithcode.com/paper/path-aggregation-network-for-instance">Path Aggregation Network for Instance Segmentation</a></td><td></td><td></td><td>2018</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">73</td><td><a href="https://paperswithcode.com/paper/190807919">HTC (HRNetV2p-W48)</a></td><td>47.3</td><td>51.2</td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">74</td><td><a href="https://paperswithcode.com/paper/hybrid-task-cascade-for-instance-segmentation">HTC (ResNeXt-101-FPN)</a></td><td>47.1</td><td>44.7</td><td><a href="https://paperswithcode.com/paper/hybrid-task-cascade-for-instance-segmentation">Hybrid Task Cascade for Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">75</td><td><a href="https://paperswithcode.com/paper/centernet-object-detection-with-keypoint">CenterNet511 (Hourglass-104, multi-scale)</a></td><td>47.0</td><td>50.7</td><td><a href="https://paperswithcode.com/paper/centernet-object-detection-with-keypoint">CenterNet: Keypoint Triplets for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">76</td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNeXt101, multi-scale)</a></td><td>47.0</td><td></td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">77</td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet50-FPN-3x)</a></td><td>46.8</td><td></td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td><td></td><td></td><td>2021</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">78</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49, 896x896)</a></td><td>46.7</td><td>50.6</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">79</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101-DCN, multi-scale)</a></td><td>46.5</td><td>50.9</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">80</td><td><a href="https://paperswithcode.com/paper/houghnet-integrating-near-and-long-range">HoughNet (MS)</a></td><td>46.4</td><td>50.7</td><td><a href="https://paperswithcode.com/paper/houghnet-integrating-near-and-long-range">HoughNet: Integrating near and long-range evidence for bottom-up object detection</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong></td></tr><tr><td align="center">81</td><td><a href="https://paperswithcode.com/paper/reducing-label-noise-in-anchor-free-object">PPDet (ResNeXt-101-FPN, multiscale)</a></td><td>46.3</td><td>51.6</td><td><a href="https://paperswithcode.com/paper/reducing-label-noise-in-anchor-free-object">Reducing Label Noise in Anchor-Free Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>FPN</strong></td></tr><tr><td align="center">82</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-101)</a></td><td>46.2</td><td>50.5</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNet</strong></td></tr><tr><td align="center">83</td><td><a href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER (ResNet-101)</a></td><td>46.1</td><td>51.6</td><td><a href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER: Efficient Multi-Scale Training</a></td><td></td><td></td><td>2018</td><td><strong>ResNet</strong></td></tr><tr><td align="center">84</td><td><a href="https://paperswithcode.com/paper/190807919">Mask R-CNN (HRNetV2p-W48 + cascade)</a></td><td>46.1</td><td>50.3</td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">85</td><td><a href="https://paperswithcode.com/paper/deformable-convnets-v2-more-deformable-better">DCNv2 (ResNet-101, multi-scale)</a></td><td>46.0</td><td>50.8</td><td><a href="https://paperswithcode.com/paper/deformable-convnets-v2-more-deformable-better">Deformable ConvNets v2: More Deformable, Better Results</a></td><td></td><td></td><td>2018</td><td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">86</td><td><a href="https://paperswithcode.com/paper/localization-uncertainty-estimation-for">Gaussian-FCOS</a></td><td>46</td><td></td><td><a href="https://paperswithcode.com/paper/localization-uncertainty-estimation-for">Localization Uncertainty Estimation for Anchor-Free Object Detection</a></td><td></td><td></td><td>2020</td><td></td></tr><tr><td align="center">87</td><td><a href="https://paperswithcode.com/paper/instaboost-boosting-instance-segmentation-via">Cascade R-CNN-FPN (ResNet-101, map-guided)</a></td><td>45.9</td><td>50</td><td><a href="https://paperswithcode.com/paper/instaboost-boosting-instance-segmentation-via">InstaBoost: Boosting Instance Segmentation via Probability Map Guided Copy-Pasting</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">88</td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNeXt101, single-scale)</a></td><td>45.9</td><td></td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>single scale</strong></td></tr><tr><td align="center">89</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask+VoVNetV2-99 (single-scale)</a></td><td>45.8</td><td></td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">90</td><td><a href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">D-RFCN + SNIP (DPN-98 with flip, multi-scale)</a></td><td>45.7</td><td>51.1</td><td><a href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">An Analysis of Scale Invariance in Object Detection - SNIP</a></td><td></td><td></td><td>2017</td><td><strong>multiscale</strong></td></tr><tr><td align="center">91</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4 (CD53)</a></td><td>45.5</td><td>49.5</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">92</td><td><a href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO (608x608)</a></td><td>45.2</td><td>49.9</td><td><a href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>YOLO</strong></td></tr><tr><td align="center">93</td><td><a href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">AC-FPN Cascade R-CNN (ResNet-101, single scale)</a></td><td>45</td><td>49</td><td><a href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">Attention-guided Context Feature Pyramid Network for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong><br><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">94</td><td><a href="https://paperswithcode.com/paper/freeanchor-learning-to-match-anchors-for">FreeAnchor (ResNeXt-101)</a></td><td>44.8</td><td>48.4</td><td><a href="https://paperswithcode.com/paper/freeanchor-learning-to-match-anchors-for">FreeAnchor: Learning to Match Anchors for Visual Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">95</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-64x4d-101-FPN 4 + improvements)</a></td><td>44.7</td><td>48.4</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">96</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask+VoVNet2-57 (single-scale)</a></td><td>44.7</td><td>48.6</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">97</td><td><a href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">FSAF (ResNeXt-101, multi-scale)</a></td><td>44.6</td><td>48.6</td><td><a href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">98</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101, DCN, 500 scale)</a></td><td>44.6</td><td>47.5</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">99</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask + X-101-32x8d (single-scale)</a></td><td>44.6</td><td>48.4</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">100</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49, 640x640)</a></td><td>44.3</td><td>47.6</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">101</td><td><a href="https://paperswithcode.com/paper/you-only-look-one-level-feature">YOLOF-DC5</a></td><td>44.3</td><td>47.5</td><td><a href="https://paperswithcode.com/paper/you-only-look-one-level-feature">You Only Look One-level Feature</a></td><td></td><td></td><td>2021</td><td><strong>YOLO</strong></td></tr><tr><td align="center">102</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-50)</a></td><td>44.3</td><td>48.5</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNet</strong></td></tr><tr><td align="center">103</td><td><a href="https://paperswithcode.com/paper/feature-intertwiner-for-object-detection-1">InterNet (ResNet-101-FPN, multi-scale)</a></td><td>44.2</td><td>51.1</td><td><a href="https://paperswithcode.com/paper/feature-intertwiner-for-object-detection-1">Feature Intertwiner for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong><br><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">104</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (VGG-16, multi-scale)</a></td><td>44.2</td><td>49.3</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td><td></td><td></td><td>2018</td><td><strong>multiscale</strong></td></tr><tr><td align="center">105</td><td><a href="https://paperswithcode.com/paper/lip-local-importance-based-pooling">Faster R-CNN (LIP-ResNet-101-MD w FPN)</a></td><td>43.9</td><td>48.1</td><td><a href="https://paperswithcode.com/paper/lip-local-importance-based-pooling">LIP: Local Importance-based Pooling</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">106</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (ResNet-101, multi-scale)</a></td><td>43.9</td><td>48</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td><td></td><td></td><td>2018</td><td><strong>multiscale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">107</td><td><a href="https://paperswithcode.com/paper/learning-spatial-fusion-for-single-shot">YOLOv3 @800 + ASFF* (Darknet-53)</a></td><td>43.9</td><td>49.2</td><td><a href="https://paperswithcode.com/paper/learning-spatial-fusion-for-single-shot">Learning Spatial Fusion for Single-Shot Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>YOLO</strong></td></tr><tr><td align="center">108</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td><td>43.9</td><td>47.7</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">109</td><td><a href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">ExtremeNet (Hourglass-104, multi-scale)</a></td><td>43.7</td><td>47.0</td><td><a href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">Bottom-up Object Detection by Grouping Extreme and Center Points</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">110</td><td><a href="https://paperswithcode.com/paper/yolov4-optimal-speed-and-accuracy-of-object">YOLOv4-608</a></td><td>43.5</td><td>47.3</td><td><a href="https://paperswithcode.com/paper/yolov4-optimal-speed-and-accuracy-of-object">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">111</td><td><a href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER (ResNet-50)</a></td><td>43.5</td><td>48.6</td><td><a href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER: Efficient Multi-Scale Training</a></td><td></td><td></td><td>2018</td><td><strong>ResNet</strong></td></tr><tr><td align="center">112</td><td><a href="https://paperswithcode.com/paper/190807919">CenterNet (HRNetV2-W48)</a></td><td>43.5</td><td>46.5</td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">113</td><td><a href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">D-RFCN + SNIP (ResNet-101, multi-scale)</a></td><td>43.4</td><td>48.4</td><td><a href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">An Analysis of Scale Invariance in Object Detection - SNIP</a></td><td></td><td></td><td>2017</td><td><strong>multiscale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">114</td><td><a href="https://paperswithcode.com/paper/grid-r-cnn">Grid R-CNN (ResNeXt-101-FPN)</a></td><td>43.2</td><td>46.6</td><td><a href="https://paperswithcode.com/paper/grid-r-cnn">Grid R-CNN</a></td><td></td><td></td><td>2018</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">115</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-101-64x4d-FPN)</a></td><td>43.2</td><td>46.6</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">116</td><td><a href="https://paperswithcode.com/paper/190408900">CornerNet-Saccade (Hourglass-104, multi-scale)</a></td><td>43.2</td><td></td><td><a href="https://paperswithcode.com/paper/190408900">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">117</td><td><a href="https://paperswithcode.com/paper/libra-r-cnn-towards-balanced-learning-for">Libra R-CNN (ResNeXt-101-FPN)</a></td><td>43.0</td><td>47</td><td><a href="https://paperswithcode.com/paper/libra-r-cnn-towards-balanced-learning-for">Libra R-CNN: Towards Balanced Learning for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">118</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101-DCN)</a></td><td>42.8</td><td>46.3</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">119</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet-49 (640, RetinaNet, single-scale)</a></td><td>42.8</td><td>46.1</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">120</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-101-FPN+, cascade)</a></td><td>42.8</td><td>46.3</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">121</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-high-quality-object-detection">Cascade R-CNN</a></td><td>42.8</td><td>46.3</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-high-quality-object-detection">Cascade R-CNN: High Quality Object Detection and Instance Segmentation</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">122</td><td><a href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">TridentNet (ResNet-101)</a></td><td>42.7</td><td>46.5</td><td><a href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">Scale-Aware Trident Networks for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">123</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-32x8d-101-FPN)</a></td><td>42.7</td><td>46.1</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">124</td><td><a href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask (ResNeXt-101-FPN-GN)</a></td><td>42.6</td><td>46.0</td><td><a href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">125</td><td><a href="https://paperswithcode.com/paper/tood-task-aligned-one-stage-object-detection">TAL + TAP</a></td><td>42.5</td><td>46.4</td><td><a href="https://paperswithcode.com/paper/tood-task-aligned-one-stage-object-detection">TOOD: Task-aligned One-stage Object Detection</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">126</td><td><a href="https://paperswithcode.com/paper/190807919">Faster R-CNN (HRNetV2p-W48)</a></td><td>42.4</td><td>46.4</td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">127</td><td><a href="https://paperswithcode.com/paper/hierarchical-shot-detector">HSD (Rest101, 768x768, single-scale test)</a></td><td>42.3</td><td>46.9</td><td><a href="https://paperswithcode.com/paper/hierarchical-shot-detector">Hierarchical Shot Detector</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">128</td><td><a href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet511 (Hourglass-104, multi-scale)</a></td><td>42.1</td><td>45.3</td><td><a href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet: Detecting Objects as Paired Keypoints</a></td><td></td><td></td><td>2018</td><td><strong>multiscale</strong></td></tr><tr><td align="center">129</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td><td>42.1</td><td></td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">130</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (HRNet-W32-5l)</a></td><td>42.0</td><td>45.3</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">131</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512+ (ResNet-101)</a></td><td>41.8</td><td>45.7</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>ResNet</strong></td></tr><tr><td align="center">132</td><td><a href="https://paperswithcode.com/paper/gradient-harmonized-single-stage-detector">GHM-C + GHM-R (RetinaNet-FPN-ResNeXt-101)</a></td><td>41.6</td><td>44.2</td><td><a href="https://paperswithcode.com/paper/gradient-harmonized-single-stage-detector">Gradient Harmonized Single-stage Detector</a></td><td></td><td></td><td>2018</td><td><strong>FPN</strong></td></tr><tr><td align="center">133</td><td><a href="https://paperswithcode.com/paper/objects-as-points">CenterNet-DLA (DLA-34, multi-scale)</a></td><td>41.6</td><td></td><td><a href="https://paperswithcode.com/paper/objects-as-points">Objects as Points</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">134</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49S, 640x640)</a></td><td>41.5</td><td>44.6</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">135</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101)</a></td><td>41</td><td>44.3</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">136</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (VGG-16, single-scale)</a></td><td>41.0</td><td>45</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td><td></td><td></td><td>2018</td><td><strong>single scale</strong></td></tr><tr><td align="center">137</td><td><a href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">FSAF (ResNet-101, single-scale)</a></td><td>40.9</td><td>44</td><td><a href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">138</td><td><a href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">RetinaNet (ResNeXt-101-FPN)</a></td><td>40.8</td><td>44.1</td><td><a href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">Focal Loss for Dense Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">139</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-50-FPN+, cascade)</a></td><td>40.6</td><td>44</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">140</td><td><a href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Faster R-CNN (Cascade RPN)</a></td><td>40.6</td><td>44.5</td><td><a href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">141</td><td><a href="https://paperswithcode.com/paper/deformable-kernels-adapting-effective">ResNet-50-DW-DPN (Deformable Kernels)</a></td><td>40.6</td><td></td><td><a href="https://paperswithcode.com/paper/deformable-kernels-adapting-effective">Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">142</td><td><a href="https://paperswithcode.com/paper/acquisition-of-localization-confidence-for">IoU-Net</a></td><td>40.6</td><td></td><td><a href="https://paperswithcode.com/paper/acquisition-of-localization-confidence-for">Acquisition of Localization Confidence for Accurate Object Detection</a></td><td></td><td></td><td>2018</td><td></td></tr><tr><td align="center">143</td><td><a href="https://paperswithcode.com/paper/190807919">FCOS (HRNetV2p-W48)</a></td><td>40.5</td><td></td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">144</td><td><a href="https://paperswithcode.com/paper/softer-nms-rethinking-bounding-box-regression">ResNet-50-FPN Mask R-CNN + KL Loss + var voting + soft-NMS</a></td><td>40.4</td><td></td><td><a href="https://paperswithcode.com/paper/softer-nms-rethinking-bounding-box-regression">Bounding Box Regression with Uncertainty for Accurate Object Detection</a></td><td></td><td></td><td>2018</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">145</td><td><a href="https://paperswithcode.com/paper/rdsnet-a-new-deep-architecture-for-reciprocal">RDSNet (ResNet-101, RetinaNet, mask, MBRM)</a></td><td>40.3</td><td>43</td><td><a href="https://paperswithcode.com/paper/rdsnet-a-new-deep-architecture-for-reciprocal">RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">146</td><td><a href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">ExtremeNet (Hourglass-104, single-scale)</a></td><td>40.2</td><td>43.2</td><td><a href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">Bottom-up Object Detection by Grouping Extreme and Center Points</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">147</td><td><a href="https://paperswithcode.com/paper/cross-iteration-batch-normalization">Mask R-CNN (ResNet-101-FPN, CBN)</a></td><td>40.1</td><td>44.1</td><td><a href="https://paperswithcode.com/paper/cross-iteration-batch-normalization">Cross-Iteration Batch Normalization</a></td><td></td><td></td><td>2020</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">148</td><td><a href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Fast R-CNN (Cascade RPN)</a></td><td>40.1</td><td>43.8</td><td><a href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">149</td><td><a href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN (ResNeXt-101-FPN)</a></td><td>39.8</td><td>43.4</td><td><a href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN</a></td><td></td><td></td><td>2017</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">150</td><td><a href="https://paperswithcode.com/paper/region-proposal-by-guided-anchoring">GA-Faster-RCNN</a></td><td>39.8</td><td>43.5</td><td><a href="https://paperswithcode.com/paper/region-proposal-by-guided-anchoring">Region Proposal by Guided Anchoring</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">151</td><td><a href="https://paperswithcode.com/paper/chainercv-a-library-for-deep-learning-in">FPN (ResNet101 backbone)</a></td><td>39.5</td><td></td><td><a href="https://paperswithcode.com/paper/chainercv-a-library-for-deep-learning-in">ChainerCV: a Library for Deep Learning in Computer Vision</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">152</td><td><a href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask (ResNet-50-FPN)</a></td><td>39.4</td><td>42.3</td><td><a href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">153</td><td><a href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO (320x320)</a></td><td>39.3</td><td>42.7</td><td><a href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>YOLO</strong></td></tr><tr><td align="center">154</td><td><a href="https://paperswithcode.com/paper/190409925">AA-ResNet-10 + RetinaNet</a></td><td>39.2</td><td></td><td><a href="https://paperswithcode.com/paper/190409925">Attention Augmented Convolutional Networks</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">155</td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNet50, single-scale)</a></td><td>39.2</td><td></td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">156</td><td><a href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">RetinaNet (ResNet-101-FPN)</a></td><td>39.1</td><td>42.3</td><td><a href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">Focal Loss for Dense Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">157</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-101-FPN+)</a></td><td>38.8</td><td>41.9</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">158</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (ResNet-101, single-scale)</a></td><td>38.8</td><td>41.7</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td><td></td><td></td><td>2018</td><td><strong>single scale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">159</td><td><a href="https://paperswithcode.com/paper/saccadenet-a-fast-and-accurate-object">SaccadeNet (DLA-34-DCN)</a></td><td>38.5</td><td>41.4</td><td><a href="https://paperswithcode.com/paper/saccadenet-a-fast-and-accurate-object">SaccadeNet: A Fast and Accurate Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>DCN</strong></td></tr><tr><td align="center">160</td><td><a href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN (ResNet-101-FPN)</a></td><td>38.2</td><td>41.7</td><td><a href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">161</td><td><a href="https://paperswithcode.com/paper/segmentation-is-all-you-need">WSMA-Seg</a></td><td>38.1</td><td></td><td><a href="https://paperswithcode.com/paper/segmentation-is-all-you-need">Segmentation is All You Need</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">162</td><td><a href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Faster R-CNN + FPN + CGD</a></td><td>37.9</td><td></td><td><a href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Compact Global Descriptor for Neural Networks</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">163</td><td><a href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet511 (Hourglass-52, single-scale)</a></td><td>37.8</td><td>40.1</td><td><a href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet: Detecting Objects as Paired Keypoints</a></td><td></td><td></td><td>2018</td><td><strong>single scale</strong></td></tr><tr><td align="center">164</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512+ (VGG-16)</a></td><td>37.6</td><td>40.8</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">165</td><td><a href="https://paperswithcode.com/paper/deformable-convolutional-networks">DeformConv-R-FCN (Aligned-Inception-ResNet)</a></td><td>37.5</td><td></td><td><a href="https://paperswithcode.com/paper/deformable-convolutional-networks">Deformable Convolutional Networks</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">166</td><td><a href="https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data">Faster R-CNN (ImageNet+300M)</a></td><td>37.4</td><td>40.1</td><td><a href="https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">167</td><td><a href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">Mask R-CNN (Bottleneck-injected ResNet-50, FPN)</a></td><td>36.9</td><td></td><td><a href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation</a></td><td></td><td></td><td>2020</td><td><strong>FPN</strong><br>！！<strong>ResNet</strong></td></tr><tr><td align="center">168</td><td><a href="https://paperswithcode.com/paper/beyond-skip-connections-top-down-modulation">Faster R-CNN + TDM</a></td><td>36.8</td><td></td><td><a href="https://paperswithcode.com/paper/beyond-skip-connections-top-down-modulation">Beyond Skip Connections: Top-Down Modulation for Object Detection</a></td><td></td><td></td><td>2016</td><td></td></tr><tr><td align="center">169</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-50-FPN+)</a></td><td>36.5</td><td>39.2</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong>;<br><strong>ResNet</strong></td></tr><tr><td align="center">170</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512 (ResNet-101)</a></td><td>36.4</td><td>39.5</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>ResNet</strong></td></tr><tr><td align="center">171</td><td><a href="https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection">Faster R-CNN + FPN</a></td><td>36.2</td><td></td><td><a href="https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection">Feature Pyramid Networks for Object Detection</a></td><td></td><td></td><td>2016</td><td><strong>FPN</strong></td></tr><tr><td align="center">172</td><td><a href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">Faster R-CNN (Bottleneck-injected ResNet-50 and FPN)</a></td><td>35.9</td><td></td><td><a href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation</a></td><td></td><td></td><td>2020</td><td><strong>FPN</strong>;<br><strong>ResNet</strong></td></tr><tr><td align="center">173</td><td><a href="https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition">Faster R-CNN (box refinement, context, multi-scale testing)</a></td><td>34.9</td><td></td><td><a href="https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition">Deep Residual Learning for Image Recognition</a></td><td></td><td></td><td>2015</td><td><strong>multiscale</strong></td></tr><tr><td align="center">174</td><td><a href="https://paperswithcode.com/paper/speedaccuracy-trade-offs-for-modern">Faster R-CNN</a></td><td>34.7</td><td></td><td><a href="https://paperswithcode.com/paper/speedaccuracy-trade-offs-for-modern">Speed/accuracy trade-offs for modern convolutional object detectors</a></td><td></td><td></td><td>2016</td><td></td></tr><tr><td align="center">175</td><td><a href="https://paperswithcode.com/paper/190408900">CornerNet-Squeeze</a></td><td>34.4</td><td></td><td><a href="https://paperswithcode.com/paper/190408900">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">176</td><td><a href="https://paperswithcode.com/paper/a-multipath-network-for-object-detection">MultiPath Network</a></td><td>33.2</td><td></td><td><a href="https://paperswithcode.com/paper/a-multipath-network-for-object-detection">A MultiPath Network for Object Detection</a></td><td></td><td></td><td>2016</td><td></td></tr><tr><td align="center">177</td><td><a href="https://paperswithcode.com/paper/inside-outside-net-detecting-objects-in">ION</a></td><td>33.1</td><td>34.6</td><td><a href="https://paperswithcode.com/paper/inside-outside-net-detecting-objects-in">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</a></td><td></td><td></td><td>2015</td><td></td></tr><tr><td align="center">178</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512 (VGG-16)</a></td><td>33</td><td>35.5</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">179</td><td><a href="https://paperswithcode.com/paper/yolov3-an-incremental-improvement">YOLOv3 + Darknet-53</a></td><td>33.0</td><td></td><td><a href="https://paperswithcode.com/paper/yolov3-an-incremental-improvement">YOLOv3: An Incremental Improvement</a></td><td></td><td></td><td>2018</td><td><strong>YOLO</strong></td></tr><tr><td align="center">180</td><td><a href="https://paperswithcode.com/paper/ssd-single-shot-multibox-detector">SSD512</a></td><td>28.8</td><td>30.3</td><td><a href="https://paperswithcode.com/paper/ssd-single-shot-multibox-detector">SSD: Single Shot MultiBox Detector</a></td><td></td><td></td><td>2015</td><td></td></tr><tr><td align="center">181</td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MobileNetV2)</a></td><td>26.1</td><td></td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">182</td><td><a href="https://paperswithcode.com/paper/espnetv2-a-light-weight-power-efficient-and">ESPNetv2-512</a></td><td>26.0</td><td></td><td><a href="https://paperswithcode.com/paper/espnetv2-a-light-weight-power-efficient-and">ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network</a></td><td></td><td></td><td>2018</td><td></td></tr><tr><td align="center">183</td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MobileNetV3)</a></td><td>25.5</td><td></td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">184</td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MNASNet-B1)</a></td><td>24.6</td><td></td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">185</td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN x0.7 (MobileNetV2)</a></td><td>23.8</td><td></td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">186</td><td><a href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">MobielNet-v1-SSD-300x300+CGD</a></td><td>21.4</td><td></td><td><a href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Compact Global Descriptor for Neural Networks</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">187</td><td><a href="https://paperswithcode.com/paper/fast-r-cnn">Fast-RCNN</a></td><td>19.7</td><td></td><td><a href="https://paperswithcode.com/paper/fast-r-cnn">Fast R-CNN</a></td><td></td><td></td><td>2015</td><td></td></tr><tr><td align="center">188</td><td><a href="https://paperswithcode.com/paper/mobilenets-efficient-convolutional-neural">MobileNet</a></td><td>19.3</td><td></td><td><a href="https://paperswithcode.com/paper/mobilenets-efficient-convolutional-neural">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">189</td><td><a href="https://paperswithcode.com/paper/vision-transformer-with-deformable-attention">DAT-S (RetinaNet)</a></td><td></td><td>51.2</td><td><a href="https://paperswithcode.com/paper/vision-transformer-with-deformable-attention">Vision Transformer with Deformable Attention</a></td><td></td><td></td><td>2022</td><td></td></tr><tr><td align="center">190</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask-VoVNet99 (multi-scale)</a></td><td></td><td>53.2</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">191</td><td><a href="https://paperswithcode.com/paper/190807919">Mask R-CNN (HRNetV2p-W32 + cascade)</a></td><td></td><td>48.6</td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">192</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td><td></td><td>45.2</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">193</td><td><a href="https://paperswithcode.com/paper/virtex-learning-visual-representations-from">VirTex Mask R-CNN (ResNet-50-FPN)</a></td><td></td><td>44.8</td><td><a href="https://paperswithcode.com/paper/virtex-learning-visual-representations-from">VirTex: Learning Visual Representations from Textual Annotations</a></td><td></td><td></td><td>2020</td><td><strong>FPN</strong>;<br><strong>ResNet</strong></td></tr><tr><td align="center">194</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">Centermask + ResNet101</a></td><td></td><td>46.9</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">195</td><td><a href="https://paperswithcode.com/paper/pafnet-an-efficient-anchor-free-object">PAFNet (ResNet50-vd)</a></td><td></td><td>45.3</td><td><a href="https://paperswithcode.com/paper/pafnet-an-efficient-anchor-free-object">PAFNet: An Efficient Anchor-Free Object Detector Guidance</a></td><td></td><td></td><td>2021</td><td><strong>ResNet</strong></td></tr><tr><td align="center">196</td><td><a href="https://paperswithcode.com/paper/dctd-deep-conditional-target-densities-for">IoU-Net+EnergyRegression</a></td><td></td><td>41.8</td><td><a href="https://paperswithcode.com/paper/dctd-deep-conditional-target-densities-for">Energy-Based Models for Deep Probabilistic Regression</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">197</td><td><a href="https://paperswithcode.com/paper/190807919">Cascade R-CNN (HRNetV2p-W48)</a></td><td></td><td>48.6</td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">198</td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet50-FPN-3x, single-scale)</a></td><td></td><td></td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">199</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td><td></td><td></td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">200</td><td><a href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet-D7x (single-scale)</a></td><td></td><td></td><td><a href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet: Scalable and Efficient Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr></tbody></table><hr><h2 id="APS"><a href="#APS" class="headerlink" title="APS"></a>APS</h2><p><img src="/blog/blog/2022/02/16/ji-yu-coco-shu-ju-ji-yan-zheng-de-mu-biao-jian-ce-suan-fa-tian-ti-pai-xing-bang/%E5%9F%BA%E4%BA%8ECOCO%E6%95%B0%E6%8D%AE%E9%9B%86%E9%AA%8C%E8%AF%81%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E5%A4%A9%E6%A2%AF%E6%8E%92%E8%A1%8C%E6%A6%9C/1571518-20220216181109388-1416537678.jpg"></p><table><thead><tr><th align="center">Rank</th><th>Model</th><th><strong>box AP</strong></th><th>APS</th><th>Paper</th><th>Code</th><th>Result</th><th>Year</th><th>Tags</th></tr></thead><tbody><tr><td align="center">1</td><td><a href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and">SwinV2-G (HTC++)</a></td><td>63.1</td><td></td><td><a href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and">Swin Transformer V2: Scaling Up Capacity and Resolution</a></td><td><a href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and#code">Link</a></td><td></td><td>2021</td><td><strong>Swin-Transformer</strong></td></tr><tr><td align="center">2</td><td><a href="https://paperswithcode.com/paper/florence-a-new-foundation-model-for-computer">Florence-CoSwin-H</a></td><td>62.4</td><td></td><td><a href="https://paperswithcode.com/paper/florence-a-new-foundation-model-for-computer">Florence: A New Foundation Model for Computer Vision</a></td><td></td><td></td><td>2021</td><td><strong>Swin-Transformer</strong></td></tr><tr><td align="center">3</td><td><a href="https://paperswithcode.com/paper/grounded-language-image-pre-training">GLIP (Swin-L, multi-scale)</a></td><td>61.5</td><td>45.3</td><td><a href="https://paperswithcode.com/paper/grounded-language-image-pre-training">Grounded Language-Image Pre-training</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong>;<br>Vision Language**;<br><strong>Dynamic Head</strong>;<br>**BERT-Base</td></tr><tr><td align="center">4</td><td><a href="https://paperswithcode.com/paper/end-to-end-semi-supervised-object-detection">Soft Teacher + Swin-L (HTC++, multi-scale)</a></td><td>61.3</td><td></td><td><a href="https://paperswithcode.com/paper/end-to-end-semi-supervised-object-detection">End-to-End Semi-Supervised Object Detection with Soft Teacher</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong>;<br>Swin-Transformer</td></tr><tr><td align="center">5</td><td><a href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">DyHead (Swin-L, multi scale, self-training)</a></td><td>60.6</td><td></td><td><a href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">Dynamic Head: Unifying Object Detection Heads with Attentions</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong>;<br>Swin-Transformer</td></tr><tr><td align="center">6</td><td><a href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">Dual-Swin-L (HTC, multi-scale)</a></td><td>60.1</td><td></td><td><a href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">CBNetV2: A Composite Backbone Network Architecture for Object Detection</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br>Swin-Transformer</td></tr><tr><td align="center">7</td><td><a href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">Dual-Swin-L (HTC, single-scale)</a></td><td>59.4</td><td></td><td><a href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">CBNetV2: A Composite Backbone Network Architecture for Object Detection</a></td><td></td><td></td><td>2021</td><td><strong>Swin-Transformer</strong></td></tr><tr><td align="center">8</td><td><a href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal-L (DyHead, multi-scale)</a></td><td>58.9</td><td></td><td><a href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal Self-attention for Local-Global Interactions in Vision Transformers</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br>Focal-Transformer</td></tr><tr><td align="center">9</td><td><a href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">DyHead (Swin-L, multi scale)</a></td><td>58.7</td><td>41.7</td><td><a href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">Dynamic Head: Unifying Object Detection Heads with Attentions</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br>Swin-Transformer</td></tr><tr><td align="center">10</td><td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin-L (HTC++, multi scale)</a></td><td>58.7</td><td></td><td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br><strong>Swin-Transformer</strong></td></tr><tr><td align="center">11</td><td><a href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal-L (HTC++, multi-scale)</a></td><td>58.4</td><td></td><td><a href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal Self-attention for Local-Global Interactions in Vision Transformers</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong></td></tr><tr><td align="center">12</td><td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin-L (HTC++, single scale)</a></td><td>57.7</td><td></td><td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>Swin-Transformer</strong></td></tr><tr><td align="center">13</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-D6 (1280, single-scale, 34 fps)</a></td><td>57.3</td><td>40.4</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br>YOLO</td></tr><tr><td align="center">14</td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (Swin-L, single)</a></td><td>56.5</td><td></td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td><td></td><td></td><td>2021</td><td><strong>Transformer</strong><br><strong>single scale</strong></td></tr><tr><td align="center">15</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-E6 (1280, single-scale, 45 fps)</a></td><td>56.4</td><td>39.1</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">16</td><td><a href="https://paperswithcode.com/paper/probabilistic-two-stage-detection">CenterNet2 (Res2Net-101-DCN-BiFPN, self-training, 1560 single-scale)</a></td><td>56.4</td><td>38.7</td><td><a href="https://paperswithcode.com/paper/probabilistic-two-stage-detection">Probabilistic two-stage detection</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>FPN</strong><br><strong>DCN</strong></td></tr><tr><td align="center">17</td><td><a href="https://paperswithcode.com/paper/queryinst-parallelly-supervised-mask-query">QueryInst (single-scale)</a></td><td>56.1</td><td>37.4</td><td><a href="https://paperswithcode.com/paper/queryinst-parallelly-supervised-mask-query">Instances as Queries</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">18</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P7 with TTA</a></td><td>55.8</td><td></td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">19</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-64x4d, multi-scale)</a></td><td>55.7</td><td>37.7</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">20</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-W6 (1280, single-scale, 66 fps)</a></td><td>55.5</td><td>37.6</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">21</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P7 CSP-P7 (single-scale, 16 fps)</a></td><td>55.4</td><td>38.1</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">22</td><td><a href="https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural">CSP-p6 + Mish (multi-scale)</a></td><td>55.2</td><td>37.6</td><td><a href="https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural">Mish: A Self Regularized Non-Monotonic Activation Function</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">23</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P6 with TTA</a></td><td>54.9</td><td></td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">24</td><td><a href="https://paperswithcode.com/paper/simple-copy-paste-is-a-strong-data">Cascade Eff-B7 NAS-FPN (1280)</a></td><td>54.8</td><td></td><td><a href="https://paperswithcode.com/paper/simple-copy-paste-is-a-strong-data">Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>NAS-FPN</strong></td></tr><tr><td align="center">25</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-32x4d, multi-scale)</a></td><td>54.7</td><td>37.4</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">26</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P6 CSP-P6 (single-scale, 32 fps)</a></td><td>54.3</td><td>36.6</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">27</td><td><a href="https://paperswithcode.com/paper/rethinking-pre-training-and-self-training">SpineNet-190 (1280, with Self-training on OpenImages, single-scale)</a></td><td>54.3</td><td></td><td><a href="https://paperswithcode.com/paper/rethinking-pre-training-and-self-training">Rethinking Pre-training and Self-training</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong></td></tr><tr><td align="center">28</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08d (Res2Net-101, DCN, multi-scale)</a></td><td>54.1</td><td>35.8</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">29</td><td><a href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet-D7 (single-scale)</a></td><td>53.7</td><td></td><td><a href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet: Scalable and Efficient Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">30</td><td><a href="https://paperswithcode.com/paper/probabilistic-anchor-assignment-with-iou">PAA (ResNext-152-32x8d + DCN, multi-scale)</a></td><td>53.5</td><td>36.0</td><td><a href="https://paperswithcode.com/paper/probabilistic-anchor-assignment-with-iou">Probabilistic Anchor Assignment with IoU Prediction for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">31</td><td><a href="https://paperswithcode.com/paper/location-sensitive-visual-recognition-with">LSNet (Res2Net-101+ DCN, multi-scale)</a></td><td>53.5</td><td>35.2</td><td><a href="https://paperswithcode.com/paper/location-sensitive-visual-recognition-with">Location-Sensitive Visual Recognition with Cross-IOU Loss</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">32</td><td><a href="https://paperswithcode.com/paper/resnest-split-attention-networks">ResNeSt-200 (multi-scale)</a></td><td>53.3</td><td>35.1</td><td><a href="https://paperswithcode.com/paper/resnest-split-attention-networks">ResNeSt: Split-Attention Networks</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong></td></tr><tr><td align="center">33</td><td><a href="https://paperswithcode.com/paper/cbnet-a-novel-composite-backbone-network">Cascade Mask R-CNN (Triple-ResNeXt152, multi-scale)</a></td><td>53.3</td><td>35.5</td><td><a href="https://paperswithcode.com/paper/cbnet-a-novel-composite-backbone-network">CBNet: A Novel Composite Backbone Network Architecture for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">34</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-32x4d, single-scale)</a></td><td>53.3</td><td>33.9</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>single scale</strong></td></tr><tr><td align="center">35</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (Res2Net-101, DCN, multiscale)</a></td><td>53.3</td><td>35.7</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">36</td><td><a href="https://paperswithcode.com/paper/relationnet-bridging-visual-representations">RelationNet++ (ResNeXt-64x4d-101-DCN)</a></td><td>52.7</td><td></td><td><a href="https://paperswithcode.com/paper/relationnet-bridging-visual-representations">RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">37</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P5 with TTA</a></td><td>52.5</td><td></td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">38</td><td><a href="https://paperswithcode.com/paper/deformable-detr-deformable-transformers-for-1">Deformable DETR (ResNeXt-101+DCN)</a></td><td>52.3</td><td>34.4</td><td><a href="https://paperswithcode.com/paper/deformable-detr-deformable-transformers-for-1">Deformable DETR: Deformable Transformers for End-to-End Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">39</td><td><a href="https://paperswithcode.com/paper/global-context-networks">GCNet (ResNeXt-101 + DCN + cascade + GC r4)</a></td><td>52.3</td><td></td><td><a href="https://paperswithcode.com/paper/global-context-networks">Global Context Networks</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong><br><strong>GCN</strong></td></tr><tr><td align="center">40</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-190, 1280x1280)</a></td><td>52.1</td><td>35.4</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">41</td><td><a href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints v2 (ResNeXt-101, DCN, multi-scale)</a></td><td>52.1</td><td>34.5</td><td><a href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints V2: Verification Meets Regression for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong>;<br><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td align="center"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td align="center">42</td><td><a href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">AC-FPN Cascade R-CNN (X-152-32x8d-FPN-IN5k, multi scale, only CEM)</a></td><td>51.9</td><td>34.2</td><td><a href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">Attention-guided Context Feature Pyramid Network for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>FPN</strong></td></tr><tr><td align="center">43</td><td><a href="https://paperswithcode.com/paper/ota-optimal-transport-assignment-for-object">OTA (ResNeXt-101+DCN, multiscale)</a></td><td>51.5</td><td>34.1</td><td><a href="https://paperswithcode.com/paper/ota-optimal-transport-assignment-for-object">OTA: Optimal Transport Assignment for Object Detection</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">44</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08d (Res2Net-101, DCN, single-scale)</a></td><td>51.3</td><td>31.7</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">45</td><td><a href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">TSD (SENet154-DCN,multi-scale)</a></td><td>51.2</td><td>33.8</td><td><a href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">Revisiting the Sibling Head in Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">46</td><td><a href="https://paperswithcode.com/paper/yolox-exceeding-yolo-series-in-2021">YOLOX-X (Modified CSP v5)</a></td><td>51.2</td><td>31.2</td><td><a href="https://paperswithcode.com/paper/yolox-exceeding-yolo-series-in-2021">YOLOX: Exceeding YOLO Series in 2021</a></td><td></td><td></td><td>2021</td><td><strong>YOLO</strong></td></tr><tr><td align="center">47</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-143, 1280x1280)</a></td><td>50.7</td><td>33.6</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">48</td><td><a href="https://paperswithcode.com/paper/bridging-the-gap-between-anchor-based-and">ATSS (ResNetXt-64x4d-101+DCN,multi-scale)</a></td><td>50.7</td><td>33.2</td><td><a href="https://paperswithcode.com/paper/bridging-the-gap-between-anchor-based-and">Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">49</td><td><a href="https://paperswithcode.com/paper/learning-data-augmentation-strategies-for">NAS-FPN (AmoebaNet-D, learned aug)</a></td><td>50.7</td><td>34.2</td><td><a href="https://paperswithcode.com/paper/learning-data-augmentation-strategies-for">Learning Data Augmentation Strategies for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">50</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (Res2Net-101, DCN)</a></td><td>50.6</td><td>31.3</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>DCN</strong></td></tr><tr><td align="center">51</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, DCN, multiscale test)</a></td><td>50.2</td><td>32.0</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">52</td><td><a href="https://paperswithcode.com/paper/scale-equalizing-pyramid-convolution-for">FreeAnchor + SEPC (DCN, ResNext-101-64x4d)</a></td><td>50.1</td><td>31.3</td><td><a href="https://paperswithcode.com/paper/scale-equalizing-pyramid-convolution-for">Scale-Equalizing Pyramid Convolution for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">53</td><td><a href="https://paperswithcode.com/paper/d2det-towards-high-quality-object-detection">D2Det (ResNet-101-DCN, multi-scale test)</a></td><td>50.1</td><td>32.7</td><td><a href="https://paperswithcode.com/paper/d2det-towards-high-quality-object-detection">D2Det: Towards High Quality Object Detection and Instance Segmentation</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">54</td><td><a href="https://paperswithcode.com/paper/dynamic-r-cnn-towards-high-quality-object">Dynamic R-CNN (ResNet-101-DCN, multi-scale)</a></td><td>50.1</td><td>32.8</td><td><a href="https://paperswithcode.com/paper/dynamic-r-cnn-towards-high-quality-object">Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">55</td><td><a href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">TSD (ResNet-101-Deformable, Image Pyramid)</a></td><td>49.4</td><td>32.7</td><td><a href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">Revisiting the Sibling Head in Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>ResNet</strong></td></tr><tr><td align="center">56</td><td><a href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints v2 (ResNeXt-101, DCN)</a></td><td>49.4</td><td>30.3</td><td><a href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints V2: Verification Meets Regression for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">57</td><td><a href="https://paperswithcode.com/paper/corner-proposal-network-for-anchor-free-two">CPNDet (Hourglass-104, multi-scale)</a></td><td>49.2</td><td>31.0</td><td><a href="https://paperswithcode.com/paper/corner-proposal-network-for-anchor-free-two">Corner Proposal Network for Anchor-free, Two-stage Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong></td></tr><tr><td align="center">58</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNeXt-101, 32x4d, DCN)</a></td><td>49</td><td>29.7</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">59</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, DCN, single scale)</a></td><td>48.9</td><td>30.8</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>single scale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">60</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08 (Res2Net-50, DCN, single-scale)</a></td><td>48.8</td><td>30.1</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">61</td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (ResNet101, single scale)</a></td><td>48.7</td><td></td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td><td></td><td></td><td>2021</td><td><strong>Transformer</strong><br><strong>single scale</strong></td></tr><tr><td align="center">62</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-96, 1024x1024)</a></td><td>48.6</td><td>32</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">63</td><td><a href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">TridentNet (ResNet-101-Deformable, Image Pyramid)</a></td><td>48.4</td><td>31.8</td><td><a href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">Scale-Aware Trident Networks for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">64</td><td><a href="https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze">GCNet (ResNeXt-101 + DCN + cascade + GC r4)</a></td><td>48.4</td><td></td><td><a href="https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>DCN</strong><br><strong>GCN</strong></td></tr><tr><td align="center">65</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-101-DCN)</a></td><td>48.3</td><td>28.8</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">66</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-learning-qualified-and">GFL (X-101-32x4d-DCN, single-scale)</a></td><td>48.2</td><td>29.2</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-learning-qualified-and">Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>single scale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">67</td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet101-FPN-3x, single-scale)</a></td><td>48.1</td><td>28.7</td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">68</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, single scale)</a></td><td>47.8</td><td>30.2</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>single scale</strong></td></tr><tr><td align="center">69</td><td><a href="https://paperswithcode.com/paper/matrix-nets-a-new-deep-architecture-for">MatrixNet Corners (ResNet-152, multi-scale)</a></td><td>47.8</td><td>29.7</td><td><a href="https://paperswithcode.com/paper/matrix-nets-a-new-deep-architecture-for">Matrix Nets: A New Deep Architecture for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">70</td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (ResNet50, single scale)</a></td><td>47.8</td><td></td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td><td></td><td></td><td>2021</td><td><strong>Transformer</strong><br><strong>single scale</strong></td></tr><tr><td align="center">71</td><td><a href="https://paperswithcode.com/paper/soft-anchor-point-object-detection">SAPD (ResNeXt-101, single-scale)</a></td><td>47.4</td><td>28.1</td><td><a href="https://paperswithcode.com/paper/soft-anchor-point-object-detection">Soft Anchor-Point Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>single scale</strong></td></tr><tr><td align="center">72</td><td><a href="https://paperswithcode.com/paper/path-aggregation-network-for-instance">PANet (ResNeXt-101, multi-scale)</a></td><td>47.4</td><td>30.1</td><td><a href="https://paperswithcode.com/paper/path-aggregation-network-for-instance">Path Aggregation Network for Instance Segmentation</a></td><td></td><td></td><td>2018</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">73</td><td><a href="https://paperswithcode.com/paper/190807919">HTC (HRNetV2p-W48)</a></td><td>47.3</td><td>28.0</td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">74</td><td><a href="https://paperswithcode.com/paper/hybrid-task-cascade-for-instance-segmentation">HTC (ResNeXt-101-FPN)</a></td><td>47.1</td><td>22.8</td><td><a href="https://paperswithcode.com/paper/hybrid-task-cascade-for-instance-segmentation">Hybrid Task Cascade for Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">75</td><td><a href="https://paperswithcode.com/paper/centernet-object-detection-with-keypoint">CenterNet511 (Hourglass-104, multi-scale)</a></td><td>47.0</td><td>28.9</td><td><a href="https://paperswithcode.com/paper/centernet-object-detection-with-keypoint">CenterNet: Keypoint Triplets for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">76</td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNeXt101, multi-scale)</a></td><td>47.0</td><td></td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">77</td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet50-FPN-3x)</a></td><td>46.8</td><td></td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td><td></td><td></td><td>2021</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">78</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49, 896x896)</a></td><td>46.7</td><td>29.1</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">79</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101-DCN, multi-scale)</a></td><td>46.5</td><td>30.3</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">80</td><td><a href="https://paperswithcode.com/paper/houghnet-integrating-near-and-long-range">HoughNet (MS)</a></td><td>46.4</td><td>29.1</td><td><a href="https://paperswithcode.com/paper/houghnet-integrating-near-and-long-range">HoughNet: Integrating near and long-range evidence for bottom-up object detection</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong></td></tr><tr><td align="center">81</td><td><a href="https://paperswithcode.com/paper/reducing-label-noise-in-anchor-free-object">PPDet (ResNeXt-101-FPN, multiscale)</a></td><td>46.3</td><td>31.4</td><td><a href="https://paperswithcode.com/paper/reducing-label-noise-in-anchor-free-object">Reducing Label Noise in Anchor-Free Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>FPN</strong></td></tr><tr><td align="center">82</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-101)</a></td><td>46.2</td><td>27.8</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNet</strong></td></tr><tr><td align="center">83</td><td><a href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER (ResNet-101)</a></td><td>46.1</td><td>29.6</td><td><a href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER: Efficient Multi-Scale Training</a></td><td></td><td></td><td>2018</td><td><strong>ResNet</strong></td></tr><tr><td align="center">84</td><td><a href="https://paperswithcode.com/paper/190807919">Mask R-CNN (HRNetV2p-W48 + cascade)</a></td><td>46.1</td><td>27.1</td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">85</td><td><a href="https://paperswithcode.com/paper/deformable-convnets-v2-more-deformable-better">DCNv2 (ResNet-101, multi-scale)</a></td><td>46.0</td><td>27.8</td><td><a href="https://paperswithcode.com/paper/deformable-convnets-v2-more-deformable-better">Deformable ConvNets v2: More Deformable, Better Results</a></td><td></td><td></td><td>2018</td><td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">86</td><td><a href="https://paperswithcode.com/paper/localization-uncertainty-estimation-for">Gaussian-FCOS</a></td><td>46</td><td></td><td><a href="https://paperswithcode.com/paper/localization-uncertainty-estimation-for">Localization Uncertainty Estimation for Anchor-Free Object Detection</a></td><td></td><td></td><td>2020</td><td></td></tr><tr><td align="center">87</td><td><a href="https://paperswithcode.com/paper/instaboost-boosting-instance-segmentation-via">Cascade R-CNN-FPN (ResNet-101, map-guided)</a></td><td>45.9</td><td>26.3</td><td><a href="https://paperswithcode.com/paper/instaboost-boosting-instance-segmentation-via">InstaBoost: Boosting Instance Segmentation via Probability Map Guided Copy-Pasting</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">88</td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNeXt101, single-scale)</a></td><td>45.9</td><td></td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>single scale</strong></td></tr><tr><td align="center">89</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask+VoVNetV2-99 (single-scale)</a></td><td>45.8</td><td>27.8</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">90</td><td><a href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">D-RFCN + SNIP (DPN-98 with flip, multi-scale)</a></td><td>45.7</td><td>29.3</td><td><a href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">An Analysis of Scale Invariance in Object Detection - SNIP</a></td><td></td><td></td><td>2017</td><td><strong>multiscale</strong></td></tr><tr><td align="center">91</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4 (CD53)</a></td><td>45.5</td><td>27</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">92</td><td><a href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO (608x608)</a></td><td>45.2</td><td>26.3</td><td><a href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>YOLO</strong></td></tr><tr><td align="center">93</td><td><a href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">AC-FPN Cascade R-CNN (ResNet-101, single scale)</a></td><td>45</td><td>26.9</td><td><a href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">Attention-guided Context Feature Pyramid Network for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong><br><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">94</td><td><a href="https://paperswithcode.com/paper/freeanchor-learning-to-match-anchors-for">FreeAnchor (ResNeXt-101)</a></td><td>44.8</td><td>27</td><td><a href="https://paperswithcode.com/paper/freeanchor-learning-to-match-anchors-for">FreeAnchor: Learning to Match Anchors for Visual Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">95</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-64x4d-101-FPN 4 + improvements)</a></td><td>44.7</td><td>27.6</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">96</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask+VoVNet2-57 (single-scale)</a></td><td>44.7</td><td>27.1</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">97</td><td><a href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">FSAF (ResNeXt-101, multi-scale)</a></td><td>44.6</td><td>29.7</td><td><a href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">98</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101, DCN, 500 scale)</a></td><td>44.6</td><td>24.6</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">99</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask + X-101-32x8d (single-scale)</a></td><td>44.6</td><td></td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">100</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49, 640x640)</a></td><td>44.3</td><td>25.9</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">101</td><td><a href="https://paperswithcode.com/paper/you-only-look-one-level-feature">YOLOF-DC5</a></td><td>44.3</td><td>24.0</td><td><a href="https://paperswithcode.com/paper/you-only-look-one-level-feature">You Only Look One-level Feature</a></td><td></td><td></td><td>2021</td><td><strong>YOLO</strong></td></tr><tr><td align="center">102</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-50)</a></td><td>44.3</td><td>26.8</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNet</strong></td></tr><tr><td align="center">103</td><td><a href="https://paperswithcode.com/paper/feature-intertwiner-for-object-detection-1">InterNet (ResNet-101-FPN, multi-scale)</a></td><td>44.2</td><td>27.2</td><td><a href="https://paperswithcode.com/paper/feature-intertwiner-for-object-detection-1">Feature Intertwiner for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong><br><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">104</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (VGG-16, multi-scale)</a></td><td>44.2</td><td>29.2</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td><td></td><td></td><td>2018</td><td><strong>multiscale</strong></td></tr><tr><td align="center">105</td><td><a href="https://paperswithcode.com/paper/lip-local-importance-based-pooling">Faster R-CNN (LIP-ResNet-101-MD w FPN)</a></td><td>43.9</td><td>25.4</td><td><a href="https://paperswithcode.com/paper/lip-local-importance-based-pooling">LIP: Local Importance-based Pooling</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">106</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (ResNet-101, multi-scale)</a></td><td>43.9</td><td>29.6</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td><td></td><td></td><td>2018</td><td><strong>multiscale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">107</td><td><a href="https://paperswithcode.com/paper/learning-spatial-fusion-for-single-shot">YOLOv3 @800 + ASFF* (Darknet-53)</a></td><td>43.9</td><td>27.0</td><td><a href="https://paperswithcode.com/paper/learning-spatial-fusion-for-single-shot">Learning Spatial Fusion for Single-Shot Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>YOLO</strong></td></tr><tr><td align="center">108</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td><td>43.9</td><td>26.8</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">109</td><td><a href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">ExtremeNet (Hourglass-104, multi-scale)</a></td><td>43.7</td><td>24.1</td><td><a href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">Bottom-up Object Detection by Grouping Extreme and Center Points</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">110</td><td><a href="https://paperswithcode.com/paper/yolov4-optimal-speed-and-accuracy-of-object">YOLOv4-608</a></td><td>43.5</td><td>26.7</td><td><a href="https://paperswithcode.com/paper/yolov4-optimal-speed-and-accuracy-of-object">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">111</td><td><a href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER (ResNet-50)</a></td><td>43.5</td><td>26.1</td><td><a href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER: Efficient Multi-Scale Training</a></td><td></td><td></td><td>2018</td><td><strong>ResNet</strong></td></tr><tr><td align="center">112</td><td><a href="https://paperswithcode.com/paper/190807919">CenterNet (HRNetV2-W48)</a></td><td>43.5</td><td>22.2</td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">113</td><td><a href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">D-RFCN + SNIP (ResNet-101, multi-scale)</a></td><td>43.4</td><td>27.2</td><td><a href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">An Analysis of Scale Invariance in Object Detection - SNIP</a></td><td></td><td></td><td>2017</td><td><strong>multiscale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">114</td><td><a href="https://paperswithcode.com/paper/grid-r-cnn">Grid R-CNN (ResNeXt-101-FPN)</a></td><td>43.2</td><td>25.1</td><td><a href="https://paperswithcode.com/paper/grid-r-cnn">Grid R-CNN</a></td><td></td><td></td><td>2018</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">115</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-101-64x4d-FPN)</a></td><td>43.2</td><td>26.5</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">116</td><td><a href="https://paperswithcode.com/paper/190408900">CornerNet-Saccade (Hourglass-104, multi-scale)</a></td><td>43.2</td><td>24.4</td><td><a href="https://paperswithcode.com/paper/190408900">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">117</td><td><a href="https://paperswithcode.com/paper/libra-r-cnn-towards-balanced-learning-for">Libra R-CNN (ResNeXt-101-FPN)</a></td><td>43.0</td><td>25.3</td><td><a href="https://paperswithcode.com/paper/libra-r-cnn-towards-balanced-learning-for">Libra R-CNN: Towards Balanced Learning for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">118</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101-DCN)</a></td><td>42.8</td><td>24.9</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">119</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet-49 (640, RetinaNet, single-scale)</a></td><td>42.8</td><td>23.7</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">120</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-101-FPN+, cascade)</a></td><td>42.8</td><td>23.7</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">121</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-high-quality-object-detection">Cascade R-CNN</a></td><td>42.8</td><td>23.7</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-high-quality-object-detection">Cascade R-CNN: High Quality Object Detection and Instance Segmentation</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">122</td><td><a href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">TridentNet (ResNet-101)</a></td><td>42.7</td><td>23.9</td><td><a href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">Scale-Aware Trident Networks for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">123</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-32x8d-101-FPN)</a></td><td>42.7</td><td>26.0</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">124</td><td><a href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask (ResNeXt-101-FPN-GN)</a></td><td>42.6</td><td>24.8</td><td><a href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">125</td><td><a href="https://paperswithcode.com/paper/tood-task-aligned-one-stage-object-detection">TAL + TAP</a></td><td>42.5</td><td></td><td><a href="https://paperswithcode.com/paper/tood-task-aligned-one-stage-object-detection">TOOD: Task-aligned One-stage Object Detection</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">126</td><td><a href="https://paperswithcode.com/paper/190807919">Faster R-CNN (HRNetV2p-W48)</a></td><td>42.4</td><td>24.9</td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">127</td><td><a href="https://paperswithcode.com/paper/hierarchical-shot-detector">HSD (Rest101, 768x768, single-scale test)</a></td><td>42.3</td><td>22.8</td><td><a href="https://paperswithcode.com/paper/hierarchical-shot-detector">Hierarchical Shot Detector</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">128</td><td><a href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet511 (Hourglass-104, multi-scale)</a></td><td>42.1</td><td>20.8</td><td><a href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet: Detecting Objects as Paired Keypoints</a></td><td></td><td></td><td>2018</td><td><strong>multiscale</strong></td></tr><tr><td align="center">129</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td><td>42.1</td><td></td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">130</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (HRNet-W32-5l)</a></td><td>42.0</td><td>25.4</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">131</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512+ (ResNet-101)</a></td><td>41.8</td><td>25.6</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>ResNet</strong></td></tr><tr><td align="center">132</td><td><a href="https://paperswithcode.com/paper/gradient-harmonized-single-stage-detector">GHM-C + GHM-R (RetinaNet-FPN-ResNeXt-101)</a></td><td>41.6</td><td>22.3</td><td><a href="https://paperswithcode.com/paper/gradient-harmonized-single-stage-detector">Gradient Harmonized Single-stage Detector</a></td><td></td><td></td><td>2018</td><td><strong>FPN</strong></td></tr><tr><td align="center">133</td><td><a href="https://paperswithcode.com/paper/objects-as-points">CenterNet-DLA (DLA-34, multi-scale)</a></td><td>41.6</td><td>21.5</td><td><a href="https://paperswithcode.com/paper/objects-as-points">Objects as Points</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">134</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49S, 640x640)</a></td><td>41.5</td><td>23.3</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">135</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101)</a></td><td>41</td><td>23.6</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">136</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (VGG-16, single-scale)</a></td><td>41.0</td><td>22.1</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td><td></td><td></td><td>2018</td><td><strong>single scale</strong></td></tr><tr><td align="center">137</td><td><a href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">FSAF (ResNet-101, single-scale)</a></td><td>40.9</td><td>24</td><td><a href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">138</td><td><a href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">RetinaNet (ResNeXt-101-FPN)</a></td><td>40.8</td><td>24.1</td><td><a href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">Focal Loss for Dense Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">139</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-50-FPN+, cascade)</a></td><td>40.6</td><td>22.6</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">140</td><td><a href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Faster R-CNN (Cascade RPN)</a></td><td>40.6</td><td>22.0</td><td><a href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">141</td><td><a href="https://paperswithcode.com/paper/deformable-kernels-adapting-effective">ResNet-50-DW-DPN (Deformable Kernels)</a></td><td>40.6</td><td>24.6</td><td><a href="https://paperswithcode.com/paper/deformable-kernels-adapting-effective">Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">142</td><td><a href="https://paperswithcode.com/paper/acquisition-of-localization-confidence-for">IoU-Net</a></td><td>40.6</td><td></td><td><a href="https://paperswithcode.com/paper/acquisition-of-localization-confidence-for">Acquisition of Localization Confidence for Accurate Object Detection</a></td><td></td><td></td><td>2018</td><td></td></tr><tr><td align="center">143</td><td><a href="https://paperswithcode.com/paper/190807919">FCOS (HRNetV2p-W48)</a></td><td>40.5</td><td>23.4</td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">144</td><td><a href="https://paperswithcode.com/paper/softer-nms-rethinking-bounding-box-regression">ResNet-50-FPN Mask R-CNN + KL Loss + var voting + soft-NMS</a></td><td>40.4</td><td></td><td><a href="https://paperswithcode.com/paper/softer-nms-rethinking-bounding-box-regression">Bounding Box Regression with Uncertainty for Accurate Object Detection</a></td><td></td><td></td><td>2018</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">145</td><td><a href="https://paperswithcode.com/paper/rdsnet-a-new-deep-architecture-for-reciprocal">RDSNet (ResNet-101, RetinaNet, mask, MBRM)</a></td><td>40.3</td><td>22.1</td><td><a href="https://paperswithcode.com/paper/rdsnet-a-new-deep-architecture-for-reciprocal">RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">146</td><td><a href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">ExtremeNet (Hourglass-104, single-scale)</a></td><td>40.2</td><td>20.4</td><td><a href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">Bottom-up Object Detection by Grouping Extreme and Center Points</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">147</td><td><a href="https://paperswithcode.com/paper/cross-iteration-batch-normalization">Mask R-CNN (ResNet-101-FPN, CBN)</a></td><td>40.1</td><td>35.8</td><td><a href="https://paperswithcode.com/paper/cross-iteration-batch-normalization">Cross-Iteration Batch Normalization</a></td><td></td><td></td><td>2020</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">148</td><td><a href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Fast R-CNN (Cascade RPN)</a></td><td>40.1</td><td>22.1</td><td><a href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">149</td><td><a href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN (ResNeXt-101-FPN)</a></td><td>39.8</td><td>22.1</td><td><a href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN</a></td><td></td><td></td><td>2017</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">150</td><td><a href="https://paperswithcode.com/paper/region-proposal-by-guided-anchoring">GA-Faster-RCNN</a></td><td>39.8</td><td>21.8</td><td><a href="https://paperswithcode.com/paper/region-proposal-by-guided-anchoring">Region Proposal by Guided Anchoring</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">151</td><td><a href="https://paperswithcode.com/paper/chainercv-a-library-for-deep-learning-in">FPN (ResNet101 backbone)</a></td><td>39.5</td><td></td><td><a href="https://paperswithcode.com/paper/chainercv-a-library-for-deep-learning-in">ChainerCV: a Library for Deep Learning in Computer Vision</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">152</td><td><a href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask (ResNet-50-FPN)</a></td><td>39.4</td><td>21.9</td><td><a href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">153</td><td><a href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO (320x320)</a></td><td>39.3</td><td>16.7</td><td><a href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>YOLO</strong></td></tr><tr><td align="center">154</td><td><a href="https://paperswithcode.com/paper/190409925">AA-ResNet-10 + RetinaNet</a></td><td>39.2</td><td></td><td><a href="https://paperswithcode.com/paper/190409925">Attention Augmented Convolutional Networks</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">155</td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNet50, single-scale)</a></td><td>39.2</td><td></td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">156</td><td><a href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">RetinaNet (ResNet-101-FPN)</a></td><td>39.1</td><td>21.8</td><td><a href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">Focal Loss for Dense Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">157</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-101-FPN+)</a></td><td>38.8</td><td>21.3</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">158</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (ResNet-101, single-scale)</a></td><td>38.8</td><td>20.5</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td><td></td><td></td><td>2018</td><td><strong>single scale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">159</td><td><a href="https://paperswithcode.com/paper/saccadenet-a-fast-and-accurate-object">SaccadeNet (DLA-34-DCN)</a></td><td>38.5</td><td>19.2</td><td><a href="https://paperswithcode.com/paper/saccadenet-a-fast-and-accurate-object">SaccadeNet: A Fast and Accurate Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>DCN</strong></td></tr><tr><td align="center">160</td><td><a href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN (ResNet-101-FPN)</a></td><td>38.2</td><td>20.1</td><td><a href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">161</td><td><a href="https://paperswithcode.com/paper/segmentation-is-all-you-need">WSMA-Seg</a></td><td>38.1</td><td></td><td><a href="https://paperswithcode.com/paper/segmentation-is-all-you-need">Segmentation is All You Need</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">162</td><td><a href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Faster R-CNN + FPN + CGD</a></td><td>37.9</td><td></td><td><a href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Compact Global Descriptor for Neural Networks</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">163</td><td><a href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet511 (Hourglass-52, single-scale)</a></td><td>37.8</td><td>17.0</td><td><a href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet: Detecting Objects as Paired Keypoints</a></td><td></td><td></td><td>2018</td><td><strong>single scale</strong></td></tr><tr><td align="center">164</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512+ (VGG-16)</a></td><td>37.6</td><td>22.7</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">165</td><td><a href="https://paperswithcode.com/paper/deformable-convolutional-networks">DeformConv-R-FCN (Aligned-Inception-ResNet)</a></td><td>37.5</td><td>19.4</td><td><a href="https://paperswithcode.com/paper/deformable-convolutional-networks">Deformable Convolutional Networks</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">166</td><td><a href="https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data">Faster R-CNN (ImageNet+300M)</a></td><td>37.4</td><td>17.5</td><td><a href="https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">167</td><td><a href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">Mask R-CNN (Bottleneck-injected ResNet-50, FPN)</a></td><td>36.9</td><td></td><td><a href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation</a></td><td></td><td></td><td>2020</td><td><strong>FPN</strong><br>！！<strong>ResNet</strong></td></tr><tr><td align="center">168</td><td><a href="https://paperswithcode.com/paper/beyond-skip-connections-top-down-modulation">Faster R-CNN + TDM</a></td><td>36.8</td><td></td><td><a href="https://paperswithcode.com/paper/beyond-skip-connections-top-down-modulation">Beyond Skip Connections: Top-Down Modulation for Object Detection</a></td><td></td><td></td><td>2016</td><td></td></tr><tr><td align="center">169</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-50-FPN+)</a></td><td>36.5</td><td>20.3</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong>;<br><strong>ResNet</strong></td></tr><tr><td align="center">170</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512 (ResNet-101)</a></td><td>36.4</td><td>16.6</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>ResNet</strong></td></tr><tr><td align="center">171</td><td><a href="https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection">Faster R-CNN + FPN</a></td><td>36.2</td><td></td><td><a href="https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection">Feature Pyramid Networks for Object Detection</a></td><td></td><td></td><td>2016</td><td><strong>FPN</strong></td></tr><tr><td align="center">172</td><td><a href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">Faster R-CNN (Bottleneck-injected ResNet-50 and FPN)</a></td><td>35.9</td><td></td><td><a href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation</a></td><td></td><td></td><td>2020</td><td><strong>FPN</strong>;<br><strong>ResNet</strong></td></tr><tr><td align="center">173</td><td><a href="https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition">Faster R-CNN (box refinement, context, multi-scale testing)</a></td><td>34.9</td><td></td><td><a href="https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition">Deep Residual Learning for Image Recognition</a></td><td></td><td></td><td>2015</td><td><strong>multiscale</strong></td></tr><tr><td align="center">174</td><td><a href="https://paperswithcode.com/paper/speedaccuracy-trade-offs-for-modern">Faster R-CNN</a></td><td>34.7</td><td></td><td><a href="https://paperswithcode.com/paper/speedaccuracy-trade-offs-for-modern">Speed/accuracy trade-offs for modern convolutional object detectors</a></td><td></td><td></td><td>2016</td><td></td></tr><tr><td align="center">175</td><td><a href="https://paperswithcode.com/paper/190408900">CornerNet-Squeeze</a></td><td>34.4</td><td></td><td><a href="https://paperswithcode.com/paper/190408900">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">176</td><td><a href="https://paperswithcode.com/paper/a-multipath-network-for-object-detection">MultiPath Network</a></td><td>33.2</td><td></td><td><a href="https://paperswithcode.com/paper/a-multipath-network-for-object-detection">A MultiPath Network for Object Detection</a></td><td></td><td></td><td>2016</td><td></td></tr><tr><td align="center">177</td><td><a href="https://paperswithcode.com/paper/inside-outside-net-detecting-objects-in">ION</a></td><td>33.1</td><td>14.5</td><td><a href="https://paperswithcode.com/paper/inside-outside-net-detecting-objects-in">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</a></td><td></td><td></td><td>2015</td><td></td></tr><tr><td align="center">178</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512 (VGG-16)</a></td><td>33</td><td>16.3</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">179</td><td><a href="https://paperswithcode.com/paper/yolov3-an-incremental-improvement">YOLOv3 + Darknet-53</a></td><td>33.0</td><td></td><td><a href="https://paperswithcode.com/paper/yolov3-an-incremental-improvement">YOLOv3: An Incremental Improvement</a></td><td></td><td></td><td>2018</td><td><strong>YOLO</strong></td></tr><tr><td align="center">180</td><td><a href="https://paperswithcode.com/paper/ssd-single-shot-multibox-detector">SSD512</a></td><td>28.8</td><td></td><td><a href="https://paperswithcode.com/paper/ssd-single-shot-multibox-detector">SSD: Single Shot MultiBox Detector</a></td><td></td><td></td><td>2015</td><td></td></tr><tr><td align="center">181</td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MobileNetV2)</a></td><td>26.1</td><td></td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">182</td><td><a href="https://paperswithcode.com/paper/espnetv2-a-light-weight-power-efficient-and">ESPNetv2-512</a></td><td>26.0</td><td></td><td><a href="https://paperswithcode.com/paper/espnetv2-a-light-weight-power-efficient-and">ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network</a></td><td></td><td></td><td>2018</td><td></td></tr><tr><td align="center">183</td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MobileNetV3)</a></td><td>25.5</td><td></td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">184</td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MNASNet-B1)</a></td><td>24.6</td><td></td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">185</td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN x0.7 (MobileNetV2)</a></td><td>23.8</td><td></td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">186</td><td><a href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">MobielNet-v1-SSD-300x300+CGD</a></td><td>21.4</td><td></td><td><a href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Compact Global Descriptor for Neural Networks</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">187</td><td><a href="https://paperswithcode.com/paper/fast-r-cnn">Fast-RCNN</a></td><td>19.7</td><td></td><td><a href="https://paperswithcode.com/paper/fast-r-cnn">Fast R-CNN</a></td><td></td><td></td><td>2015</td><td></td></tr><tr><td align="center">188</td><td><a href="https://paperswithcode.com/paper/mobilenets-efficient-convolutional-neural">MobileNet</a></td><td>19.3</td><td></td><td><a href="https://paperswithcode.com/paper/mobilenets-efficient-convolutional-neural">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">189</td><td><a href="https://paperswithcode.com/paper/vision-transformer-with-deformable-attention">DAT-S (RetinaNet)</a></td><td></td><td>32.3</td><td><a href="https://paperswithcode.com/paper/vision-transformer-with-deformable-attention">Vision Transformer with Deformable Attention</a></td><td></td><td></td><td>2022</td><td></td></tr><tr><td align="center">190</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask-VoVNet99 (multi-scale)</a></td><td></td><td>32.4</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">191</td><td><a href="https://paperswithcode.com/paper/190807919">Mask R-CNN (HRNetV2p-W32 + cascade)</a></td><td></td><td></td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">192</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td><td></td><td></td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">193</td><td><a href="https://paperswithcode.com/paper/virtex-learning-visual-representations-from">VirTex Mask R-CNN (ResNet-50-FPN)</a></td><td></td><td></td><td><a href="https://paperswithcode.com/paper/virtex-learning-visual-representations-from">VirTex: Learning Visual Representations from Textual Annotations</a></td><td></td><td></td><td>2020</td><td><strong>FPN</strong>;<br><strong>ResNet</strong></td></tr><tr><td align="center">194</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">Centermask + ResNet101</a></td><td></td><td></td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">195</td><td><a href="https://paperswithcode.com/paper/pafnet-an-efficient-anchor-free-object">PAFNet (ResNet50-vd)</a></td><td></td><td>22.8</td><td><a href="https://paperswithcode.com/paper/pafnet-an-efficient-anchor-free-object">PAFNet: An Efficient Anchor-Free Object Detector Guidance</a></td><td></td><td></td><td>2021</td><td><strong>ResNet</strong></td></tr><tr><td align="center">196</td><td><a href="https://paperswithcode.com/paper/dctd-deep-conditional-target-densities-for">IoU-Net+EnergyRegression</a></td><td></td><td></td><td><a href="https://paperswithcode.com/paper/dctd-deep-conditional-target-densities-for">Energy-Based Models for Deep Probabilistic Regression</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">197</td><td><a href="https://paperswithcode.com/paper/190807919">Cascade R-CNN (HRNetV2p-W48)</a></td><td></td><td>26.0</td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">198</td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet50-FPN-3x, single-scale)</a></td><td></td><td>27.8</td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">199</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td><td></td><td>24.9</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">200</td><td><a href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet-D7x (single-scale)</a></td><td></td><td></td><td><a href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet: Scalable and Efficient Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr></tbody></table><hr><table><thead><tr><th align="center">Rank</th><th>Model</th><th><strong>box AP</strong></th><th>AP50</th><th>AP75</th><th>APS</th><th>APM</th><th>APL</th><th>AP</th><th align="center">Extra Training Data</th><th>Paper</th><th>Code</th><th>Result</th><th>Year</th><th>Tags</th></tr></thead><tbody><tr><td align="center">1</td><td><a href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and">SwinV2-G (HTC++)</a></td><td>63.1</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and">Swin Transformer V2: Scaling Up Capacity and Resolution</a></td><td><a href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and#code">Link</a></td><td></td><td>2021</td><td><strong>Swin-Transformer</strong></td></tr><tr><td align="center">2</td><td><a href="https://paperswithcode.com/paper/florence-a-new-foundation-model-for-computer">Florence-CoSwin-H</a></td><td>62.4</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/florence-a-new-foundation-model-for-computer">Florence: A New Foundation Model for Computer Vision</a></td><td></td><td></td><td>2021</td><td><strong>Swin-Transformer</strong></td></tr><tr><td align="center">3</td><td><a href="https://paperswithcode.com/paper/grounded-language-image-pre-training">GLIP (Swin-L, multi-scale)</a></td><td>61.5</td><td>79.5</td><td>67.7</td><td>45.3</td><td>64.9</td><td>75.0</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/grounded-language-image-pre-training">Grounded Language-Image Pre-training</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong>;<br>Vision Language**;<br><strong>Dynamic Head</strong>;<br>**BERT-Base</td></tr><tr><td align="center">4</td><td><a href="https://paperswithcode.com/paper/end-to-end-semi-supervised-object-detection">Soft Teacher + Swin-L (HTC++, multi-scale)</a></td><td>61.3</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/end-to-end-semi-supervised-object-detection">End-to-End Semi-Supervised Object Detection with Soft Teacher</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong>;<br>Swin-Transformer</td></tr><tr><td align="center">5</td><td><a href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">DyHead (Swin-L, multi scale, self-training)</a></td><td>60.6</td><td>78.5</td><td>66.6</td><td></td><td>64.0</td><td>74.2</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">Dynamic Head: Unifying Object Detection Heads with Attentions</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong>;<br>Swin-Transformer</td></tr><tr><td align="center">6</td><td><a href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">Dual-Swin-L (HTC, multi-scale)</a></td><td>60.1</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">CBNetV2: A Composite Backbone Network Architecture for Object Detection</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br>Swin-Transformer</td></tr><tr><td align="center">7</td><td><a href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">Dual-Swin-L (HTC, single-scale)</a></td><td>59.4</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">CBNetV2: A Composite Backbone Network Architecture for Object Detection</a></td><td></td><td></td><td>2021</td><td><strong>Swin-Transformer</strong></td></tr><tr><td align="center">8</td><td><a href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal-L (DyHead, multi-scale)</a></td><td>58.9</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal Self-attention for Local-Global Interactions in Vision Transformers</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br>Focal-Transformer</td></tr><tr><td align="center">9</td><td><a href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">DyHead (Swin-L, multi scale)</a></td><td>58.7</td><td>77.1</td><td>64.5</td><td>41.7</td><td>62.0</td><td>72.8</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">Dynamic Head: Unifying Object Detection Heads with Attentions</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br>Swin-Transformer</td></tr><tr><td align="center">10</td><td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin-L (HTC++, multi scale)</a></td><td>58.7</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br><strong>Swin-Transformer</strong></td></tr><tr><td align="center">11</td><td><a href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal-L (HTC++, multi-scale)</a></td><td>58.4</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal Self-attention for Local-Global Interactions in Vision Transformers</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong></td></tr><tr><td align="center">12</td><td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin-L (HTC++, single scale)</a></td><td>57.7</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>Swin-Transformer</strong></td></tr><tr><td align="center">13</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-D6 (1280, single-scale, 34 fps)</a></td><td>57.3</td><td>75.0</td><td>62.7</td><td>40.4</td><td>61.2</td><td>69.2</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br>YOLO</td></tr><tr><td align="center">14</td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (Swin-L, single)</a></td><td>56.5</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td><td></td><td></td><td>2021</td><td><strong>Transformer</strong><br><strong>single scale</strong></td></tr><tr><td align="center">15</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-E6 (1280, single-scale, 45 fps)</a></td><td>56.4</td><td>74.1</td><td>61.6</td><td>39.1</td><td>60.1</td><td>68.2</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">16</td><td><a href="https://paperswithcode.com/paper/probabilistic-two-stage-detection">CenterNet2 (Res2Net-101-DCN-BiFPN, self-training, 1560 single-scale)</a></td><td>56.4</td><td>74.0</td><td>61.6</td><td>38.7</td><td>59.7</td><td>68.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/probabilistic-two-stage-detection">Probabilistic two-stage detection</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>FPN</strong><br><strong>DCN</strong></td></tr><tr><td align="center">17</td><td><a href="https://paperswithcode.com/paper/queryinst-parallelly-supervised-mask-query">QueryInst (single-scale)</a></td><td>56.1</td><td>75.9</td><td>61.9</td><td>37.4</td><td>58.9</td><td>70.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/queryinst-parallelly-supervised-mask-query">Instances as Queries</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">18</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P7 with TTA</a></td><td>55.8</td><td>73.2</td><td>61.2</td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">19</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-64x4d, multi-scale)</a></td><td>55.7</td><td>74.2</td><td>61.1</td><td>37.7</td><td>58.4</td><td>68.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">20</td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-W6 (1280, single-scale, 66 fps)</a></td><td>55.5</td><td>73.2</td><td>60.6</td><td>37.6</td><td>59.5</td><td>67.7</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">21</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P7 CSP-P7 (single-scale, 16 fps)</a></td><td>55.4</td><td>73.3</td><td>60.7</td><td>38.1</td><td>59.5</td><td>67.4</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">22</td><td><a href="https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural">CSP-p6 + Mish (multi-scale)</a></td><td>55.2</td><td>72.9</td><td>60.5</td><td>37.6</td><td>59.0</td><td>66.9</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural">Mish: A Self Regularized Non-Monotonic Activation Function</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">23</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P6 with TTA</a></td><td>54.9</td><td>72.6</td><td>60.2</td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">24</td><td><a href="https://paperswithcode.com/paper/simple-copy-paste-is-a-strong-data">Cascade Eff-B7 NAS-FPN (1280)</a></td><td>54.8</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/simple-copy-paste-is-a-strong-data">Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>NAS-FPN</strong></td></tr><tr><td align="center">25</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-32x4d, multi-scale)</a></td><td>54.7</td><td>73.5</td><td>60.1</td><td>37.4</td><td>57.3</td><td>66.4</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">26</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P6 CSP-P6 (single-scale, 32 fps)</a></td><td>54.3</td><td>72.3</td><td>59.5</td><td>36.6</td><td>58.2</td><td>65.5</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">27</td><td><a href="https://paperswithcode.com/paper/rethinking-pre-training-and-self-training">SpineNet-190 (1280, with Self-training on OpenImages, single-scale)</a></td><td>54.3</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/rethinking-pre-training-and-self-training">Rethinking Pre-training and Self-training</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong></td></tr><tr><td align="center">28</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08d (Res2Net-101, DCN, multi-scale)</a></td><td>54.1</td><td>71.6</td><td>59.9</td><td>35.8</td><td>57.2</td><td>67.4</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">29</td><td><a href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet-D7 (single-scale)</a></td><td>53.7</td><td>72.4</td><td></td><td></td><td>57.0</td><td>66.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet: Scalable and Efficient Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">30</td><td><a href="https://paperswithcode.com/paper/probabilistic-anchor-assignment-with-iou">PAA (ResNext-152-32x8d + DCN, multi-scale)</a></td><td>53.5</td><td>71.6</td><td>59.1</td><td>36.0</td><td>56.3</td><td>66.9</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/probabilistic-anchor-assignment-with-iou">Probabilistic Anchor Assignment with IoU Prediction for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">31</td><td><a href="https://paperswithcode.com/paper/location-sensitive-visual-recognition-with">LSNet (Res2Net-101+ DCN, multi-scale)</a></td><td>53.5</td><td>71.1</td><td>59.2</td><td>35.2</td><td>56.4</td><td>65.8</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/location-sensitive-visual-recognition-with">Location-Sensitive Visual Recognition with Cross-IOU Loss</a></td><td></td><td></td><td>2021</td><td><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">32</td><td><a href="https://paperswithcode.com/paper/resnest-split-attention-networks">ResNeSt-200 (multi-scale)</a></td><td>53.3</td><td>72.0</td><td>58.0</td><td>35.1</td><td>56.2</td><td>66.8</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/resnest-split-attention-networks">ResNeSt: Split-Attention Networks</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong></td></tr><tr><td align="center">33</td><td><a href="https://paperswithcode.com/paper/cbnet-a-novel-composite-backbone-network">Cascade Mask R-CNN (Triple-ResNeXt152, multi-scale)</a></td><td>53.3</td><td>71.9</td><td>58.5</td><td>35.5</td><td>55.8</td><td>66.7</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/cbnet-a-novel-composite-backbone-network">CBNet: A Novel Composite Backbone Network Architecture for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">34</td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-32x4d, single-scale)</a></td><td>53.3</td><td>71.6</td><td>58.5</td><td>33.9</td><td>56.5</td><td>66.9</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>single scale</strong></td></tr><tr><td align="center">35</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (Res2Net-101, DCN, multiscale)</a></td><td>53.3</td><td>70.9</td><td>59.2</td><td>35.7</td><td>56.1</td><td>65.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">36</td><td><a href="https://paperswithcode.com/paper/relationnet-bridging-visual-representations">RelationNet++ (ResNeXt-64x4d-101-DCN)</a></td><td>52.7</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/relationnet-bridging-visual-representations">RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">37</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P5 with TTA</a></td><td>52.5</td><td>70.3</td><td>58</td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">38</td><td><a href="https://paperswithcode.com/paper/deformable-detr-deformable-transformers-for-1">Deformable DETR (ResNeXt-101+DCN)</a></td><td>52.3</td><td>71.9</td><td>58.1</td><td>34.4</td><td>54.4</td><td>65.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/deformable-detr-deformable-transformers-for-1">Deformable DETR: Deformable Transformers for End-to-End Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">39</td><td><a href="https://paperswithcode.com/paper/global-context-networks">GCNet (ResNeXt-101 + DCN + cascade + GC r4)</a></td><td>52.3</td><td>70.9</td><td>56.9</td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/global-context-networks">Global Context Networks</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong><br><strong>GCN</strong></td></tr><tr><td align="center">40</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-190, 1280x1280)</a></td><td>52.1</td><td>71.8</td><td>56.5</td><td>35.4</td><td>55</td><td>63.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">41</td><td><a href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints v2 (ResNeXt-101, DCN, multi-scale)</a></td><td>52.1</td><td>70.1</td><td>57.5</td><td>34.5</td><td>54.6</td><td>63.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints V2: Verification Meets Regression for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong>;<br><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td align="center"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td align="center">42</td><td><a href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">AC-FPN Cascade R-CNN (X-152-32x8d-FPN-IN5k, multi scale, only CEM)</a></td><td>51.9</td><td>70.4</td><td>57</td><td>34.2</td><td>54.8</td><td>64.7</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">Attention-guided Context Feature Pyramid Network for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>FPN</strong></td></tr><tr><td align="center">43</td><td><a href="https://paperswithcode.com/paper/ota-optimal-transport-assignment-for-object">OTA (ResNeXt-101+DCN, multiscale)</a></td><td>51.5</td><td>68.6</td><td>57.1</td><td>34.1</td><td>53.7</td><td>64.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/ota-optimal-transport-assignment-for-object">OTA: Optimal Transport Assignment for Object Detection</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">44</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08d (Res2Net-101, DCN, single-scale)</a></td><td>51.3</td><td>70.0</td><td>55.8</td><td>31.7</td><td>55.3</td><td>64.9</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">45</td><td><a href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">TSD (SENet154-DCN,multi-scale)</a></td><td>51.2</td><td>71.9</td><td>56.0</td><td>33.8</td><td>54.8</td><td>64.2</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">Revisiting the Sibling Head in Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">46</td><td><a href="https://paperswithcode.com/paper/yolox-exceeding-yolo-series-in-2021">YOLOX-X (Modified CSP v5)</a></td><td>51.2</td><td>69.6</td><td>55.7</td><td>31.2</td><td>56.1</td><td>66.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/yolox-exceeding-yolo-series-in-2021">YOLOX: Exceeding YOLO Series in 2021</a></td><td></td><td></td><td>2021</td><td><strong>YOLO</strong></td></tr><tr><td align="center">47</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-143, 1280x1280)</a></td><td>50.7</td><td>70.4</td><td>54.9</td><td>33.6</td><td>53.9</td><td>62.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">48</td><td><a href="https://paperswithcode.com/paper/bridging-the-gap-between-anchor-based-and">ATSS (ResNetXt-64x4d-101+DCN,multi-scale)</a></td><td>50.7</td><td>68.9</td><td>56.3</td><td>33.2</td><td>52.9</td><td>62.4</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/bridging-the-gap-between-anchor-based-and">Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">49</td><td><a href="https://paperswithcode.com/paper/learning-data-augmentation-strategies-for">NAS-FPN (AmoebaNet-D, learned aug)</a></td><td>50.7</td><td></td><td></td><td>34.2</td><td>55.5</td><td>64.5</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/learning-data-augmentation-strategies-for">Learning Data Augmentation Strategies for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">50</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (Res2Net-101, DCN)</a></td><td>50.6</td><td>69</td><td>55.3</td><td>31.3</td><td>54.3</td><td>63.5</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>DCN</strong></td></tr><tr><td align="center">51</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, DCN, multiscale test)</a></td><td>50.2</td><td>70.3</td><td>53.9</td><td>32.0</td><td>53.1</td><td>63.0</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">52</td><td><a href="https://paperswithcode.com/paper/scale-equalizing-pyramid-convolution-for">FreeAnchor + SEPC (DCN, ResNext-101-64x4d)</a></td><td>50.1</td><td>69.8</td><td>54.3</td><td>31.3</td><td>53.3</td><td>63.7</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/scale-equalizing-pyramid-convolution-for">Scale-Equalizing Pyramid Convolution for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">53</td><td><a href="https://paperswithcode.com/paper/d2det-towards-high-quality-object-detection">D2Det (ResNet-101-DCN, multi-scale test)</a></td><td>50.1</td><td>69.4</td><td>54.9</td><td>32.7</td><td>52.7</td><td>62.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/d2det-towards-high-quality-object-detection">D2Det: Towards High Quality Object Detection and Instance Segmentation</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">54</td><td><a href="https://paperswithcode.com/paper/dynamic-r-cnn-towards-high-quality-object">Dynamic R-CNN (ResNet-101-DCN, multi-scale)</a></td><td>50.1</td><td>68.3</td><td>55.6</td><td>32.8</td><td>53.0</td><td>61.2</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/dynamic-r-cnn-towards-high-quality-object">Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">55</td><td><a href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">TSD (ResNet-101-Deformable, Image Pyramid)</a></td><td>49.4</td><td>69.6</td><td>54.4</td><td>32.7</td><td>52.5</td><td>61.0</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">Revisiting the Sibling Head in Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>ResNet</strong></td></tr><tr><td align="center">56</td><td><a href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints v2 (ResNeXt-101, DCN)</a></td><td>49.4</td><td>68.9</td><td>53.4</td><td>30.3</td><td>52.1</td><td>62.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints V2: Verification Meets Regression for Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">57</td><td><a href="https://paperswithcode.com/paper/corner-proposal-network-for-anchor-free-two">CPNDet (Hourglass-104, multi-scale)</a></td><td>49.2</td><td>67.3</td><td>53.7</td><td>31.0</td><td>51.9</td><td>62.4</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/corner-proposal-network-for-anchor-free-two">Corner Proposal Network for Anchor-free, Two-stage Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong></td></tr><tr><td align="center">58</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNeXt-101, 32x4d, DCN)</a></td><td>49</td><td>67.6</td><td>53.5</td><td>29.7</td><td>52.4</td><td>61.4</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">59</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, DCN, single scale)</a></td><td>48.9</td><td>69.3</td><td>52.5</td><td>30.8</td><td>51.5</td><td>62.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>single scale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">60</td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08 (Res2Net-50, DCN, single-scale)</a></td><td>48.8</td><td>67.5</td><td>53.0</td><td>30.1</td><td>52.3</td><td>61.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td><td></td><td></td><td>2021</td><td><strong>single scale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">61</td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (ResNet101, single scale)</a></td><td>48.7</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td><td></td><td></td><td>2021</td><td><strong>Transformer</strong><br><strong>single scale</strong></td></tr><tr><td align="center">62</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-96, 1024x1024)</a></td><td>48.6</td><td>68.4</td><td>52.5</td><td>32</td><td>52.3</td><td>62</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">63</td><td><a href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">TridentNet (ResNet-101-Deformable, Image Pyramid)</a></td><td>48.4</td><td>69.7</td><td>53.5</td><td>31.8</td><td>51.3</td><td>60.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">Scale-Aware Trident Networks for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">64</td><td><a href="https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze">GCNet (ResNeXt-101 + DCN + cascade + GC r4)</a></td><td>48.4</td><td>67.6</td><td>52.7</td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>DCN</strong><br><strong>GCN</strong></td></tr><tr><td align="center">65</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-101-DCN)</a></td><td>48.3</td><td>66.5</td><td>52.8</td><td>28.8</td><td>51.9</td><td>60.7</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">66</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-learning-qualified-and">GFL (X-101-32x4d-DCN, single-scale)</a></td><td>48.2</td><td>67.4</td><td>52.6</td><td>29.2</td><td>51.7</td><td>60.2</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-learning-qualified-and">Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>single scale</strong><br><strong>DCN</strong></td></tr><tr><td align="center">67</td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet101-FPN-3x, single-scale)</a></td><td>48.1</td><td></td><td></td><td>28.7</td><td>50.4</td><td>61.5</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">68</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, single scale)</a></td><td>47.8</td><td>68.4</td><td>51.1</td><td>30.2</td><td>50.8</td><td>59.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>single scale</strong></td></tr><tr><td align="center">69</td><td><a href="https://paperswithcode.com/paper/matrix-nets-a-new-deep-architecture-for">MatrixNet Corners (ResNet-152, multi-scale)</a></td><td>47.8</td><td>66.2</td><td>52.3</td><td>29.7</td><td>50.4</td><td>60.7</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/matrix-nets-a-new-deep-architecture-for">Matrix Nets: A New Deep Architecture for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">70</td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (ResNet50, single scale)</a></td><td>47.8</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td><td></td><td></td><td>2021</td><td><strong>Transformer</strong><br><strong>single scale</strong></td></tr><tr><td align="center">71</td><td><a href="https://paperswithcode.com/paper/soft-anchor-point-object-detection">SAPD (ResNeXt-101, single-scale)</a></td><td>47.4</td><td>67.4</td><td>51.1</td><td>28.1</td><td>50.3</td><td>61.5</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/soft-anchor-point-object-detection">Soft Anchor-Point Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>single scale</strong></td></tr><tr><td align="center">72</td><td><a href="https://paperswithcode.com/paper/path-aggregation-network-for-instance">PANet (ResNeXt-101, multi-scale)</a></td><td>47.4</td><td>67.2</td><td>51.8</td><td>30.1</td><td>51.7</td><td>60.0</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/path-aggregation-network-for-instance">Path Aggregation Network for Instance Segmentation</a></td><td></td><td></td><td>2018</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">73</td><td><a href="https://paperswithcode.com/paper/190807919">HTC (HRNetV2p-W48)</a></td><td>47.3</td><td>65.9</td><td>51.2</td><td>28.0</td><td>49.7</td><td>59.8</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">74</td><td><a href="https://paperswithcode.com/paper/hybrid-task-cascade-for-instance-segmentation">HTC (ResNeXt-101-FPN)</a></td><td>47.1</td><td>63.9</td><td>44.7</td><td>22.8</td><td>43.9</td><td>54.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/hybrid-task-cascade-for-instance-segmentation">Hybrid Task Cascade for Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">75</td><td><a href="https://paperswithcode.com/paper/centernet-object-detection-with-keypoint">CenterNet511 (Hourglass-104, multi-scale)</a></td><td>47.0</td><td>64.5</td><td>50.7</td><td>28.9</td><td>49.9</td><td>58.9</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/centernet-object-detection-with-keypoint">CenterNet: Keypoint Triplets for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">76</td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNeXt101, multi-scale)</a></td><td>47.0</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">77</td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet50-FPN-3x)</a></td><td>46.8</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td><td></td><td></td><td>2021</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">78</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49, 896x896)</a></td><td>46.7</td><td>66.3</td><td>50.6</td><td>29.1</td><td>50.1</td><td>61.7</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">79</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101-DCN, multi-scale)</a></td><td>46.5</td><td>67.4</td><td>50.9</td><td>30.3</td><td>49.7</td><td>57.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">80</td><td><a href="https://paperswithcode.com/paper/houghnet-integrating-near-and-long-range">HoughNet (MS)</a></td><td>46.4</td><td>65.1</td><td>50.7</td><td>29.1</td><td>48.5</td><td>58.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/houghnet-integrating-near-and-long-range">HoughNet: Integrating near and long-range evidence for bottom-up object detection</a></td><td></td><td></td><td>2020</td><td><strong>multiscale</strong></td></tr><tr><td align="center">81</td><td><a href="https://paperswithcode.com/paper/reducing-label-noise-in-anchor-free-object">PPDet (ResNeXt-101-FPN, multiscale)</a></td><td>46.3</td><td>64.8</td><td>51.6</td><td>31.4</td><td>49.9</td><td>56.4</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/reducing-label-noise-in-anchor-free-object">Reducing Label Noise in Anchor-Free Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>FPN</strong></td></tr><tr><td align="center">82</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-101)</a></td><td>46.2</td><td>64.3</td><td>50.5</td><td>27.8</td><td>49.9</td><td>57</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNet</strong></td></tr><tr><td align="center">83</td><td><a href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER (ResNet-101)</a></td><td>46.1</td><td>67.0</td><td>51.6</td><td>29.6</td><td>48.9</td><td>58.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER: Efficient Multi-Scale Training</a></td><td></td><td></td><td>2018</td><td><strong>ResNet</strong></td></tr><tr><td align="center">84</td><td><a href="https://paperswithcode.com/paper/190807919">Mask R-CNN (HRNetV2p-W48 + cascade)</a></td><td>46.1</td><td>64.0</td><td>50.3</td><td>27.1</td><td>48.6</td><td>58.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">85</td><td><a href="https://paperswithcode.com/paper/deformable-convnets-v2-more-deformable-better">DCNv2 (ResNet-101, multi-scale)</a></td><td>46.0</td><td>67.9</td><td>50.8</td><td>27.8</td><td>49.1</td><td>59.5</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/deformable-convnets-v2-more-deformable-better">Deformable ConvNets v2: More Deformable, Better Results</a></td><td></td><td></td><td>2018</td><td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">86</td><td><a href="https://paperswithcode.com/paper/localization-uncertainty-estimation-for">Gaussian-FCOS</a></td><td>46</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/localization-uncertainty-estimation-for">Localization Uncertainty Estimation for Anchor-Free Object Detection</a></td><td></td><td></td><td>2020</td><td></td></tr><tr><td align="center">87</td><td><a href="https://paperswithcode.com/paper/instaboost-boosting-instance-segmentation-via">Cascade R-CNN-FPN (ResNet-101, map-guided)</a></td><td>45.9</td><td>64.2</td><td>50</td><td>26.3</td><td>49</td><td>58.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/instaboost-boosting-instance-segmentation-via">InstaBoost: Boosting Instance Segmentation via Probability Map Guided Copy-Pasting</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">88</td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNeXt101, single-scale)</a></td><td>45.9</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>single scale</strong></td></tr><tr><td align="center">89</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask+VoVNetV2-99 (single-scale)</a></td><td>45.8</td><td>64.5</td><td></td><td>27.8</td><td>48.3</td><td>57.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">90</td><td><a href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">D-RFCN + SNIP (DPN-98 with flip, multi-scale)</a></td><td>45.7</td><td>67.3</td><td>51.1</td><td>29.3</td><td>48.8</td><td>57.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">An Analysis of Scale Invariance in Object Detection - SNIP</a></td><td></td><td></td><td>2017</td><td><strong>multiscale</strong></td></tr><tr><td align="center">91</td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4 (CD53)</a></td><td>45.5</td><td>64.1</td><td>49.5</td><td>27</td><td>49</td><td>56.7</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">92</td><td><a href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO (608x608)</a></td><td>45.2</td><td>65.2</td><td>49.9</td><td>26.3</td><td>47.8</td><td>57.2</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>YOLO</strong></td></tr><tr><td align="center">93</td><td><a href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">AC-FPN Cascade R-CNN (ResNet-101, single scale)</a></td><td>45</td><td>64.4</td><td>49</td><td>26.9</td><td>47.7</td><td>56.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">Attention-guided Context Feature Pyramid Network for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong><br><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">94</td><td><a href="https://paperswithcode.com/paper/freeanchor-learning-to-match-anchors-for">FreeAnchor (ResNeXt-101)</a></td><td>44.8</td><td>64.3</td><td>48.4</td><td>27</td><td>47.9</td><td>56</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/freeanchor-learning-to-match-anchors-for">FreeAnchor: Learning to Match Anchors for Visual Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">95</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-64x4d-101-FPN 4 + improvements)</a></td><td>44.7</td><td>64.1</td><td>48.4</td><td>27.6</td><td>47.5</td><td>55.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">96</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask+VoVNet2-57 (single-scale)</a></td><td>44.7</td><td>63.1</td><td>48.6</td><td>27.1</td><td></td><td>55.9</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">97</td><td><a href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">FSAF (ResNeXt-101, multi-scale)</a></td><td>44.6</td><td>65.2</td><td>48.6</td><td>29.7</td><td>47.1</td><td>54.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>multiscale</strong></td></tr><tr><td align="center">98</td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101, DCN, 500 scale)</a></td><td>44.6</td><td>65.0</td><td>47.5</td><td>24.6</td><td>48.1</td><td>58.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNeXt</strong><br><strong>DCN</strong></td></tr><tr><td align="center">99</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask + X-101-32x8d (single-scale)</a></td><td>44.6</td><td>63.4</td><td>48.4</td><td></td><td>47.2</td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">100</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49, 640x640)</a></td><td>44.3</td><td>63.8</td><td>47.6</td><td>25.9</td><td>47.7</td><td>61.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">101</td><td><a href="https://paperswithcode.com/paper/you-only-look-one-level-feature">YOLOF-DC5</a></td><td>44.3</td><td>62.9</td><td>47.5</td><td>24.0</td><td>48.5</td><td>60.4</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/you-only-look-one-level-feature">You Only Look One-level Feature</a></td><td></td><td></td><td>2021</td><td><strong>YOLO</strong></td></tr><tr><td align="center">102</td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-50)</a></td><td>44.3</td><td>62.3</td><td>48.5</td><td>26.8</td><td>47.7</td><td>54.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>ResNet</strong></td></tr><tr><td align="center">103</td><td><a href="https://paperswithcode.com/paper/feature-intertwiner-for-object-detection-1">InterNet (ResNet-101-FPN, multi-scale)</a></td><td>44.2</td><td>67.5</td><td>51.1</td><td>27.2</td><td>50.3</td><td>57.7</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/feature-intertwiner-for-object-detection-1">Feature Intertwiner for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong><br><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">104</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (VGG-16, multi-scale)</a></td><td>44.2</td><td>64.6</td><td>49.3</td><td>29.2</td><td>47.9</td><td>55.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td><td></td><td></td><td>2018</td><td><strong>multiscale</strong></td></tr><tr><td align="center">105</td><td><a href="https://paperswithcode.com/paper/lip-local-importance-based-pooling">Faster R-CNN (LIP-ResNet-101-MD w FPN)</a></td><td>43.9</td><td>65.7</td><td>48.1</td><td>25.4</td><td>46.7</td><td>56.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/lip-local-importance-based-pooling">LIP: Local Importance-based Pooling</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">106</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (ResNet-101, multi-scale)</a></td><td>43.9</td><td>64.4</td><td>48</td><td>29.6</td><td>49.6</td><td>54.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td><td></td><td></td><td>2018</td><td><strong>multiscale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">107</td><td><a href="https://paperswithcode.com/paper/learning-spatial-fusion-for-single-shot">YOLOv3 @800 + ASFF* (Darknet-53)</a></td><td>43.9</td><td>64.1</td><td>49.2</td><td>27.0</td><td>46.6</td><td>53.4</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/learning-spatial-fusion-for-single-shot">Learning Spatial Fusion for Single-Shot Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>YOLO</strong></td></tr><tr><td align="center">108</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td><td>43.9</td><td>63.5</td><td>47.7</td><td>26.8</td><td>46.9</td><td>55.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">109</td><td><a href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">ExtremeNet (Hourglass-104, multi-scale)</a></td><td>43.7</td><td>60.5</td><td>47.0</td><td>24.1</td><td>46.9</td><td>57.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">Bottom-up Object Detection by Grouping Extreme and Center Points</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">110</td><td><a href="https://paperswithcode.com/paper/yolov4-optimal-speed-and-accuracy-of-object">YOLOv4-608</a></td><td>43.5</td><td>65.7</td><td>47.3</td><td>26.7</td><td>46.7</td><td>53.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/yolov4-optimal-speed-and-accuracy-of-object">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></td><td></td><td></td><td>2020</td><td><strong>single scale</strong><br><strong>YOLO</strong></td></tr><tr><td align="center">111</td><td><a href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER (ResNet-50)</a></td><td>43.5</td><td>65.0</td><td>48.6</td><td>26.1</td><td>46.3</td><td>56.0</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER: Efficient Multi-Scale Training</a></td><td></td><td></td><td>2018</td><td><strong>ResNet</strong></td></tr><tr><td align="center">112</td><td><a href="https://paperswithcode.com/paper/190807919">CenterNet (HRNetV2-W48)</a></td><td>43.5</td><td></td><td>46.5</td><td>22.2</td><td></td><td>57.8</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">113</td><td><a href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">D-RFCN + SNIP (ResNet-101, multi-scale)</a></td><td>43.4</td><td>65.5</td><td>48.4</td><td>27.2</td><td>46.5</td><td>54.9</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">An Analysis of Scale Invariance in Object Detection - SNIP</a></td><td></td><td></td><td>2017</td><td><strong>multiscale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">114</td><td><a href="https://paperswithcode.com/paper/grid-r-cnn">Grid R-CNN (ResNeXt-101-FPN)</a></td><td>43.2</td><td>63.0</td><td>46.6</td><td>25.1</td><td>46.5</td><td>55.2</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/grid-r-cnn">Grid R-CNN</a></td><td></td><td></td><td>2018</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">115</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-101-64x4d-FPN)</a></td><td>43.2</td><td>62.8</td><td>46.6</td><td>26.5</td><td>46.2</td><td>53.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">116</td><td><a href="https://paperswithcode.com/paper/190408900">CornerNet-Saccade (Hourglass-104, multi-scale)</a></td><td>43.2</td><td></td><td></td><td>24.4</td><td>44.6</td><td>57.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/190408900">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">117</td><td><a href="https://paperswithcode.com/paper/libra-r-cnn-towards-balanced-learning-for">Libra R-CNN (ResNeXt-101-FPN)</a></td><td>43.0</td><td>64</td><td>47</td><td>25.3</td><td>45.6</td><td>54.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/libra-r-cnn-towards-balanced-learning-for">Libra R-CNN: Towards Balanced Learning for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">118</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101-DCN)</a></td><td>42.8</td><td>65.0</td><td>46.3</td><td>24.9</td><td>46.2</td><td>54.7</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>DCN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">119</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet-49 (640, RetinaNet, single-scale)</a></td><td>42.8</td><td>62.3</td><td>46.1</td><td>23.7</td><td>45.2</td><td>57.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">120</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-101-FPN+, cascade)</a></td><td>42.8</td><td>62.1</td><td>46.3</td><td>23.7</td><td>45.5</td><td>55.2</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">121</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-high-quality-object-detection">Cascade R-CNN</a></td><td>42.8</td><td>62.1</td><td>46.3</td><td>23.7</td><td>45.5</td><td>55.2</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-high-quality-object-detection">Cascade R-CNN: High Quality Object Detection and Instance Segmentation</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">122</td><td><a href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">TridentNet (ResNet-101)</a></td><td>42.7</td><td>63.6</td><td>46.5</td><td>23.9</td><td>46.6</td><td>56.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">Scale-Aware Trident Networks for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">123</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-32x8d-101-FPN)</a></td><td>42.7</td><td>62.2</td><td>46.1</td><td>26.0</td><td>45.6</td><td>52.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">124</td><td><a href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask (ResNeXt-101-FPN-GN)</a></td><td>42.6</td><td>62.5</td><td>46.0</td><td>24.8</td><td>45.6</td><td>53.8</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">125</td><td><a href="https://paperswithcode.com/paper/tood-task-aligned-one-stage-object-detection">TAL + TAP</a></td><td>42.5</td><td>60.3</td><td>46.4</td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/tood-task-aligned-one-stage-object-detection">TOOD: Task-aligned One-stage Object Detection</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">126</td><td><a href="https://paperswithcode.com/paper/190807919">Faster R-CNN (HRNetV2p-W48)</a></td><td>42.4</td><td>63.6</td><td>46.4</td><td>24.9</td><td>44.6</td><td>53.0</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">127</td><td><a href="https://paperswithcode.com/paper/hierarchical-shot-detector">HSD (Rest101, 768x768, single-scale test)</a></td><td>42.3</td><td>61.2</td><td>46.9</td><td>22.8</td><td>47.3</td><td>55.9</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/hierarchical-shot-detector">Hierarchical Shot Detector</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">128</td><td><a href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet511 (Hourglass-104, multi-scale)</a></td><td>42.1</td><td>57.8</td><td>45.3</td><td>20.8</td><td>44.8</td><td>56.7</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet: Detecting Objects as Paired Keypoints</a></td><td></td><td></td><td>2018</td><td><strong>multiscale</strong></td></tr><tr><td align="center">129</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td><td>42.1</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">130</td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (HRNet-W32-5l)</a></td><td>42.0</td><td>60.4</td><td>45.3</td><td>25.4</td><td>45.0</td><td>51.0</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">131</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512+ (ResNet-101)</a></td><td>41.8</td><td>62.9</td><td>45.7</td><td>25.6</td><td>45.1</td><td>54.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>ResNet</strong></td></tr><tr><td align="center">132</td><td><a href="https://paperswithcode.com/paper/gradient-harmonized-single-stage-detector">GHM-C + GHM-R (RetinaNet-FPN-ResNeXt-101)</a></td><td>41.6</td><td>62.8</td><td>44.2</td><td>22.3</td><td>45.1</td><td>55.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/gradient-harmonized-single-stage-detector">Gradient Harmonized Single-stage Detector</a></td><td></td><td></td><td>2018</td><td><strong>FPN</strong></td></tr><tr><td align="center">133</td><td><a href="https://paperswithcode.com/paper/objects-as-points">CenterNet-DLA (DLA-34, multi-scale)</a></td><td>41.6</td><td></td><td></td><td>21.5</td><td>43.9</td><td>56.0</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/objects-as-points">Objects as Points</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">134</td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49S, 640x640)</a></td><td>41.5</td><td>60.5</td><td>44.6</td><td>23.3</td><td>45</td><td>58</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">135</td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101)</a></td><td>41</td><td>62.9</td><td>44.3</td><td>23.6</td><td>44.1</td><td>51.7</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">136</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (VGG-16, single-scale)</a></td><td>41.0</td><td>59.7</td><td>45</td><td>22.1</td><td>46.5</td><td>53.8</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td><td></td><td></td><td>2018</td><td><strong>single scale</strong></td></tr><tr><td align="center">137</td><td><a href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">FSAF (ResNet-101, single-scale)</a></td><td>40.9</td><td>61.5</td><td>44</td><td>24</td><td>44.2</td><td>51.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">138</td><td><a href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">RetinaNet (ResNeXt-101-FPN)</a></td><td>40.8</td><td>61.1</td><td>44.1</td><td>24.1</td><td>44.2</td><td>51.2</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">Focal Loss for Dense Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">139</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-50-FPN+, cascade)</a></td><td>40.6</td><td>59.9</td><td>44</td><td>22.6</td><td>42.7</td><td>52.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">140</td><td><a href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Faster R-CNN (Cascade RPN)</a></td><td>40.6</td><td>58.9</td><td>44.5</td><td>22.0</td><td>42.8</td><td>52.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">141</td><td><a href="https://paperswithcode.com/paper/deformable-kernels-adapting-effective">ResNet-50-DW-DPN (Deformable Kernels)</a></td><td>40.6</td><td></td><td></td><td>24.6</td><td>43.9</td><td>53.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/deformable-kernels-adapting-effective">Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">142</td><td><a href="https://paperswithcode.com/paper/acquisition-of-localization-confidence-for">IoU-Net</a></td><td>40.6</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/acquisition-of-localization-confidence-for">Acquisition of Localization Confidence for Accurate Object Detection</a></td><td></td><td></td><td>2018</td><td></td></tr><tr><td align="center">143</td><td><a href="https://paperswithcode.com/paper/190807919">FCOS (HRNetV2p-W48)</a></td><td>40.5</td><td>59.3</td><td></td><td>23.4</td><td>42.6</td><td>51.0</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">144</td><td><a href="https://paperswithcode.com/paper/softer-nms-rethinking-bounding-box-regression">ResNet-50-FPN Mask R-CNN + KL Loss + var voting + soft-NMS</a></td><td>40.4</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/softer-nms-rethinking-bounding-box-regression">Bounding Box Regression with Uncertainty for Accurate Object Detection</a></td><td></td><td></td><td>2018</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">145</td><td><a href="https://paperswithcode.com/paper/rdsnet-a-new-deep-architecture-for-reciprocal">RDSNet (ResNet-101, RetinaNet, mask, MBRM)</a></td><td>40.3</td><td>60.1</td><td>43</td><td>22.1</td><td>43.5</td><td>51.5</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/rdsnet-a-new-deep-architecture-for-reciprocal">RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">146</td><td><a href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">ExtremeNet (Hourglass-104, single-scale)</a></td><td>40.2</td><td>55.5</td><td>43.2</td><td>20.4</td><td>43.2</td><td>53.1</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">Bottom-up Object Detection by Grouping Extreme and Center Points</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr><tr><td align="center">147</td><td><a href="https://paperswithcode.com/paper/cross-iteration-batch-normalization">Mask R-CNN (ResNet-101-FPN, CBN)</a></td><td>40.1</td><td>60.5</td><td>44.1</td><td>35.8</td><td>57.3</td><td>38.5</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/cross-iteration-batch-normalization">Cross-Iteration Batch Normalization</a></td><td></td><td></td><td>2020</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">148</td><td><a href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Fast R-CNN (Cascade RPN)</a></td><td>40.1</td><td>59.4</td><td>43.8</td><td>22.1</td><td>42.4</td><td>51.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">149</td><td><a href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN (ResNeXt-101-FPN)</a></td><td>39.8</td><td>62.3</td><td>43.4</td><td>22.1</td><td>43.2</td><td>51.2</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN</a></td><td></td><td></td><td>2017</td><td><strong>ResNeXt</strong><br><strong>FPN</strong></td></tr><tr><td align="center">150</td><td><a href="https://paperswithcode.com/paper/region-proposal-by-guided-anchoring">GA-Faster-RCNN</a></td><td>39.8</td><td>59.2</td><td>43.5</td><td>21.8</td><td>42.6</td><td>50.7</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/region-proposal-by-guided-anchoring">Region Proposal by Guided Anchoring</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">151</td><td><a href="https://paperswithcode.com/paper/chainercv-a-library-for-deep-learning-in">FPN (ResNet101 backbone)</a></td><td>39.5</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/chainercv-a-library-for-deep-learning-in">ChainerCV: a Library for Deep Learning in Computer Vision</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">152</td><td><a href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask (ResNet-50-FPN)</a></td><td>39.4</td><td>58.6</td><td>42.3</td><td>21.9</td><td>42.0</td><td>51.0</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">153</td><td><a href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO (320x320)</a></td><td>39.3</td><td>59.3</td><td>42.7</td><td>16.7</td><td>41.4</td><td>57.8</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>YOLO</strong></td></tr><tr><td align="center">154</td><td><a href="https://paperswithcode.com/paper/190409925">AA-ResNet-10 + RetinaNet</a></td><td>39.2</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/190409925">Attention Augmented Convolutional Networks</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">155</td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNet50, single-scale)</a></td><td>39.2</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">156</td><td><a href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">RetinaNet (ResNet-101-FPN)</a></td><td>39.1</td><td>59.1</td><td>42.3</td><td>21.8</td><td>42.7</td><td>50.2</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">Focal Loss for Dense Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">157</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-101-FPN+)</a></td><td>38.8</td><td>61.1</td><td>41.9</td><td>21.3</td><td>41.8</td><td>49.8</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">158</td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (ResNet-101, single-scale)</a></td><td>38.8</td><td>59.4</td><td>41.7</td><td>20.5</td><td>43.9</td><td>53.4</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td><td></td><td></td><td>2018</td><td><strong>single scale</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">159</td><td><a href="https://paperswithcode.com/paper/saccadenet-a-fast-and-accurate-object">SaccadeNet (DLA-34-DCN)</a></td><td>38.5</td><td>55.6</td><td>41.4</td><td>19.2</td><td>42.1</td><td>50.6</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/saccadenet-a-fast-and-accurate-object">SaccadeNet: A Fast and Accurate Object Detector</a></td><td></td><td></td><td>2020</td><td><strong>DCN</strong></td></tr><tr><td align="center">160</td><td><a href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN (ResNet-101-FPN)</a></td><td>38.2</td><td>60.3</td><td>41.7</td><td>20.1</td><td>41.1</td><td>50.2</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong><br><strong>ResNet</strong></td></tr><tr><td align="center">161</td><td><a href="https://paperswithcode.com/paper/segmentation-is-all-you-need">WSMA-Seg</a></td><td>38.1</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/segmentation-is-all-you-need">Segmentation is All You Need</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">162</td><td><a href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Faster R-CNN + FPN + CGD</a></td><td>37.9</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Compact Global Descriptor for Neural Networks</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">163</td><td><a href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet511 (Hourglass-52, single-scale)</a></td><td>37.8</td><td>53.7</td><td>40.1</td><td>17.0</td><td>39.0</td><td>50.5</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet: Detecting Objects as Paired Keypoints</a></td><td></td><td></td><td>2018</td><td><strong>single scale</strong></td></tr><tr><td align="center">164</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512+ (VGG-16)</a></td><td>37.6</td><td>58.7</td><td>40.8</td><td>22.7</td><td>40.3</td><td>48.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">165</td><td><a href="https://paperswithcode.com/paper/deformable-convolutional-networks">DeformConv-R-FCN (Aligned-Inception-ResNet)</a></td><td>37.5</td><td>58.0</td><td></td><td>19.4</td><td>40.1</td><td>52.5</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/deformable-convolutional-networks">Deformable Convolutional Networks</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">166</td><td><a href="https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data">Faster R-CNN (ImageNet+300M)</a></td><td>37.4</td><td>58</td><td>40.1</td><td>17.5</td><td>41.1</td><td>51.2</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">167</td><td><a href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">Mask R-CNN (Bottleneck-injected ResNet-50, FPN)</a></td><td>36.9</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation</a></td><td></td><td></td><td>2020</td><td><strong>FPN</strong><br>！！<strong>ResNet</strong></td></tr><tr><td align="center">168</td><td><a href="https://paperswithcode.com/paper/beyond-skip-connections-top-down-modulation">Faster R-CNN + TDM</a></td><td>36.8</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/beyond-skip-connections-top-down-modulation">Beyond Skip Connections: Top-Down Modulation for Object Detection</a></td><td></td><td></td><td>2016</td><td></td></tr><tr><td align="center">169</td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-50-FPN+)</a></td><td>36.5</td><td>59</td><td>39.2</td><td>20.3</td><td>38.8</td><td>46.4</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>FPN</strong>;<br><strong>ResNet</strong></td></tr><tr><td align="center">170</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512 (ResNet-101)</a></td><td>36.4</td><td>57.5</td><td>39.5</td><td>16.6</td><td>39.9</td><td>51.4</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td><td></td><td></td><td>2017</td><td><strong>ResNet</strong></td></tr><tr><td align="center">171</td><td><a href="https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection">Faster R-CNN + FPN</a></td><td>36.2</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection">Feature Pyramid Networks for Object Detection</a></td><td></td><td></td><td>2016</td><td><strong>FPN</strong></td></tr><tr><td align="center">172</td><td><a href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">Faster R-CNN (Bottleneck-injected ResNet-50 and FPN)</a></td><td>35.9</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation</a></td><td></td><td></td><td>2020</td><td><strong>FPN</strong>;<br><strong>ResNet</strong></td></tr><tr><td align="center">173</td><td><a href="https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition">Faster R-CNN (box refinement, context, multi-scale testing)</a></td><td>34.9</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition">Deep Residual Learning for Image Recognition</a></td><td></td><td></td><td>2015</td><td><strong>multiscale</strong></td></tr><tr><td align="center">174</td><td><a href="https://paperswithcode.com/paper/speedaccuracy-trade-offs-for-modern">Faster R-CNN</a></td><td>34.7</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/speedaccuracy-trade-offs-for-modern">Speed/accuracy trade-offs for modern convolutional object detectors</a></td><td></td><td></td><td>2016</td><td></td></tr><tr><td align="center">175</td><td><a href="https://paperswithcode.com/paper/190408900">CornerNet-Squeeze</a></td><td>34.4</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/190408900">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">176</td><td><a href="https://paperswithcode.com/paper/a-multipath-network-for-object-detection">MultiPath Network</a></td><td>33.2</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/a-multipath-network-for-object-detection">A MultiPath Network for Object Detection</a></td><td></td><td></td><td>2016</td><td></td></tr><tr><td align="center">177</td><td><a href="https://paperswithcode.com/paper/inside-outside-net-detecting-objects-in">ION</a></td><td>33.1</td><td>55.7</td><td>34.6</td><td>14.5</td><td>35.2</td><td>47.2</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/inside-outside-net-detecting-objects-in">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</a></td><td></td><td></td><td>2015</td><td></td></tr><tr><td align="center">178</td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512 (VGG-16)</a></td><td>33</td><td>54.5</td><td>35.5</td><td>16.3</td><td>36.3</td><td>44.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">179</td><td><a href="https://paperswithcode.com/paper/yolov3-an-incremental-improvement">YOLOv3 + Darknet-53</a></td><td>33.0</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/yolov3-an-incremental-improvement">YOLOv3: An Incremental Improvement</a></td><td></td><td></td><td>2018</td><td><strong>YOLO</strong></td></tr><tr><td align="center">180</td><td><a href="https://paperswithcode.com/paper/ssd-single-shot-multibox-detector">SSD512</a></td><td>28.8</td><td>48.5</td><td>30.3</td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/ssd-single-shot-multibox-detector">SSD: Single Shot MultiBox Detector</a></td><td></td><td></td><td>2015</td><td></td></tr><tr><td align="center">181</td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MobileNetV2)</a></td><td>26.1</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">182</td><td><a href="https://paperswithcode.com/paper/espnetv2-a-light-weight-power-efficient-and">ESPNetv2-512</a></td><td>26.0</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/espnetv2-a-light-weight-power-efficient-and">ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network</a></td><td></td><td></td><td>2018</td><td></td></tr><tr><td align="center">183</td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MobileNetV3)</a></td><td>25.5</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">184</td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MNASNet-B1)</a></td><td>24.6</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">185</td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN x0.7 (MobileNetV2)</a></td><td>23.8</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td><td></td><td></td><td>2019</td><td><strong>FPN</strong></td></tr><tr><td align="center">186</td><td><a href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">MobielNet-v1-SSD-300x300+CGD</a></td><td>21.4</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Compact Global Descriptor for Neural Networks</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">187</td><td><a href="https://paperswithcode.com/paper/fast-r-cnn">Fast-RCNN</a></td><td>19.7</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/fast-r-cnn">Fast R-CNN</a></td><td></td><td></td><td>2015</td><td></td></tr><tr><td align="center">188</td><td><a href="https://paperswithcode.com/paper/mobilenets-efficient-convolutional-neural">MobileNet</a></td><td>19.3</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/mobilenets-efficient-convolutional-neural">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></td><td></td><td></td><td>2017</td><td></td></tr><tr><td align="center">189</td><td><a href="https://paperswithcode.com/paper/vision-transformer-with-deformable-attention">DAT-S (RetinaNet)</a></td><td></td><td>69.6</td><td>51.2</td><td>32.3</td><td>51.8</td><td>63.4</td><td>47.9</td><td align="center"></td><td><a href="https://paperswithcode.com/paper/vision-transformer-with-deformable-attention">Vision Transformer with Deformable Attention</a></td><td></td><td></td><td>2022</td><td></td></tr><tr><td align="center">190</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask-VoVNet99 (multi-scale)</a></td><td></td><td>68.3</td><td>53.2</td><td>32.4</td><td></td><td>60.0</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>multiscale</strong></td></tr><tr><td align="center">191</td><td><a href="https://paperswithcode.com/paper/190807919">Mask R-CNN (HRNetV2p-W32 + cascade)</a></td><td></td><td>62.5</td><td>48.6</td><td></td><td></td><td>56.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">192</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td><td></td><td>61.9</td><td>45.2</td><td></td><td>46.8</td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">193</td><td><a href="https://paperswithcode.com/paper/virtex-learning-visual-representations-from">VirTex Mask R-CNN (ResNet-50-FPN)</a></td><td></td><td>61.7</td><td>44.8</td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/virtex-learning-visual-representations-from">VirTex: Learning Visual Representations from Textual Annotations</a></td><td></td><td></td><td>2020</td><td><strong>FPN</strong>;<br><strong>ResNet</strong></td></tr><tr><td align="center">194</td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">Centermask + ResNet101</a></td><td></td><td>61.6</td><td>46.9</td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td><td></td><td></td><td>2019</td><td><strong>ResNet</strong></td></tr><tr><td align="center">195</td><td><a href="https://paperswithcode.com/paper/pafnet-an-efficient-anchor-free-object">PAFNet (ResNet50-vd)</a></td><td></td><td>59.8</td><td>45.3</td><td>22.8</td><td>45.8</td><td>59.2</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/pafnet-an-efficient-anchor-free-object">PAFNet: An Efficient Anchor-Free Object Detector Guidance</a></td><td></td><td></td><td>2021</td><td><strong>ResNet</strong></td></tr><tr><td align="center">196</td><td><a href="https://paperswithcode.com/paper/dctd-deep-conditional-target-densities-for">IoU-Net+EnergyRegression</a></td><td></td><td>58.5</td><td>41.8</td><td></td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/dctd-deep-conditional-target-densities-for">Energy-Based Models for Deep Probabilistic Regression</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">197</td><td><a href="https://paperswithcode.com/paper/190807919">Cascade R-CNN (HRNetV2p-W48)</a></td><td></td><td></td><td>48.6</td><td>26.0</td><td>47.3</td><td>56.3</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td><td></td><td></td><td>2019</td><td></td></tr><tr><td align="center">198</td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet50-FPN-3x, single-scale)</a></td><td></td><td></td><td></td><td>27.8</td><td>48.7</td><td>59.9</td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td><td></td><td></td><td>2021</td><td></td></tr><tr><td align="center">199</td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td><td></td><td></td><td></td><td>24.9</td><td></td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td><td></td><td></td><td>2019</td><td><strong>ResNeXt</strong></td></tr><tr><td align="center">200</td><td><a href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet-D7x (single-scale)</a></td><td></td><td></td><td></td><td></td><td>57.9</td><td></td><td></td><td align="center"></td><td><a href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet: Scalable and Efficient Object Detection</a></td><td></td><td></td><td>2019</td><td><strong>single scale</strong></td></tr></tbody></table>]]></content>
      
      
      
        <tags>
            
            <tag> Object_Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>目标检测综述学习总结</title>
      <link href="/blog/2022/02/15/mu-biao-jian-ce-zong-shu-xue-xi-zong-jie/"/>
      <url>/blog/2022/02/15/mu-biao-jian-ce-zong-shu-xue-xi-zong-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="目标检测算法综述学习总结"><a href="#目标检测算法综述学习总结" class="headerlink" title="目标检测算法综述学习总结"></a>目标检测算法综述学习总结</h1><hr><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>近年来，CNN的飞速发展促进了计算机视觉算法的成熟。本文简要介绍了几种具有代表性的目标检测算法，并根据其优缺点，系统地分析了算法存在的问题、改进方法和未来的发展方向。<br>它一般分为单级检测模型和双级检测模型，基于目标检测过程中是否需要提取候选区域的检测模型进一步任务的检测。在双级检测模型中具有缩放功能算法分为单尺度检测和多尺度检测可以适当地与网络结构集成，从而提高面向小目标的网络模型。同时，在单级anchor检测模型的发展基础上也延伸出anchor-base和anchor-free。可预测的目标检测算法的发展将向我们展示未来。</p><p>关键词:计算机视觉，目标检测，卷积神经网络</p><hr><h2 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h2><p>在当今社会不断发展的今天，计算机视觉技术已经融入到生活的各个方面。目标检测是计算机视觉技术中一项非常基础而又非常重要的任务。目标检测应用于社会安全管理、交通车辆监测、环境污染检测、森林灾害等领域。在预警和国防安全领域有非常突出的应用成果。目标检测的任务主要包括数字图像中单个或多个感兴趣目标的识别和定位。人们对包含目标的训练图像进行处理，提取稳定的、独特的特征或特定的抽象语义信息特征，然后将这些可区分的特征进行匹配或使用分类算法对每个类别赋予置信度进行分类。</p><p>目标检测算法已研究多年。20世纪90年代，出现了许多有效的传统目标检测算法。它们主要是利用传统的特征提取算法提取特征，然后结合模板匹配算法或分类器进行目标识别。然而，由于缺乏强语义信息和复杂的计算，传统算法在发展中遇到了瓶颈。</p><p>2014年，Ross Girshick提出了一种基于卷积网络的目标检测模型RCNN[1]，该模型检测精度高，特异性鲁棒性和泛化能力强，使人们更加重视利用卷积神经网络提取图像的高级语义信息，并提出了许多优秀的卷积神经网络检测模型。</p><hr><h2 id="二、传统的目标检测算法模型"><a href="#二、传统的目标检测算法模型" class="headerlink" title="二、传统的目标检测算法模型"></a>二、传统的目标检测算法模型</h2><p>传统的目标检测训练模型大致可以分为两个步骤</p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zong-shu-xue-xi-zong-jie/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20220215094431932-899141630.png"></p><p>基于图像特征采集的不同模式可分为两大类：</p><p>基于<strong>特征区域</strong>的特征算法模型。(Haar, LBP, HOG特征等)，主要通过选择合适的检测帧或特征模板来获得易于区分的特征向量。</p><p>基于<strong>特征点</strong>的特征算法模型。(SIFT， SURF， ORB feature等)，主要定位复杂场景中一些稳定且独特的特征点，如极值点、明亮区域中的暗点、黑暗区域中的亮点。然后使用具有较高可分辨性的特征点描述符来区分不同的特征点</p><p>传统方法中使用的分类匹配方法很多，可以分为<strong>相似度模型</strong>(K-Nearest Neighbor， Rocchio)概率模型(Bayes)线性模型(SVM)非线性模型(decision tree[9])集成分类器(Adaboost)。</p><hr><h2 id="三、卷积神经网络"><a href="#三、卷积神经网络" class="headerlink" title="三、卷积神经网络"></a>三、卷积神经网络</h2><p>2014年，RCNN卷积网络的提出开启了目标检测发展的新阶段。它的精度和稳定性大大超过了传统的目标检测算法，因此很快被人们所接受。卷积神经网络的检测模型主要分为单级和两级检测模型。不同的是，两阶段检测模型需要训练候选区域网络(region proposal network,RPN)，但增加了计算复杂度，模型难以实现实时检测。单级模型摒弃了这一环节，将目标检测问题转化为回归问题。虽然牺牲了模型的精度，但大大提高了模型的计算速度，并且模型可以实时检测。</p><h3 id="3-1-卷积神经网络检测的过程"><a href="#3-1-卷积神经网络检测的过程" class="headerlink" title="3.1 卷积神经网络检测的过程"></a>3.1 卷积神经网络检测的过程</h3><p>卷积神经网络目标检测与传统检测也有一定的相似之处，可以看作是特征提取和利用特征来识别目标。特征提取网络主干和“detection head(检测头)”</p><p>卷积神经网络利用卷积网络提取图像的高级语义特征，这些特征特征通常是网络的主干，然后对特征映射进行处理，如将一个全连接的网络与softmax或svm连接，形成一个classification head(分类头)来完成分类任务。将核心处理为特征维度，利用位置损失函数进行目标定位。</p><p>网络通过损失函数判断误差，并通过网络的反向梯度传播更新网络权值参数，从而不断减小损失函数的值，提高检测精度。检测网络通过大量的训练数据进行多次计算，可以从这组数据中学习一组最优的权值来预测检测目标</p><h3 id="3-2-卷积神经网络模型的骨干网络"><a href="#3-2-卷积神经网络模型的骨干网络" class="headerlink" title="3.2 卷积神经网络模型的骨干网络"></a>3.2 卷积神经网络模型的骨干网络</h3><p>目前比较著名的骨干网络有:<br>(1998) LeNet-5<br>(2012) AlexNet<br>(2013) ZFNet<br>(2014) GoogLeNet<br>(2014) VGGNet<br>(2015) ResNet<br>(2016) ResNet v2<br>(2017) ReNetXt<br>(2018) DenseNet<br>(2019) VoVNet和VoVNet<br>(2020) VoVNet-v2</p><p>基本上是将图像的图像按照一定的空间位置通过各种卷积核进行乘法和累加，得到下一级特征图像。</p><h2 id="四、卷积神经网络骨干网络的发展"><a href="#四、卷积神经网络骨干网络的发展" class="headerlink" title="四、卷积神经网络骨干网络的发展"></a>四、卷积神经网络骨干网络的发展</h2><h3 id="4-1-LeNet-5"><a href="#4-1-LeNet-5" class="headerlink" title="4.1 LeNet-5"></a>4.1 LeNet-5</h3><p>最早的经典卷积特征骨干网络是Lecun等人在1995年提出的LeNet-5[11]。虽然当时的网络模型比较简单，但在卷积神经网络中已经包含了最基本的卷积池。转换层和全连通层对卷积神经网络的发展起到了指导作用，卷积神经网络主要用于手写识别。</p><h3 id="4-2-AlexNet"><a href="#4-2-AlexNet" class="headerlink" title="4.2 AlexNet"></a>4.2 AlexNet</h3><p>卷积神经网络在2012年开始流行。其中，Alex Krizhevsky提出AlexNet网络[12]的结构总体上与LeNet相似，即先卷积后全连接。但网络更复杂，使用了五层卷积，三层全连接网络，最后的输出层是为1000个通道的softmax。AlexNet使用两个gpu进行计算，大大提高了计算效率。在ILSVRC-2012竞赛中，它获得了前5名的错误率为15.3%。</p><p>为了获得更大的接受野，网络的第一层使用了一个11*11的卷积核，并在每个卷积层添加LRN局部响应归一化以提高精度， 但在2015年，用于大规模图像识别的深度卷积网络。提到LRN基本上是没有用的。</p><h3 id="4-3-ZFNet"><a href="#4-3-ZFNet" class="headerlink" title="4.3 ZFNet"></a>4.3 ZFNet</h3><p>由于人们想要了解卷积神经网络的工作原理，ZFNet[13]在2013年被提出，它提供了一个可视化的网络来了解卷积网络的各个层次。AlexNet的主要改进是在使用deconvnet和visual feature map来可视化它的同时，使用更小的卷积来降低时间复杂度，同时赢得了ILSVRC冠军。通过对神经网络的可视化可以看出，低级网络提取了图像的边缘纹理特征，高级网络提取了图像的抽象特征。该特征具有平移和尺度不变性，但不具有旋转不变性。</p><h3 id="4-4-GoogLeNet"><a href="#4-4-GoogLeNet" class="headerlink" title="4.4 GoogLeNet"></a>4.4 GoogLeNet</h3><p>为了进一步提高神经网络的性能,最直接的方法是增加网络的深度和广度,但它会导致太多的参数增加的计算量,和有限的训练集将导致如梯度扩散或过度拟合的问题。例如，22层的AlexNet有大约6000万个参数。2014年提出的GoogLeNet在相同情况下只有500万个参数。它主要使用卷积的解决方案。GoogLeNe v1将一个5x5的卷积运算分解为两个3x3的卷积运算。当他们获得相同的接受域时，参数减少了2.78倍。GoogLeNe v2除3x3卷积运算分解为1x3和3x1卷积运算。GoogLeNe V3将7<em>7卷积核分解为7</em>1和1*7卷积核，深化了网络的深度，减少了参数。GoogLeNe v4是在GoogLeNe v3的基础上增加残留网络，大大增加了深度。</p><h3 id="4-5-VGGNet"><a href="#4-5-VGGNet" class="headerlink" title="4.5 VGGNet"></a>4.5 VGGNet</h3><p>Karen Simonyan等人在2014年提出的VGGNet[18]相当于AlexNet的网络深化版，由卷积层和全连接层两部分组成。所有激活层使用relu，池化层使用最大池化。其结构简单，特征提取能力强，在ILSVRC2014和2014的分类项目中排名第二在定位项目中排名第一。测试中使用的VGGNet使用了一个1*1的卷积核对全连通层进行改进，成为一个有卷积的全连通层。这克服了传统全连接层需要固定输入尺寸的缺点。因此，采用多尺度训练，训练图像尺度在[256,512]边长范围内随机选取。这种尺度抖动方法可以增强训练集。</p><h3 id="4-6-ResNet-series"><a href="#4-6-ResNet-series" class="headerlink" title="4.6 ResNet series"></a>4.6 ResNet series</h3><p>随着网络深度的增加，获得的特征越来越丰富，但优化效果较差，由于梯度爆炸和消失等原因使得检测精度降低。对于较浅的网络，可以对每一层的输入数据进行归一化，使网络收敛。但深度网络仍存在优化问题。因此，何凯明在2015年提出了ResNet来打破这一瓶颈，主要采用了跳跃连接结构。</p><p>在2016年提出的ResNet v2[20]的基础上，通过改变归一层、池化层和卷积层的顺序，得到一组性能最好的跳变结构(图22)，ReNetXt[21]是在2017年提出的，它借鉴了GoogLeNet的思想，在卷积层的两边增加了1*1的卷积，减少了控制核的数量，参数减少了约三分之二。</p><h3 id="4-7-DenseNet"><a href="#4-7-DenseNet" class="headerlink" title="4.7 DenseNet"></a>4.7 DenseNet</h3><p>以前的卷积网络要么像GoogLeNet一样宽，要么像ResNet一样深。2018年发表的DenseNet[22]作者通过实验发现了两种神经网络的两个特征:</p><ul><li><p>1.去掉中间层后，下一层直接与上一层连接，即神经网络不是递进的层次结构，不需要将相邻层连接起来。</p></li><li><p>ResNet的许多层是在训练过程中随机删除，不会影响算法的收敛性和预测结果，该网络证明了ResNet具有明显的冗余性，网络中的每一层只提取一个几乎没有特征（所谓的残差）。与ResNet相比，DenseNet具有明显的优势，提高了性能，减少了参数。</p></li></ul><h3 id="4-8-SENet"><a href="#4-8-SENet" class="headerlink" title="4.8 SENet"></a>4.8 SENet</h3><p>SENet[23]于2019年发布，针对检测任务，并提出了一种结合注意抑制对当前任务无用特征的思想的信道权值。SE模块主要用于卷积层的权值分配。这种子模块形式使其与其他网络兼容。本文主要应用于ResNet网络。SE模块嵌入在ResNeXt、BN-Inception、Inception-ResNet-v2中，并且已经取得了很大的进展。由此可以看出，SE的增益效应不仅局限于某些特殊的网络结构，而且具有很强的泛化性。</p><h3 id="4-9-EfficientNet"><a href="#4-9-EfficientNet" class="headerlink" title="4.9  EfficientNet"></a>4.9  EfficientNet</h3><p>考虑到以前的网络主要通过单一的宽度(WideResNet和mobilenet)、深度和网络模型的分辨率来提高网络的准确性。EfficientNet网络模型[24]量化了这三个维度之间的关系，并使用一个恒定的比率来简单地增加，以同时平衡网络的三个维度。</p><h3 id="4-10-VoVNet-series"><a href="#4-10-VoVNet-series" class="headerlink" title="4.10  VoVNet series"></a>4.10  VoVNet series</h3><p>2019年提出的VoVNet网络[25]已经完全超越ResNet，可以用作实时目标检测的骨干网。考虑到能量消耗和模型推理速度等因素，优化内存访问成本(输入输出通道数相同时效率最高)和GPU计算效率(GPU处理大张量强，CPU处理小张量强)更为关键。</p><p>同时,在改进的2020 CenterMask文章中,剩余块和eSE模块添加(在原来的SE模块改进中，使用一个FC代替原来的两个FC以减少信息损失)大大增加其性能和VoVNet v2结构形式。与ResNet相比，VoVNet网络具有更强的小目标提取能力，速度和精度都更好。</p><hr><h2 id="五、目标检测的网络模型"><a href="#五、目标检测的网络模型" class="headerlink" title="五、目标检测的网络模型"></a>五、目标检测的网络模型</h2><p>网络模型通常用于检测图像中的子区域。因为遍历检测由于滑动框架需要大量计算，因此使用候选框架，首先要定位感兴趣的区域，然后检测每个候选区域，以极大地降低成本计算网络复杂度，通过这种算法提取候选区域，然后检测和定位目标称为两阶段检测算法。两阶段检测算法的准确率较高，但计算量仍较大，难以实现实时检测。</p><p>考虑到两阶段检测的实用性，单阶段检测算法不需要提取候选区域，而是对每个feature map进行回归预测，大大降低了网络算法的时间复杂度。近年来单级检测算法在保持较高检测速度的同时，其准确率已经接近两级检测算法，使得其发展受到了越来越多的人的关注。</p><h3 id="5-1-二阶段目标检测网络模型"><a href="#5-1-二阶段目标检测网络模型" class="headerlink" title="5.1 二阶段目标检测网络模型"></a>5.1 二阶段目标检测网络模型</h3><p>两阶段检测模型的发展从最初的RCNN开始，围绕RCNN模型有很多改进的模型，如SPP-Net、Fast-RCNN、Fast-RCNN、R-FCN等。这些模型都改进了RCNN网络在单尺度上的特征，大大提高了精度和速度。</p><p>结合多尺度特征融合的思想，对RCNN网络进行了改进。如ION、FPN、MASK-RCNN等，这种多尺度特征融合提高了网络模型检测小目标的能力</p><h4 id="5-1-1-基于单尺度特征模型"><a href="#5-1-1-基于单尺度特征模型" class="headerlink" title="5.1.1 基于单尺度特征模型"></a>5.1.1 基于单尺度特征模型</h4><p>Ross Girshick等人在2014年提出的R-CNN[26]过程相对简单。首先,选择超过2000个候选帧随机利用输入图像(选择性搜索),然后放大到227 * 227,然后使用AlexNet CNN提取特性来获得一个2000 * 4096的矩阵,然后使用支持向量机算法进行分类,也就是说,用特征矩阵的矩阵4096 * 20(代表20类)。分数大于某一阈值的类别被判定为这个类。R-CNN模型在VOC 2010上的准确率达到了53.7mAP。</p><p>何凯明等人在2015年提出的SPP-Net[27]解决了当时RCNN网络的两个纯粹问题:</p><ul><li>1.从原始图像中随机选取候选帧，并对每一候选帧进行特征网络处理。这种重复卷积计算的提取大大增加了计算量。</li><li>2.需要固定大小的输入图像，因此需要裁剪或缩放原始图像。</li></ul><p>这些操作可能导致目标信息丢失，影响目标的准确性。<br>第一个问题使用共享特征卷积层，最后一个卷积层选择候选区域，减少计算量。<br>对于第二个问题，根本原因是全连通层需要输入一个固定维的特征向量。</p><p>为了解决这一问题，SPP-Net在特征网络的最后一个卷积层(feature map)上增加了金字塔池化层，用于有序输出。SPP-Net(ZF5)模型对VOC 2007的精度达到59.2mAP。</p><p>Ross Girshick等人在2015年提出的Fast-RCNN[28]借鉴了中共享卷积图像特征层的方法。</p><p>所有预测框共享一个卷积网络映射到特征映射层，同时在特征映射的最后一层中，提出了SPP-Net的特征池层的简化版本，以输出固定维特征向量，然后连接到完全连接的层以进行后续操作。</p><p>结合SPP-Net的思想对RCNN网络进行优化，大大降低了网络的时间复杂度(但使用选择性搜索候选框仍然非常耗时)。为Faster-RCNN未来的发展奠定了基础。SPP-Net模型对VOC 2007的精度达到66.9mAP。</p><p>任少青等人2016提出了Faster-RCNN[29]，解决了Fast-RCNN中的两个问题:</p><ul><li>1.建议框使用选择性搜索算法，这大大增加了网络计算的数量。</li><li>2.在定位框架的目标损失函数在最优解不稳定处，使用L1距离点。</li></ul><p>Faster-RCNN训练网络是端到端网络，实现了大部分计算的共享，具有较高的检测精度和抗干扰性。虽然实时性不高，但其独特的区域建议网络RPN为整个阶段检测的目标开辟了新的思路。Faster-RCNN(VGG-16)模型对VOC 2007的准确率达到69.9mAP。</p><p>Jifeng Dai等人在2016年提出的R-FCN[30]解决了Faster-RCNN网络模型的问题，因为集合ROI层中的每个建议框都需要单独连接到完全连接的层进行分类和定位。每个特征点会生成9个建议框，消耗大量计算资源。R-FCN在通过ROI后与所有的建议框共享计算结果。骨干特征网络使用更深的残差网络，但由于深度的增加，特征图进一步减小。当原始图像上的物体发生位移时，经过卷积网络后特征地图上的感知能力变弱，网络的平移变异性发生变化。</p><p>两者的区别：(分类任务需要更好的翻译不变性，定位任务需要更好的翻译可变性)因此R-FCN添加了一个位置敏感的得分图来解决这个问题。R-FCN(ResNet-101)模型对VOC 2007和VOC 2012的精度达到75.5mAP。</p><h4 id="5-1-2-基于多尺度特征融合模型"><a href="#5-1-2-基于多尺度特征融合模型" class="headerlink" title="5.1.2 基于多尺度特征融合模型"></a>5.1.2 基于多尺度特征融合模型</h4><p>Sean Bell等人在2015年提出的ION[31]模型在当时的目标检测模型上存在两个问题:</p><ul><li><p>1.1.当时，Fast-RCNN或SPP-Net检测到提议的建议框</p><p>目标周围，也就是建议框体的外部缺少上下文信息。</p></li><li><p>2.两者都只使用最后一层的特征图，只使用高级的语义特征，而缺乏对低级细节特征的使用。</p></li></ul><p>对于问题1，ION网络模型采用递归神经网络的思想，通过连接两个IRNN单元来提取上下文信息。</p><p>对于问题2，采用多尺度特征融合检测。<br>ION网络模型利用上下文信息获取相对宽泛的图像特征信息，结合多通道融合获得的图像细节信息，以获得更好的预测结果。从而提高了小目标的检测精度，并提出了被遮挡目标的检测精度。ION模型在COCO数据集上的精度达到33.1AP。</p><p>Tsung Yi Lin等人在2017年[32]提出的FPN利用底层网络结构的高级语义信息融合来提高特征图的分辨率，在更大的特征图上进行预测有助于获得更多的小目标。</p><p>这些特征信息，使得小目标预测效果明显提高。该FPN模型在COCO数据集上的精度达到了59.1AP。</p><p>何开明等人在2018年提出的Mask-RCNN在结构上与Faster-RCNN相似。它是一个灵活的多任务检测框架，可以完成: 目标检测、目标实例分割和目标关键点检测。</p><p>简单地说，就是一个“探测头”（分割任务层）添加到Faster-RCNN框架结构中。由于引入了Mask layer层，网络能够处理segmentation分割任务和key point关键点任务。ROI Align避免了两种Faster-RCNN并提高检测精度。COCO上Mask-RCNN模型的精度达到36.4AP。</p><h3 id="5-2-单阶段目标检测网络模型"><a href="#5-2-单阶段目标检测网络模型" class="headerlink" title="5.2 单阶段目标检测网络模型"></a>5.2 单阶段目标检测网络模型</h3><p>最早的单级检测模型是YOLO v1。一种改进是在特征提取网络获得的特征图上使用anchor-base锚基，根据预先设定的anchor frame锚帧，逐点检测目标，如SSD、YOLO V2、RetinaNet、YOLO V3、YOLO V4、EfficientDet等</p><p>同时，另一种改进是利用anchor-free无锚思想，通过网络直接点对目标的两个角点和中心点，利用这些关键点来实现目标的返回定位任务。如:CornerNet、CenterNet、CornerNet- lite、FCOS、CenterMask等。</p><p>anchor-free无锚模型克服了锚基模型的以下五个缺点:</p><ul><li>1.检测性能对锚帧的大小、宽高比和数量非常敏感，因此需要仔细调整锚帧相关的超参数。</li><li>2.锚架尺寸和宽高比确定。因此，对于大变形的候选目标，特别是小变形目标，检测器的处理是很困难的。</li><li>3.预定义的锚箱也限制了检测器的泛化能力，因为它们需要为不同的对象大小或宽高比设计。</li><li>4.为了提高recall rate(召回率)，需要在图像上放置密集的anchor frames(锚定帧)。这些anchor boxes(锚框)大多属于负样本，导致正样本和负样本不平衡。</li><li>5.大量的anchor boxes(锚框)增加了计算交集比时的计算量和内存使用量。</li></ul><h4 id="5-2-1-基于anchor-base锚基检测模型"><a href="#5-2-1-基于anchor-base锚基检测模型" class="headerlink" title="5.2.1 基于anchor-base锚基检测模型"></a>5.2.1 基于anchor-base锚基检测模型</h4><p>Wei Liu等人在2016年提出的SSD[34]网络模型对Yolo v1目标检测帧定位不准确和小目标检测不佳的问题提出了两种改进。</p><ul><li>1.SSD采用多尺度融合来提高检测精度(即在包含丰富空间细节信息的大规模feature maps特征图上预测小目标对象，在包含高度抽象语义信息的高级feature maps特征图上预测较大目标对象)。</li><li>2.SSD使用了与Faster-RCNN类似的Anchors锚点(不同纵横比的候选帧)，在一定程度上克服了YOLO v1算法定位不准确和小目标定位困难的问题。SSD(512)模型在可可上的精度达到了26.8AP</li></ul><p>Joseph RedmonR等人在2016年提出的YOLO V2[35]改进了YOLO V1模型中小目标检测和不准确目标帧定位的难度。YOLO V2首先使用Darknet-19特征提取网络来取代YOLO V1的GoogleNet。</p><p>使用更高分辨率的特征图进行预测，并使用多标签模型来组合数据集，使扁平的网络结构简化为structure tree结构树。</p><p>同时，采用联合训练分类和检测数据机制来扩展训练数据集，提高检测准确率，其准确率超过了两阶段Faster-RCNN。YOLO V2模型在COCO上的精度达到21.6AP。</p><p>Tsung Yi Lin等人在2018年提出的RetinaNet模型在训练期间通常具有比正样本多得多的负样本。这种不平衡(往往导致最终计算出的训练损失占绝对多数，但包含了信息量小的负样本为主的负样本。但是提供的关键信息的一些正向的样本不能发挥正常的作用，以至于通常使用训练时几乎不可能按照正确的指导模型进行训练从而描绘出损失)</p><p>所以RetinaNet将样本划分为hard固定样本时，会产生很大的准确性误差，在(0.4&lt;IOU&lt;0.5)范围内难以区分样本。</p><p>采用Focal Loss(消除类别不平衡+挖掘难度大的样本)提高精度。retavanet (ResNeXt-101-FPN)模型在COCO上的准确性达到40.8AP。</p><p>Joseph Redmon等人在2018年提出的YOLO v3[37]是YOLO v3在YOLO V2基础上的进一步改进，其检测更加准确，速度仍然非常快。</p><p>主要改变是使用Darknet-53取代Darknet-19主干特征提取网络（YOLO v3）使用darknet-53的前52层（无全连接层）。</p><p>加上多尺度融合检测，在不同的层中获得y1、y2和y3的三个输出（每个预测尺度的特征映射点上只有三个先验框）。并修正了损失。YOLO v3(Darknet-53)模型COCO上数据的准确性达到33.0AP。</p><p>Alexey Bochkovskiy等人在2020年提出的YOLO v4[38]，在传统YOLO系列的框架上，采用了CNN领域近年来的最佳优化策略，从数据处理、backbone骨干网络、网络训练、activation激活函数、Loss函数等方面进行优化。</p><p>由Mingxing Tan等人在2020年提出的EfficientDet[39]在保持低浮点运算量的情况下实现了高精度。根据不同的精度要求，EfficientDet模型尺寸从D0增加到D7。</p><p>EfficientSet主要使用FPN网络和</p><p>将不同层次的特征图进行多层特征融合，形成BiFPN层次结构，并遵循高效网特征提取网络的思想，用一个简单的参数φ来实现其主干网络、特征融合网络BiFPN、Box/Class预测了网络规模，使网络更高效。</p><p>EfficientSet-D0(512)模型在COCO上的的准确度达到34.6AP，EfficientSet-D7x(1536)模型在COCO上的的准确度达到55.1AP。</p><h4 id="5-2-2-基于anchor-free无锚检测模型"><a href="#5-2-2-基于anchor-free无锚检测模型" class="headerlink" title="5.2.2 基于anchor-free无锚检测模型"></a>5.2.2 基于anchor-free无锚检测模型</h4><p>Joseph Redmon等人在2016年首次提出了更经典的单阶段YOLO V1[40]模型。为了提高检测速度，单级检测去除两级RPN(区域提议网络)，直接在输出层确定目标类别和目标帧。</p><p>目标的定位以整个图像为输入，将目标检测任务转化为回归任务。早期的Yolo v1算法结构简洁，能很好地反映单级检测网络模型的特点。YOLO v1模型在COCO上的准确率达到57.9mAP。</p><p>Hei Law等人在2018年提出的CornerNet[41]利用单个卷积网络改变目标边界来预测一对关键点(即目标框的左上角和右下角)。</p><p>该设计可以消除常用的单级检测，预测锚杆的需求。同时，对池化层进行了改进，Corner Pooling角点池化可以用来定位包围框的角点(图46)。在MS COCO数据集上实现了42.1%的AP，优于当时所有单级探测器的检测性能，与两级检测器的检测性能相当。</p><p>CornerNet511(单尺度，Hourglass-104)模型在COCO上的的精度达到40.6AP，而CornerNet511(多尺度，Hourglass-104)模型在COCO上的的精度达到42.2AP。</p><p>Kaiwen Duan等人在2019年提出的CenterNet[42]也是一种单级无锚框架网络模型。对于CornerNet，只通过检测目标的左上角和右下角来确定目标。该方法没有充分利用目标内部的特征信息，因此针对误检测帧的现象，提出了一种改进的具有更丰富语义信息的级联Corner Pooling角点池化算法和一种用于检测目标Center Point中心点特征的中心池算法。利用三重组关键点对目标进行检测，大大提高了检测精度，成为当时最好的单级检测模型，速度约为270ms(52层特征网络)和340ms(104层特征网络)。</p><p>CenterNet 511(单尺度，沙漏-104)模型在CCOCO上的精度达到44.9AP, CenterNet 511(多尺度，沙漏-104)模型在COCO上的精度达到47.0AP。</p><p>Hei Law等人在2019年提出的CornerNet- lite[43]在CornerNet的基础上对骨干网络进行了优化，形成了CornerNet- squeeze。</p><p>该算法利用注意机制进行裁剪，去除网络检测目标的冗余图像部分(类似于两阶段检测，首先将目标的近似区域裁剪出来进行检测)。</p><p>该方法在速度和精度上取得了很好的突破，达到了当时单级探测器的最高精度(47.0%)。CornerNet-Saccade模型在COCO上的准确性达到43.2AP。</p><p>2019年Zhi Tian等人提出的FCOS[44]网络模型大致由FPN特征金字塔和三个分支检测头组成。FCOS摒弃了传统的锚框，直接对feature map上的每个点进行回归操作。</p><p>而FPN的多尺度分层检测大大减少了同一位置多个检测帧产生的模糊样本。中心度加权与非最大抑制(non-maximum suppression, non-maximum suppression)相结合是抑制低质量BB距离的一种很好的方法。</p><p>与一些最主流的一阶和二阶检测器相比，FCOS在检测效率上优于Faster R-CNN、YOLO、SSD等经典算法。FCOS为了提高准确度而缺乏速度，但在准确度和速度上优于RetinaNet。FCOS(ResNeXt-64x4d-101-FPN)模型在coco上的精度达到了44.7AP。</p><p>Youngwan Lee等人在2020年提出的CenterMask[45]是基于FCOS的，在注意机制中加入SAG-Mask实例分割模块，替代了其特征提取骨干网络(VoVNet-V2)。</p><p>使用ResNet101-FPN骨干网络可以达到38.3%的Mask_AP，超过以往所有网络，但速度只有13.9FPS。轻量级的CenterMask-Lite可以达到33.4%的Mask_AP和38%的Box_AP，速度可以达到35 FPS，可以满足实时性要求。CenterMask (V-39-FPN)模型在COCO上的精度达到了36.3AP_mask。</p><hr><h2 id="六、目标检测算法的未来发展方向和总结"><a href="#六、目标检测算法的未来发展方向和总结" class="headerlink" title="六、目标检测算法的未来发展方向和总结"></a>六、目标检测算法的未来发展方向和总结</h2><p>为了追求更快、更准确的目标检测算法模型，该算法模型将合并更多其他先进的模型算法，单阶段方法和两阶段方法将逐渐合并。</p><p>例如，单级算法提出的目标位置估计的CornerNet Lite模型是伪模型，两阶段模型采用了两阶段目标检测的思想。</p><p>随着检测任务需求的多样化，目标检测模型不再是单一的任务模型，增加了实例分割(类似于多目标检测，全景分割(它是语义分割和实例分割的结合：语义分割是指为图像上的每个像素指定一个类别（可以通过颜色区分），但不区分个体）。经过全景分割，我们可以知道哪个个体图像上的每个像素属于哪个类别，这是一个更精细的分类任务。同时，还有检测人体姿态的关键点(即用点替换人体的关节，用相邻线段连接，抽象地表示人体姿态动作)。</p><h2 id="七、参考材料"><a href="#七、参考材料" class="headerlink" title="七、参考材料"></a>七、参考材料</h2><p>Summary of Target Detection Algorithms2021</p>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Survey </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>目标检测之性能指标</title>
      <link href="/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/"/>
      <url>/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/</url>
      
        <content type="html"><![CDATA[<h2 id="一、目标检测之性能指标"><a href="#一、目标检测之性能指标" class="headerlink" title="一、目标检测之性能指标"></a>一、目标检测之性能指标</h2><p>对于目标检测，我们从两个方向来进行评估，一方面是：检测精度，也就是检测的准确程度，另一方面是检测速度，也就是返回检测结果的快慢。</p><h3 id="1-1检测精度："><a href="#1-1检测精度：" class="headerlink" title="1.1检测精度："></a><strong>1.1检测精度</strong>：</h3><ul><li>Precision(准确率，精度), Recall(召回率), F1 score(精确率和召回率的调和平均数，最大为1，最小为0。)</li><li>loU(Intersection over Union)(交并比)</li><li>P-R curve(Precision-Recall curve)(精度召回曲线)</li><li>AP(Average Precision)(平均正确率)</li><li>mAP(mean Average Precision)(平均精度均值)</li></ul><h4 id="1-1-1-Precision-Recall-F1-score"><a href="#1-1-1-Precision-Recall-F1-score" class="headerlink" title="1.1.1 Precision, Recall, F1 score"></a>1.1.1 Precision, Recall, F1 score</h4><p>我们将<strong>预测情况</strong>与<strong>实际情况</strong>作为两个维度进行考虑，其中预测会有两种结果，也即为Positive(肯定的)与Negative(否定的);同时实际情况也分为两种，即为True(是)或False(否)，分别将两个维度下的四种结果进行两两叠加即得下列的混淆矩阵：</p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20210831161154109-1321421145.png"></p><p>在这个矩阵中，第一位T/F:表示预测的对错，第二位P/N：表示预测的结果。<br>若假设我们对苹果、香蕉、西瓜进行训练和预测，<br>那么由此，就可以得出以下分析：</p><p><strong>TP</strong>：TP即为预测正确，且预测结果与真实结果一致。<br>        举例说明就是，当前真实值为苹果，由于模型收敛的还算不错，正确预测为苹果，<br>        且预测结果与真实值都是苹果，这就是我们期望得到的结果。<br><strong>FP</strong>：FP即为预测错误，且预测结果与真实结果一致。<br>        举例说明就是，当前真实值为香蕉，但由于模型和参数训练等问题，原本正确的预测应为苹果，<br>        但错误预测为香蕉，反而使得预测结果与真实值都是香蕉，这属于是误打误撞的成功。<br><strong>FN</strong>：FN即为预测错误，且预测结果与真实结果不一致。<br>        举例说明就是，当前真实值为香蕉，但由于某种原因和问题，<br>        导致错误预测的结果为苹果，此时与真实结果不一致。<br><strong>TN</strong>：TN即为预测正确，且预测结果与真实结果不一致。<br>        举例说明就是，当前真实值为香蕉，根据预测的置信度和类别标签却正确预测为苹果，<br>        那么此时预测结果与真实结果也不一致。</p><p>从而就有：</p><ul><li><strong>精度Precision</strong>(查准率)是评估预测的<strong>准不准</strong>(看预测行)</li><li><strong>召回率Recall(查全率)<strong>是评估找的</strong>全不全</strong>(看实际行)</li><li><strong>F1 score</strong>是精确率和召回率的调和平均数</li></ul><p>再来看下面这张图或许也能帮助大家理解：<br>可以看到图中有若干个实心点和空心点，其中左边绿色部分的半圆形区域为TP(True Postives)，左边剩余部分的区域则为FN(False Negatives)，类似地，右边红色部分的半圈形区域为FP(False Positives)，右边剩余部分的区域则为TN(True Negatives)。</p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20220214145443585-1636099818.jpg"></p><p>那么在图中怎么表示精度和召回率呢？<br>可以看到，若我们将整个圆形部分作为分母，以TP绿色部分作为分子，那么这时候FP红色部分的面积和点数量越小，则整体的精度就越高。</p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20220214150515044-644530486.png"></p><p>同样地，若我们将全部的绿色部分作为分母，绿色圆形部分TP作为分母，这时若绿宝圆形部分占整体绿色面积的比率越大，也就是绿色圆形部分覆盖的实心点数量越多，则整体查找的范围就越大。</p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20220214150526359-1966496528.png"></p><h4 id="1-1-2-IoU-Intersection-over-Union"><a href="#1-1-2-IoU-Intersection-over-Union" class="headerlink" title="1.1.2 IoU(Intersection over Union)"></a>1.1.2 IoU(Intersection over Union)</h4><p>当然，对于目标检测任务而言，不仅包含分类，同时还有边界框回归。<br>为了评估边界框回归准确与否，这里使用IoU(交并比)指标进行评估。</p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20210831162805329-1693081214.png"></p><p>由图所示，我们令蓝色边框部分为Ground truth(基本事实)为标定的框，黄色边框部分为Prediction(预测结果)，那么如何来描述预测结果与基本事实之间吻合的程度呢？这里用一个参数比率IoU：<strong>两个框的交集/两个框的并集来进行衡量</strong>，也称作<strong>交并比</strong>。若交并比，也就是两个框之前的重叠程度越高，则说明预测的框体越准确。</p><p>Iou表示预测的边界框和真实边界框之间的重叠程度。您可以为 Iou 设置阈值以确定对象检测是否有效。假设您将 Iou 设置为 0.5，在这种情况下</p><ul><li>如果Iou ≥ 0.5，则将对象检测分类为True Positive(TP)。</li><li>如果Iou ≤ 0.5，那么这是一个错误的检测并将其归类为False Positive(FP)。</li><li>当图像中存在Ground Truth(基本事实)并且模型未能检测到对象时，将其分类为False Negative(FN)。</li><li>True Negative(TN)是我们没有预测对象的图像的每个部分，这个指标对对象检测没有用，因此我们忽略了 TN。</li></ul><p><strong>从IoU的角度来看Precision、Recall等</strong></p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20210831163508936-1897141825.png"></p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20210831163600390-1493915840.png"></p><h4 id="1-1-3-P-R-curve"><a href="#1-1-3-P-R-curve" class="headerlink" title="1.1.3 P-R curve"></a>1.1.3 P-R curve</h4><p>　　P-R曲线是以召回率R为横轴，准确率P为纵轴，然后根据模型的预测结果对样本进行排序，把最有可能是正样本的个体排在前面，而后面的则是模型认为最不可能为正例的样本，再按此顺序逐个把样本作为“正例”进行预测并计算出当前的准确率和召回率得到的曲线。</p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/5bc71dafc4411.png" alt="6.png"></p><p>　　通过上图我们可以看到，当我们只把最可能为正例的个体预测为正样本时，其准确率最高位1.0，而此时的召回率则几乎为0，而我们如果把所有的个体都预测为正样本的时候，召回率为1.0，此时准确率则最低。</p><h4 id="1-1-4-AP-Average-Precision"><a href="#1-1-4-AP-Average-Precision" class="headerlink" title="1.1.4 AP(Average Precision)"></a>1.1.4 AP(Average Precision)</h4><p>用一个简单的例子来演示平均精度(AP)的计算。<br>假设数据集中总共有5个苹果。我们收集模型为苹果作的所有预测，并根据预测的置信水平(从最高到最低)对其进行排名。<br>第二列表示预测是否正确。如果它与ground truth匹配并且IoU$\ge$0.5，则是正确的。</p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20210831164512890-2064866194.png"></p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20220214163221227-249650633.jpg"></p><p>AP计算之11点法 </p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20220214164113684-1888906616.jpg"></p><p>对于PASCAL VOC挑战来说，如果Iou&gt;0.5，则预测为正样本(TP)。</p><p>但是，如果检测到同一目标的多个检测，则视第一个检测为正样本(TP)，而视其余检测为负样本(FP)。</p><p>上面的计算方法是2010年以前的计算方法，2010年之后则改用了积分的方法来进行计算最后的AP值，相较于之前更加准确。</p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20220214164250892-1030846832.jpg"></p><p>将上图进行划分为四个不同面积大小的区域，则总体的AP就等于四者之和。</p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20220214164558976-1157183023.jpg"></p><p>最后做一个小结：</p><ul><li><p>Pascal VOC2007 uses 11 Recall Points on PR curve.<br>在PR曲线上使用11个回忆点。</p></li><li><p>Pascal VOC2010-2012 uses(all points)Area Under Curve(AUC)on PR curve.<br>在PR曲线上使用(所有点)曲线下面积(AUC)。</p></li><li><p>MS COCO uses 101 Recall points on PR curve as well as different IoU thresholds。</p><p>在PR曲线上使用101个召回点以及不同的IoU阈值，划分更加精细。</p></li></ul><h4 id="1-1-5-mAP-mean-Average-Precision"><a href="#1-1-5-mAP-mean-Average-Precision" class="headerlink" title="1.1.5 mAP(mean Average Precision)"></a>1.1.5 mAP(mean Average Precision)</h4><p>Average Precision(AP)衡量出来的是学习出来的模型在每个类别上的好坏；<br>mean Average Precision(mAP)衡量的是学出的模型在所有类别上的好坏，实际上mAP就是取所有类别上AP的平均值。</p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20220214152904273-529729863.png"></p><p>下面来看这张表，这里呈现的是，对于不同的网络，例如：VGG-16、ResNet-101、ResNet-101在相同数据下的不同类别精度数据表现以及平均精度表现。</p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20220214153032363-1625688475.png"></p><h4 id="1-1-6COCO-AP"><a href="#1-1-6COCO-AP" class="headerlink" title="1.1.6COCO AP"></a>1.1.6COCO AP</h4><p>以COCO数据集为例，AP是多个IOU的平均值（考虑Positive匹配的最小IOU）<br>mAP@[.5:.95]对应于Iou的平均AP，从0.5到0.95，步长为0.05。<br>在COCO竞赛中，AP是80个类别中超过10个Iou levels的平均值<br>mAP@.75的意思是Iou的平均检测精度mAP为0.75。</p><ul><li><p>这里可以引申出一个问题：是否Iou越大越好？</p><p>从下图可以看出，随着Iou的增加，Precision-Recall曲线中Recall的也随之下降，即为增加Iou后的查全率下降了，也就是说查找的框体随着Iou边界逼近重叠而减少了，很多预测框不准的都不认为是重要的。</p></li></ul><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20210831165812108-246231750.png"></p><p>下面是关于COCO数据集AP的一些简单介绍<br>其中AP at IoU=.95:.05:.95(primary challenge metric)是主要的一种指标。<br>IoU=0.50的时，与PASCAL VOC的AP metric相同。若IoU=0.75时，则相对比较严格。<br><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20220214163658032-944386892.png"></p><p><strong>不同尺度大小下的AP</strong><br>这里将small(小)、medium(中)、large(大)进行不同的区分<br>其中Small小目标的定义是：像素面积area&lt;32^2;<br>medium中等目标的定义是：像素面积area &gt; 32^2且area &lt; 96^2<br>large大目标的定义是：像素面积area &gt; 96^2<br><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20220214163435163-1275515687.png"></p><p><strong>平均召回率</strong></p><p>下面max对应的分别是每张图片下包含的目标个数<br><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20220214163459747-78048917.png"></p><h3 id="1-2-检测速度"><a href="#1-2-检测速度" class="headerlink" title="1.2 检测速度"></a>1.2 检测速度</h3><ul><li>前传耗时</li><li>每秒帧数FPS(Frames Per Second)(每秒传输帧率)</li><li>浮点运算量(FLOPS)</li></ul><h4 id="1-2-1-前传耗时"><a href="#1-2-1-前传耗时" class="headerlink" title="1.2.1 前传耗时"></a>1.2.1 前传耗时</h4><p>从输入一张图像到输出最终结果所消耗的时间，包含前处理耗时(如图像归一化)、网络前传耗时、后处理耗时(如非极大值抑制nms)。</p><h4 id="1-2-2-FPS-Frames-Per-Second"><a href="#1-2-2-FPS-Frames-Per-Second" class="headerlink" title="1.2.2 FPS(Frames Per Second)"></a>1.2.2 FPS(Frames Per Second)</h4><p>FPS是图像领域中的定义，是指画面每秒传输帧数，通俗来讲就是指动画或视频的画面数。FPS是测量用于保存、显示动态视频的信息数量。每秒钟帧数越多，所显示的动作就会越流畅。通常，要避免动作不流畅的最低是30。某些计算机视频格式，每秒只能提供15帧。</p><p>FPS也可以理解为我们常说的“刷新率（单位为Hz）”，例如我们常在游戏里说的“FPS值”。我们在装机选购显卡和显示器的时候，都会注意到“刷新率”。一般我们设置缺省刷新率都在75Hz（即75帧/秒）以上。例如：75Hz的刷新率刷也就是指屏幕一秒内只扫描75次，即75帧/秒。而当刷新率太低时我们肉眼都能感觉到屏幕的闪烁，不连贯，对图像显示效果和视觉感观产生不好的影响。</p><p>电影以每秒24张画面的速度播放，也就是一秒钟内在屏幕上连续投射出24张静止画面。有关动画播放速度的单位是fps，其中的f就是英文单词Frame（画面、帧），p就是Per（每），s就是Second（秒）。用中文表达就是多少帧每秒，或每秒多少帧。电影是24fps，通常简称为24帧。</p><p>结合以上性能指标描述，下面可以更容易理解下列表格中不同模型之间的性能差异及表现效果。</p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-xing-neng-zhi-biao/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/1571518-20210831165949167-421155894.png"></p><h4 id="1-2-3-浮点运算量-FLOPS"><a href="#1-2-3-浮点运算量-FLOPS" class="headerlink" title="1.2.3 浮点运算量(FLOPS)"></a>1.2.3 浮点运算量(FLOPS)</h4><p>处理一张图像所需要的浮点运算数量，跟具体软硬件没有关系，可以公平地比较不同算法之间的检测速度。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Object_Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>目标检测任务理解与总结</title>
      <link href="/blog/2022/02/15/mu-biao-jian-ce-ren-wu-li-jie-yu-zong-jie/"/>
      <url>/blog/2022/02/15/mu-biao-jian-ce-ren-wu-li-jie-yu-zong-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="目标检测任务理解与总结"><a href="#目标检测任务理解与总结" class="headerlink" title="目标检测任务理解与总结"></a>目标检测任务理解与总结</h2><hr><p>从字面意义理解，所谓目标检测任务，就是定位并检测目标，也就是说计算机在处理图像的时候需要解决两个问题：<br>1.What? —— 图像中是什么东西？我们的目标是要检测什么东西？—— <strong>识别</strong> Recognition<br>2.Where? —— 在图像的什么位置？目标的定位坐标大致范围是多少？—— <strong>定位</strong> Localization</p><p>在目标检测算法中，通过<strong>最小外接矩形</strong>(Bounding box)来进行<strong>目标定位</strong>，同时利用预设<strong>类别标签</strong>(Category label)来进行目标对象类别的区分，于是这个图像检索的问题就被描述成，计算机在进行目标检测任务时，只需要识别出对应的框体和类别名称，即解决了当前图像的目标检测问题。</p><p>其中有两项比较重要的参数是帮助描述和实现目标检测任务的关键：<strong>类别标签</strong>和<strong>置信度得分</strong>。</p><ul><li>类别标签(Category label)：对于当前标记目标类别的标签名称或标记符号称为类别标签。</li><li>置信度得分(Confidence score)： 用来描述和确认当前检测目标为某一个标记类别的接近程度。</li></ul><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-ren-wu-li-jie-yu-zong-jie/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BB%BB%E5%8A%A1%E7%90%86%E8%A7%A3%E4%B8%8E%E6%80%BB%E7%BB%93/1571518-20210831151915266-1968016985.png"></p><p>与其他类型(图像分类、实例分割)任务相比<br>从单目标对象的角度来看，分类仅针图像中的目标本身，而分类+定位则需要在分类的基础上框选其目标范围；<br>从多目标对象的角度来看，目标检测需要对图像中的不同目标进行最小外界矩形定位和标签分类，而实例分割则需要对图像中的不同目标边界轮廓进行包围标注和标签分类。</p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-ren-wu-li-jie-yu-zong-jie/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BB%BB%E5%8A%A1%E7%90%86%E8%A7%A3%E4%B8%8E%E6%80%BB%E7%BB%93/1571518-20210831153531511-1876522664.png"></p><p>具体地来说，我们将定位和检测这两种不同的问题，描述为以下的任务：<br>定位和检测：</p><ul><li><strong>定位</strong>：定位是找到检测图像中带有一个给定标签的<strong>单个目标</strong></li><li><strong>检测</strong>：检测是找到图像中带有给定标签的<strong>所有目标</strong>。</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Object_Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>目标检测之常用数据集</title>
      <link href="/blog/2022/02/15/mu-biao-jian-ce-zhi-chang-yong-shu-ju-ji/"/>
      <url>/blog/2022/02/15/mu-biao-jian-ce-zhi-chang-yong-shu-ju-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="目标检测之常用数据集"><a href="#目标检测之常用数据集" class="headerlink" title="目标检测之常用数据集"></a>目标检测之常用数据集</h2><p>我们在利用模型检测实际应用问题的时候，通常需要使用自己标注和收集数据集来进行训练。<br>但对于不同算法性能的比较，我们常常需要一些基本的数据集来进行横向对比。</p><p>下列是一些常用的目标检测数据集:</p><ul><li>PASCAL VOC</li><li>ImageNet</li><li>COCO</li></ul><h3 id="1-PASCAL-VOC-challange"><a href="#1-PASCAL-VOC-challange" class="headerlink" title="1.PASCAL VOC challange"></a>1.PASCAL VOC challange</h3><p>PASCAL VOC挑战在2005年至2012年间展开。</p><p>PASCAL VOC 2007：9963张图像，24640个标注。<br>PASCAL VOC 2012: 11530张图像，27450个标注。</p><p>该数据集中有20个分类，同时包含11530张用于训练和验证的图像，其中感兴趣区域有27450个标定目标，每个图像平均有2.4个目标。<br>以下是数据集中的20个分类：</p><ul><li><p>Person(人)：person(人)</p></li><li><p>Animal(动物)：bird(鸟)、cat(猫)、cow(牛)、dog(狗)、horse(马)、sheep(羊)</p></li><li><p>Vehicle(车辆)：aeroplane(飞机)、bicycle(自行车)、boat(船)、bus(巴士)、car(汽车)、motorbike(摩托车)、train(火车)</p></li><li><p>indoor(室内)：bottle(瓶)、chair(椅子)、dining table(餐桌)、potted plant(盆栽植物)、sofa(沙发)、tv(电视)/monitor(监视器)</p></li></ul><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-chang-yong-shu-ju-ji/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20210831155027770-259453237.png"></p><p>链接：<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/</a> </p><h3 id="2-ImageNet数据集"><a href="#2-ImageNet数据集" class="headerlink" title="2 ImageNet数据集"></a>2 ImageNet数据集</h3><p>ILSVRC 2010-2017为斯坦福大学的李飞飞教授主导并组建的数据集。</p><ul><li>ImageNet拥有用于分类、定位和检测任务评估的数据</li><li>与分类数据类似，定位任务有1000个类别。准确率是根据Top 5检测结果计算出来的。</li><li>对200个目标的检测问题有470000个图像，平均每个图像有1.1个目标。</li></ul><h3 id="3-MS-COCO数据集"><a href="#3-MS-COCO数据集" class="headerlink" title="3 MS COCO数据集"></a>3 MS COCO数据集</h3><ul><li><p>MS COCO的全称是Microsoft Common Objects in Context，起源于微软于2014年出资标注的Microsoft COCO数据集，与ImageNet竞赛一样，被视为计算机视觉领域最受关注和最权威的比赛之一。</p></li><li><p>在ImageNet竞赛停办后，COCO竞赛就成为是当前目标识别、检测等领域的一个最权威、最重要的标杆，也是目标该领域在国际上唯一能汇集Google、微软、Facebook以及国内外众多顶尖院校和优秀创新企业公共参与的大赛。</p></li><li><p>COCO(Common Objects in Context)数据集包含20万个图像：11.5万多训练集验证集图像，2万多张测试集图像。</p></li><li><p>80个类别中有超过50万个目标标注。他是最广泛公开的目标检测数据集。</p></li><li><p>平均每个图像的目标数为7.2个。</p><p><img src="/blog/blog/2022/02/15/mu-biao-jian-ce-zhi-chang-yong-shu-ju-ji/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20210831160148159-16184076.png"></p></li></ul><p>链接：<a href="http://cocodataset.org/">http://cocodataset.org</a> </p>]]></content>
      
      
      
        <tags>
            
            <tag> Object_Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用数据集</title>
      <link href="/blog/2022/02/10/chang-yong-shu-ju-ji/"/>
      <url>/blog/2022/02/10/chang-yong-shu-ju-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="常用数据集"><a href="#常用数据集" class="headerlink" title="常用数据集"></a>常用数据集</h1><hr><h2 id="PASCAL-视觉对象类-PASCAL-VOC"><a href="#PASCAL-视觉对象类-PASCAL-VOC" class="headerlink" title="PASCAL 视觉对象类 (PASCAL VOC)"></a>PASCAL 视觉对象类 (PASCAL VOC)</h2><p>PASCAL VOC 数据集 (2012) 是众所周知的常用于对象检测和分割的数据集。超过 11k 幅图像组成了训练和验证数据集，而 10k 幅图像专用于测试数据集。</p><p>分割挑战使用<a href="https://www.tensorflow.org/api_docs/python/tf/metrics/mean_iou">平均交叉联合 (mIoU)</a>指标进行评估。Intersection over Union (IoU) 是一种也用于对象检测的度量，用于评估预测位置的相关性。IoU是ground truth和预测区域之间的重叠区域和联合区域之间的比率。mIoU 是分割对象在测试数据集的所有图像上的 IoU 之间的平均值。</p><p><img src="/blog/blog/2022/02/10/chang-yong-shu-ju-ji/%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20220210173658605-1013744197.png"></p><p><img src="/blog/blog/2022/02/10/chang-yong-shu-ju-ji/%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20220210173751705-349662235.png"></p><p>用于图像分割的 2012 PASCAL VOC 数据集示例。来源：<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html</a></p><h2 id="PASCAL-上下文"><a href="#PASCAL-上下文" class="headerlink" title="PASCAL-上下文"></a>PASCAL-上下文</h2><p>PASCAL-Context 数据集 (2014) 是 2010 PASCAL VOC 数据集的扩展。它包含大约 10k 用于训练的图像，10k 用于验证和 10k 用于测试。这个新版本的特点是整个场景被分割提供了 400 多个类别。请注意，图像已由六名内部注释者在三个月内进行了注释。</p><p>PASCAL-Context 挑战的官方评估指标是 mIoU。其他几个指标由研究发布为像素精度 (pixAcc)。在这里，性能将仅与 mIoU 进行比较。</p><p><img src="/blog/blog/2022/02/10/chang-yong-shu-ju-ji/%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20220210173826638-1758966942.png"></p><p>PASCAL-Context 数据集的示例。资料来源：<a href="https://cs.stanford.edu/~roozbeh/pascal-context/*">https ://cs.stanford.edu/~roozbeh/pascal-context/</a></p><h2 id="上下文中的公共对象（COCO）"><a href="#上下文中的公共对象（COCO）" class="headerlink" title="上下文中的公共对象（COCO）"></a>上下文中的公共对象（COCO）</h2><p>图像语义分割（“物体检测”和“物体分割”）有两个 COCO 挑战（2017 年和 2018 年）。“对象检测”任务包括将对象分割和分类为 80 个类别。“东西分割”任务使用图像的大部分分割部分（天空、墙壁、草）的数据，它们包含几乎所有的视觉信息。在这篇博文中，将只比较“对象检测”任务的结果，因为引用的研究论文中很少有关于“物体分割”任务的结果。</p><p>用于对象分割的 COCO 数据集由超过 200k 的图像和超过 500k 的对象实例分割组成。它包含一个训练数据集、一个验证数据集、一个用于研究人员的测试数据集（test-dev）和一个用于挑战的测试数据集（test-challenge）。两个测试数据集的注释都不可用。这些数据集包含 80 个类别，并且仅分割了相应的对象。此挑战使用与对象检测挑战相同的指标：平均精度 (AP) 和平均召回率 (AR) 均使用联合交集 (IoU)。</p><p>有关 IoU 和 AP 指标的详细信息，请参阅我<a href="https://medium.com/comet-app/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852">之前的博客文章</a>。例如 AP，Average Recall 是使用具有特定重叠值范围的多个 IoU 计算的。对于固定的 IoU，具有相应测试/地面实况重叠的对象被保留。然后为检测到的对象计算召回指标。最终的 AR 指标是所有 IoU 范围值的计算召回率的平均值。基本上，用于分割的 AP 和 AR 度量与对象检测的工作方式相同，除了 IoU 是按像素计算的，用于语义分割的非矩形形状。</p><p><img src="/blog/blog/2022/02/10/chang-yong-shu-ju-ji/%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20220210173843850-1655375410.png"></p><p>用于对象分割的 COCO 数据集示例。来源：<a href="http://cocodataset.org/*">http ://cocodataset.org/</a></p><h2 id="城市景观"><a href="#城市景观" class="headerlink" title="城市景观"></a>城市景观</h2><p>Cityscapes 数据集于 2016 年发布，包含来自 50 个城市的复杂分段城市场景。它由 23.5k 用于训练和验证的图像（精细和粗略注释）和 1.5 张用于测试的图像（仅精细注释）组成。图像是完全分割的，例如具有 29 个类别的 PASCAL-Context 数据集（在 8 个超类别内：平面、人类、车辆、建筑、物体、自然、天空、虚空）。由于其复杂性，它通常用于评估语义分割模型。它还因其与自动驾驶应用的真实城市场景的相似性而闻名。语义分割模型的性能是使用 mIoU 指标计算的，例如 PASCAL 数据集。</p><p><img src="/blog/blog/2022/02/10/chang-yong-shu-ju-ji/%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20220210173902569-1052946298.png"></p><p>Cityscapes 数据集的示例。顶部：粗略的注释。底部：精细注释。来源：<a href="https://www.cityscapes-dataset.com/*">https ://www.cityscapes-dataset.com/</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Deep_Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deeplabsv3+训练总结</title>
      <link href="/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/"/>
      <url>/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="DeepLabv3-训练模型学习总结"><a href="#DeepLabv3-训练模型学习总结" class="headerlink" title="DeepLabv3+训练模型学习总结"></a>DeepLabv3+训练模型学习总结</h2><hr><h2 id="一、DeepLabs3-介绍"><a href="#一、DeepLabs3-介绍" class="headerlink" title="一、DeepLabs3+介绍"></a>一、DeepLabs3+介绍</h2><p><strong>DeepLabv3</strong>是一种语义分割架构，它在<a href="https://paperswithcode.com/method/deeplabv2">DeepLabv2</a>的基础上进行了一些修改。为了处理在多个尺度上分割对象的问题，设计了在级联或并行中采用多孔<a href="https://paperswithcode.com/method/convolution">卷积</a>的模块，通过采用多个多孔速率来捕获多尺度上下文。此外，来自 DeepLabv2 的 Atrous <a href="https://paperswithcode.com/method/spatial-pyramid-pooling">Spatial Pyramid Pooling</a>模块增加了编码全局上下文的图像级特征，并进一步提高了性能。</p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125090542511-130591046.png"></p><p>ASSP 模块的变化是作者在模型的最后一个特征图上应用<a href="https://paperswithcode.com/method/global-average-pooling">全局平均池</a>化，将生成的图像级特征馈送到具有 256 个滤波器（和<a href="https://paperswithcode.com/method/batch-normalization">批量归一化</a>）的 1×1 卷积，然后对模型进行双线性上采样特征到所需的空间维度。最后，改进的<a href="https://paperswithcode.com/method/aspp">ASPP</a>由 (a) 一个 1×1 卷积和三个 3×3 卷积组成，当输出步幅 = 16 时，速率 = (6, 12, 18)（均具有 256 个滤波器和批量归一化），以及（ b) 图像级特征。</p><p>另一个有趣的区别是不再需要来自 DeepLabv2 的 DenseCRF 后处理。</p><hr><h2 id="二、DeepLabv3-图像语义分割原理"><a href="#二、DeepLabv3-图像语义分割原理" class="headerlink" title="二、DeepLabv3+图像语义分割原理"></a>二、DeepLabv3+图像语义分割原理</h2><h3 id="2-1-图像分割任务及常用数据集"><a href="#2-1-图像分割任务及常用数据集" class="headerlink" title="2.1 图像分割任务及常用数据集"></a>2.1 图像分割任务及常用数据集</h3><h4 id="2-1-1-图像分割任务"><a href="#2-1-1-图像分割任务" class="headerlink" title="2.1.1 图像分割任务"></a>2.1.1 图像分割任务</h4><p><strong>图像分割</strong>（image segmentation）：根据某些规则将图片分成若干个特定的、具有独特性质的区域，并抽取出感兴趣目标。<br>• 目前图像分割任务发展出了以下几个子领域：<br>➢ <strong>语义分割</strong>（semantic segmentation）<br>➢ <strong>实例分割</strong>（instance segmentation）<br>➢ 以及刚兴起的新领域：<strong>全景分割</strong>（panoptic segmentation）</p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125090607648-1150465116.png"></p><p>在这里DeepLabv3+就属于一种语义分割的架构模型，<br>语义分割从任务上来看，要实现的最终目标是：<br>从像素层次来识别图像，也即为图像中的每个像素指定类别标记。</p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125091005440-91589559.png"></p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125091024828-1830485005.jpg"></p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125091048201-1926400491.png"></p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125091120612-1419294283.png"></p><h4 id="2-1-2-常用数据集"><a href="#2-1-2-常用数据集" class="headerlink" title="2.1.2 常用数据集"></a>2.1.2 常用数据集</h4><p>常用数据集<br>• <strong>PASCAL VOC 2012 Segmentation Competition</strong></p><blockquote><p>VOC2012数据集分为20类，包括背景为21类，分别如下：</p><ul><li>人 ：人</li><li>动物：鸟、猫、牛、狗、马、羊</li><li>车辆：飞机、自行车、船、巴士、汽车、摩托车、火车</li><li>室内：瓶、椅子、餐桌、盆栽植物、沙发、电视/监视器</li></ul><p><strong>挑战任务：</strong></p><ol><li><p>Classification/Detection Competitions</p></li><li><p>Segmentation Competition</p></li><li><p>Action Classification Competition</p></li><li><p>ImageNet Large Scale Visual Recognition Competition</p></li><li><p>Person Layout Taster Competition</p></li></ol><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125091146379-2067668652.png"></p><p>​    </p></blockquote><p>• <strong>COCO 2018 Stuff Segmentation Task</strong></p><blockquote><ul><li><p>MS COCO的全称是Microsoft Common Objects in Context，起源于是微软于2014年出资<br>标注的Microsoft COCO数据集，与ImageNet 竞赛一样，被视为是计算机视觉领域最受关<br>注和最权威的比赛之一。</p></li><li><p>而在ImageNet竞赛停办后，COCO竞赛就成为是当前物体识别、检测等领域的一个最权威、<br>最重要的标杆，也是目前该领域在国际上唯一能汇集Google、微软、Facebook以及国内外<br>众多顶尖院校和优秀创新企业共同参与的大赛</p></li><li><p>目前为止有语义分割的最大数据集，提供的类别有 80 类，有超过 33 万张图片，其中 20 万张有标注，<br>整个数据集中个体的数目超过 150 万个。</p></li><li><p>官网：<a href="http://cocodataset.org/">http://cocodataset.org/</a></p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125091517230-248704637.png"></p></li></ul></blockquote><p>• <strong>BDD100K: A Large-scale Diverse Driving Video Database</strong></p><blockquote><p>URL:<a href="https://bair.berkeley.edu/blog/2018/05/30/bdd/">https://bair.berkeley.edu/blog/2018/05/30/bdd/</a></p><p>2018年5月伯克利大学AI实验室（BAIR）发布了目前最大规模、内容最具多样性的公开驾驶数据集BDD100K，同时设计了一个图片标注系统。<br>BDD100K 数据集包含10万段高清视频，每个视频约40秒，720p，30 fps 。每个视频的第10秒对关键帧进行采样，得到10万张图片（图片尺寸：1280720 ），并进行标注。</p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125091547171-1846849121.png"></p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125091612840-661000243.png"></p></blockquote><p>• <strong>Cambridge-driving Labeled Video Database (CamVid)</strong></p><blockquote><p>CamVid是第一个具有目标类别语义标签的视频集合。<br>数据库提供32个ground truth语义标签，将每个像素与语义类别之一相关联。<br>该数据库解决了对实验数据的需求，以定量评估新兴算法。 数据是从驾驶汽车的角度拍摄的。<br>包含戴姆勒在内的三家德国单位联合提供，包含50多个城市的立体视觉数据；像素级标注；提供算法评估接口。</p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125091645161-1439752352.png"></p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125091709623-1726128362.png"></p></blockquote><p>• <strong>Cityscapes Dataset</strong></p><p>• <strong>Mapillary Vistas Dataset</strong></p><p>Mapillary Vistas是世界上最大最多样化的像素精确和特定实例标注的街道级图像公开数据集。</p><blockquote><p> <img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125091736510-777967798.png"></p></blockquote><p>• <strong>ApolloScape Scene Parsing</strong></p><blockquote><ul><li><p>百度公司提供的ApolloScape数据集将包括具有高分辨率图像和每像素标注的RGB视频，具有语义<br>分割的测量级密集3D点，立体视频和全景图像。</p></li><li><p>Scene Parsing数据集是ApolloScape的一部分，它为高级自动驾驶研究提供了一套工具和数据集。</p></li><li><p>场景解析旨在为图像中的每个像素或点云中的每个点分配类别（语义）标签。<br><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125091848426-1798961986.png"></p></li></ul></blockquote><hr><h3 id="2-2-DeepLabv3-语义分割原理"><a href="#2-2-DeepLabv3-语义分割原理" class="headerlink" title="2.2 DeepLabv3+语义分割原理"></a>2.2 DeepLabv3+语义分割原理</h3><h4 id="编码器-解码器-encoder-decoder-结构"><a href="#编码器-解码器-encoder-decoder-结构" class="headerlink" title="编码器/解码器(encoder/decoder)结构"></a>编码器/解码器(encoder/decoder)结构</h4><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220210085317739-1721115068.png"></p><h4 id="卷积-Convolution-运算"><a href="#卷积-Convolution-运算" class="headerlink" title="卷积(Convolution)运算"></a>卷积(Convolution)运算</h4><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125091910022-1933320757.jpg"></p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125091946055-397486053.png"></p><p>语义分割网络中引入膨胀卷积</p><ul><li>增大网络的感受野</li><li>特征图像尺寸的损失</li></ul><p>采用不同的方式来增大神经元的感受野</p><ul><li>传统卷积通过添加池化层</li><li>膨胀卷积在卷积核中插入零元素，对卷积核上采样，可避免池化层引起的信息损失</li></ul><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125092008024-188984546.png"></p><p>(a) 在低分辨率输入特征图上使用标准卷积(rate=1)进行稀疏特征提取<br>(b) 在高分辨率输入特征图上利用rate = 2的膨胀卷积进行密集特征提取</p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125092026766-393727078.png"></p><p><strong>顶行</strong>：在低分辨率输入特征图上使用标准卷积进行稀疏特征提取<br><strong>底行</strong>：在高分辨率输入特征图上利用r<strong>ate = 2的膨胀卷积</strong>进行密集特征提取</p><h4 id="DeepLabv3介绍"><a href="#DeepLabv3介绍" class="headerlink" title="DeepLabv3介绍"></a><strong>DeepLabv3介绍</strong></h4><blockquote><p>Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam.<br>Rethinking Atrous Convolution for Semantic Image Segmentation. CVPR, 2017<br><a href="https://arxiv.org/abs/1706.05587">https://arxiv.org/abs/1706.05587</a></p></blockquote><p><strong>主要特点</strong></p><ul><li>采用预训练的ResNet-50, 或ResNet-101来提取特征</li><li>修改第4个残差块，采用膨胀卷积(模块内的三个卷积采用不同的膨胀率）</li><li>加入image-level的ASPP</li></ul><hr><p><strong>获取多尺度上下文的架构比较</strong></p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125092048098-323509928.jpg"></p><p>(a) <strong>图像金字塔</strong>(如SIFT)<br>(b) <strong>编码-解码框架</strong><br>(c) <strong>采用不同尺度的膨胀卷积</strong><br>(d) <strong>空间金字塔池化</strong><br><strong>空间金字塔池化</strong>：使得任意大小的特征图利用多尺度特征提取都能够转换成固定大小的特征向量</p><hr><p><strong>采用级联模块和带孔卷积提取多尺度信息</strong></p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125092105500-676373412.jpg"></p><ul><li>(a) 传统卷积，随着深度增大，特征图尺寸减小</li><li>(b) 采用带孔卷积，可避免特征图尺寸缩小</li><li>output stride: the ratio of input image spatial resolution to the final output resolution<br>(before global pooling or fully connected layer). </li></ul><p><strong>采用有ASPP的并行模块增加图像级特征</strong></p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125092122091-1610020861.png"></p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220125092136851-263084213.png"></p><hr><h4 id="DeepLab-v3-介绍"><a href="#DeepLab-v3-介绍" class="headerlink" title="DeepLab-v3+介绍"></a>DeepLab-v3+介绍</h4><blockquote><p>Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam.<br>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation. Feb. 2018<br><a href="https://arxiv.org/abs/1802.02611v1">https://arxiv.org/abs/1802.02611v1</a></p></blockquote><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220124192742218-1577063977.jpg"></p><p>We improve DeepLabv3, which employs the spatial pyramid pooling module (a),<br>with the encoder-decoder structure (b).</p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220126162726454-749811330.png"></p><ul><li>通过添加一个简单有效的解码器模块来扩展DeepLab-v3，以优化分割结果，尤其是沿着目标边界</li><li>将深度可分离卷积（参考Xception）应用于ASPP和解码器模块，从而产生用于语义分割的更快和更强的<br>编码器 - 解码器网络</li></ul><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220126163413800-766313907.png"></p><p>3×3 Depthwise separable convolution decomposes a standard convolution into<br>(a) a depthwise convolution (applying a single filter for each input channel) and<br>(b) a pointwise convolution (combining the outputs from depthwise convolution across<br>channels).<br>(c) In this work, we explore atrous separable convolution where atrous convolution<br>is adopted in the depthwise convolution, as shown in (c) with rate = 2.</p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220124194413213-1103244284.png"></p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220124194432776-1103929899.png"></p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220124194500613-1240970450.png"></p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220124194521220-697113933.png"></p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220124194536173-179020837.png"></p><hr><h4 id="DeepLab系列语义分割架构模型比较"><a href="#DeepLab系列语义分割架构模型比较" class="headerlink" title="DeepLab系列语义分割架构模型比较"></a><strong>DeepLab系列语义分割架构模型比较</strong></h4><ul><li>1.DeepLabv1：使用空洞卷积来明确控制在深度卷积神经网络中计算特征响应的分辨率。</li><li>1.DeepLabv2：使用ASPP以多个采样率和有效视野的滤波器对多个尺度的目标进行鲁棒分割</li><li>3.DeepLabv3：使用图像级别特征增强ASPP模块以捕获更长距离的信息。还引入BN，以促进训练。</li><li>4.DeepLabv3+：用一个简单有效的解码器模块扩展DeepLabv3优化细分结果，尤其是沿目标边界。<br>此外，在这种编码器 - 解码器结构中，可以通过空洞卷积任意地控制所提取的编码器<br>特征的分辨率，以折衷准确率和运行时间。</li></ul><hr><h2 id="三、环境安装与测试"><a href="#三、环境安装与测试" class="headerlink" title="三、环境安装与测试"></a>三、环境安装与测试</h2><h3 id="3-1-安装pytorch"><a href="#3-1-安装pytorch" class="headerlink" title="3.1 安装pytorch"></a>3.1 安装pytorch</h3><h4 id="3-1-1-安装Anaconda"><a href="#3-1-1-安装Anaconda" class="headerlink" title="3.1.1 安装Anaconda"></a>3.1.1 安装Anaconda</h4><p>Anaconda 是一个用于科学计算的 Python 发行版，支持 Linux, Mac, Windows, 包含了众多流行的科学<br>计算、数据分析的 Python 包。</p><ol><li><p>先去官方地址下载好对应的安装包<br>下载地址:<a href="https://www.anaconda.com/download/#linux">https://www.anaconda.com/download/#linux</a></p></li><li><p>然后安装anaconda</p></li></ol>  <pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">bash ~&#x2F;Downloads&#x2F;Anaconda3-2021.05-Linux-x86_64.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>  anaconda会自动将环境变量添加到PATH里面，如果后面你发现执行conda提示没有该命令，那么<br>  你需要执行命令 source <del>/.bashrc 更新环境变量，就可以正常使用了。<br>  如果发现这样还是没用，那么需要添加环境变量。<br>  编辑</del>/.bashrc 文件，在最后面加上</p>  <pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">export PATH&#x3D;&#x2F;home&#x2F;bai&#x2F;anaconda3&#x2F;bin:$PATH<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>  保存退出后执行： source ~/.bashrc<br>  再次输入 conda list 测试看看，应该没有问题。</p><hr><h4 id="3-1-2-添加Anaconda国内镜像配置"><a href="#3-1-2-添加Anaconda国内镜像配置" class="headerlink" title="3.1.2 添加Anaconda国内镜像配置"></a>3.1.2 添加Anaconda国内镜像配置</h4><p>清华TUNA提供了 Anaconda 仓库的镜像，运行以下命令:</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F;conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main&#x2F;conda config --set show_channel_urls yes<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3-1-3-安装Pytorch"><a href="#3-1-3-安装Pytorch" class="headerlink" title="3.1.3 安装Pytorch"></a>3.1.3 安装Pytorch</h4><p>首先创建一个anaconda虚拟环境，环境名字可自己确定，这里本人使用mypytorch作为环境名:</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda create -n mypytorch python&#x3D;3.8<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>安装成功后激活mypytorch环境：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda activate mypytorch<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在所创建的虚拟环境下安装, 执行命令：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda install pytorch torchvision cudatoolkit&#x3D;10.2 -c pytorch<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意：10.2处应为cuda的安装版本号<br>编辑~/.bashrc 文件，设置使用mypytorch环境下的python3.8</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">alias python&#x3D;&#39;&#x2F;home&#x2F;linxu&#x2F;anaconda3&#x2F;envs&#x2F;mypytorch&#x2F;bin&#x2F;python3.8&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意：python路径应改为自己机器上的路径<br>保存退出后执行： <code>source ~/.bashrc</code><br>该命令将自动回到base环境，再执行 conda activate mypytorch 到mypytorch环境。</p><hr><h3 id="3-2-安装Deeplabsv3-及测试"><a href="#3-2-安装Deeplabsv3-及测试" class="headerlink" title="3.2 安装Deeplabsv3+及测试"></a>3.2 安装Deeplabsv3+及测试</h3><h4 id="3-2-1-克隆和安装deeplabv3"><a href="#3-2-1-克隆和安装deeplabv3" class="headerlink" title="3.2.1 克隆和安装deeplabv3+"></a>3.2.1 克隆和安装deeplabv3+</h4><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git clone https:&#x2F;&#x2F;github.com&#x2F;VainF&#x2F;DeepLabV3Plus-Pytorch.git <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在路径DeepLabV3Plus-Pytorch下执行</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">pip install -r requirements.txt -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="3-2-2-测试图片"><a href="#3-2-2-测试图片" class="headerlink" title="3.2.2 测试图片"></a>3.2.2 测试图片</h4><h5 id="3-2-2-1-下载PASCAL-VOC文件"><a href="#3-2-2-1-下载PASCAL-VOC文件" class="headerlink" title="3.2.2.1 下载PASCAL VOC文件"></a>3.2.2.1 下载PASCAL VOC文件</h5><p>说明：本课程数据集和程序文件可从百度网盘下载，网盘链接如下：</p><pre class="line-numbers language-baidu" data-language="baidu"><code class="language-baidu">链接：https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1WWEyIT2aC8HB20uBhBsVbQ 提取码：srpb <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>从百度网盘下载, 下载到DeepLabV3Plus-Pytorch/datasets/data目录下并解压</p><ul><li>VOCtrainval_06-Nov-2007.tar </li><li>VOCtrainval_11-May-2012.tar </li><li>VOCtest_06-Nov-2007.tar</li></ul><h5 id="3-2-2-2-解压建立数据集"><a href="#3-2-2-2-解压建立数据集" class="headerlink" title="3.2.2.2 解压建立数据集"></a>3.2.2.2 解压建立数据集</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">tar xvf VOCtrainval_06-Nov-2007.tartar xvf VOCtest_06-Nov-2007.tartar xvf VOCtrainval_11-May-2012.tar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>PASCAL VOC数据集的目录结构:<br> 建立文件夹层次为 VOCdevkit/VOC2007 和 VOCdevkit/VOC2012<br> VOC2007和VOC2012下面的文件夹：Annotations，JPEGImages和ImageSets</p><p>其中JPEGImages放所有的数据集图片；<br>Annotations放所有的xml标记文件；<br>SegmentationClass放标注的数据集掩码文件；<br>ImageSets/Segmentation下存放训练集、验证集、测试集划分文件<br>train.txt给出了训练集图片文件的列表（不含文件名后缀）<br>val.txt给出了验证集图片文件的列表<br>trainval.txt给出了训练集和验证集图片文件的列表<br>test.txt给出了测试集图片文件的列表 </p><h5 id="3-2-2-3-下载预训练权重文件"><a href="#3-2-2-3-下载预训练权重文件" class="headerlink" title="3.2.2.3 下载预训练权重文件"></a>3.2.2.3 下载预训练权重文件</h5><p>放置在DeepLabV3Plus-Pytorch下新建的weights文件夹下，例如<br>best_deeplabv3plus_mobilenet_voc_os16.pth<br>best_deeplabv3_mobilenet_voc_os16.pth<br>best_deeplabv3plus_resnet50_voc_os16<br>best_deeplabv3plus_resnet101_voc_os16</p><h5 id="3-2-2-4-测试图片"><a href="#3-2-2-4-测试图片" class="headerlink" title="3.2.2.4 测试图片"></a>3.2.2.4 测试图片</h5><p>使用deeplabv3plus_mobilenet模型</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python predict.py --input datasets&#x2F;data&#x2F;VOCdevkit&#x2F;VOC2007&#x2F;JPEGImages&#x2F;000001.jpg --dataset voc --model deeplabv3plus_mobilenet --ckpt weights&#x2F;best_deeplabv3plus_mobilenet_voc_os16.pth --save_val_results_to test_results<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>使用deeplabv3_mobilenet模型</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python predict.py --input datasets&#x2F;data&#x2F;VOCdevkit&#x2F;VOC2007&#x2F;JPEGImages&#x2F;000001.jpg --dataset voc --model deeplabv3_mobilenet --ckpt weights&#x2F;best_deeplabv3_mobilenet_voc_os16.pth --save_val_results_to test_results<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><h2 id="四、labelme图像标注及格式转换"><a href="#四、labelme图像标注及格式转换" class="headerlink" title="四、labelme图像标注及格式转换"></a>四、labelme图像标注及格式转换</h2><h3 id="4-1-labelme图像标注工具的安装与使用"><a href="#4-1-labelme图像标注工具的安装与使用" class="headerlink" title="4.1 labelme图像标注工具的安装与使用"></a>4.1 labelme图像标注工具的安装与使用</h3><h4 id="4-1-1-安装图像标注工具labelme"><a href="#4-1-1-安装图像标注工具labelme" class="headerlink" title="4.1.1 安装图像标注工具labelme"></a>4.1.1 安装图像标注工具labelme</h4><p>Ubuntu下的安装：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">pip install pyqt5<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">pip install labelme<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>如果安装过程中提示缺少某个包，可再安装上，如：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">pip install pyyaml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>如上述安装方法不能成功，使用下面的命令安装：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">pip install git+https:&#x2F;&#x2F;github.com&#x2F;wkentaro&#x2F;labelme.git<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="4-1-2-使用labelme进行图像标注"><a href="#4-1-2-使用labelme进行图像标注" class="headerlink" title="4.1.2 使用labelme进行图像标注"></a>4.1.2 使用labelme进行图像标注</h4><p>执行：</p><pre class="line-numbers language-shel" data-language="shel"><code class="language-shel">labelme<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>标注后生成json文件<br>课程pothole项目案例的数据集为1280*720的图片，136张用于训练，16张用于测试。<br>这里的数据集有5个类别：”car”, “dashedline”, “midlane”, “pothole”, “rightlane”<br>数据集图像文件及标注的json文件放置在~/mydataset目录下</p><h3 id="4-2-标注数据格式转换"><a href="#4-2-标注数据格式转换" class="headerlink" title="4.2 标注数据格式转换"></a>4.2 标注数据格式转换</h3><h4 id="4-2-1-图像标注后的数据转换"><a href="#4-2-1-图像标注后的数据转换" class="headerlink" title="4.2.1 图像标注后的数据转换"></a>4.2.1 图像标注后的数据转换</h4><p>在mydataset路径下执行</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python labelme2voc.py roadscene_train roadscene_train&#x2F;data_dataset_voc --labels labels.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220126140912210-62495580.png"></p><pre class="line-numbers language-none"><code class="language-none">python labelme2voc.py roadscene_val roadscene_val&#x2F;data_dataset_voc --labels labels.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220126141148458-21184392.png"></p><h4 id="4-2-2-项目数据准备"><a href="#4-2-2-项目数据准备" class="headerlink" title="4.2.2 项目数据准备"></a>4.2.2 项目数据准备</h4><p>把转成数据集的目录结构准备成PASCAL VOC目录结构格式。<br>在DeepLabV3Plus-Pytorch/datasets/data文件夹下，创建目录结构如下：<br>└── VOCdevkit<br> ├── VOC2007<br> ├── ImageSets<br> ├── JPEGImages<br> └── SegmentationClass<br><img src="https://img2022.cnblogs.com/blog/1571518/202201/1571518-20220126141237739-1080909454.png"></p><p>其中：<br>JPEGImages放所有的数据集图片；<br><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220126144823489-2027338592.png"><br>SegmentationClass放标注的数据集掩码文件；<br><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220126144843966-1914633980.png"><br>ImageSets/Segmentation下存放训练集、验证集、测试集划分文件<br><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220126144903307-1379630015.png"><br>train.txt给出了训练集图片文件的列表（不含文件名后缀）<br>val.txt给出了验证集图片文件的列表<br>trainval.txt给出了训练集和验证集图片文件的列表<br>test.txt给出了测试集图片文件的列表<br>课程中train.txt包括136 张图片列表；<br>trainval.txt包括136 张+16张图片列表；<br>val.txt和test.txt内容相同，包括16张图片列表</p><hr><h2 id="五、deeplabv3-网络训练和测试"><a href="#五、deeplabv3-网络训练和测试" class="headerlink" title="五、deeplabv3+网络训练和测试"></a>五、deeplabv3+网络训练和测试</h2><h3 id="5-1-网络训练"><a href="#5-1-网络训练" class="headerlink" title="5.1 网络训练"></a>5.1 网络训练</h3><h4 id="5-1-1-安装训练可视化工具visdom"><a href="#5-1-1-安装训练可视化工具visdom" class="headerlink" title="5.1.1 安装训练可视化工具visdom"></a>5.1.1 安装训练可视化工具visdom</h4><p>1）下载static.zip文件到anaconda3/envs/mypytorch/lib/python3.8/site-packages/visdom并解压<br>更正：此处应该为anaconda3/envs/mypytorch/lib/python3.8/site-packages/visdom<br>2）注释掉server.py文件中函数download_scripts_and_run()中的一句<br> #download_scripts()<br>3）启动visdom server</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># Run visdom server python -m visdom.server<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="5-2-训练网络"><a href="#5-2-训练网络" class="headerlink" title="5.2 训练网络"></a>5.2 训练网络</h3><p><strong>使用deeplabv3plus_mobilenet模型</strong></p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python main.py --model deeplabv3plus_mobilenet --enable_vis --vis_port 8097 --gpu_id 0 --year 2007 --crop_val --lr 0.01 --crop_size 513 --batch_size 16 --output_stride 16 --num_classes 6 --total_itrs 1000 --ckpt weights&#x2F;best_deeplabv3plus_mobilenet_voc_os16.pth<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>如果出现RuntimeError: CUDA out of memory. Tried to allocate 338.00 MiB (GPU 0; 9.78 GiB total capacity; 7.36 GiB already allocated; 282.06 MiB free; 7.44 GiB reserved in total by PyTorch)，<br>说明GPU 显存不够，则可将crop_size 513适当调低，建议设置为300，224，112等。</p><p>其中num_classes设置为类别数+1<br>训练好的权重在checkpoints文件夹下</p><p><strong>使用deeplabv3_mobilenet模型</strong></p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python main.py --model deeplabv3_mobilenet --enable_vis --vis_port 8097 --gpu_id 0 --year 2007 --crop_val --lr 0.01 --crop_size 513 --batch_size 16 --output_stride 16 --num_classes 6 --total_itrs 1000 --ckpt weights&#x2F;best_deeplabv3_mobilenet_voc_os16.pth<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220126143212838-168843706.png"></p><p><strong>使用deeplabv3plus_resnet50模型</strong></p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python main.py --model deeplabv3plus_resnet50 --enable_vis --vis_port 8097 --gpu_id 0 --year 2007 --crop_val --lr 0.01 --crop_size 513 --batch_size 8 --output_stride 16 --num_classes 6 --total_itrs 2000 --ckpt weights&#x2F;best_deeplabv3plus_resnet50_voc_os16.pth<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><h2 id="六、网络模型测试"><a href="#六、网络模型测试" class="headerlink" title="六、网络模型测试"></a>六、网络模型测试</h2><h4 id="6-1-性能指标统计"><a href="#6-1-性能指标统计" class="headerlink" title="6.1 性能指标统计"></a>6.1 性能指标统计</h4><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python main.py --model deeplabv3plus_mobilenet --gpu_id 0 --year 2007 --crop_val --lr 0.01 --crop_size 513 --batch_size 16 --output_stride 16 --ckpt checkpoints&#x2F;best_deeplabv3plus_mobilenet_voc_os16.pth --test_only --save_val_results<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="6-2-图片测试"><a href="#6-2-图片测试" class="headerlink" title="6.2 图片测试"></a>6.2 图片测试</h4><h5 id="单张图片测试"><a href="#单张图片测试" class="headerlink" title="单张图片测试"></a>单张图片测试</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python predict.py --input datasets&#x2F;data&#x2F;VOCdevkit&#x2F;VOC2007&#x2F;JPEGImages&#x2F;img001.jpg --dataset voc --model deeplabv3plus_mobilenet --ckpt checkpoints&#x2F;best_deeplabv3plus_mobilenet_voc_os16.pth --save_val_results_to test_results1 --crop_size 513<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="https://img2022.cnblogs.com/blog/1571518/202201/1571518-20220126144348771-646069007.png"></p><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220126144448239-197253023.png"></p><h5 id="多张图片测试"><a href="#多张图片测试" class="headerlink" title="多张图片测试"></a>多张图片测试</h5><p>如果是jpg图片，修改predict.py中的一句</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">files &#x3D; glob(os.path.join(opts.input, &#39;**&#x2F;*.png&#39;), recursive&#x3D;True)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>为</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">files &#x3D; glob(os.path.join(opts.input, &#39;**&#x2F;*.jpg&#39;), recursive&#x3D;True)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后，执行命令<br>使用deeplabv3plus_mobilenet模型</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python predict.py --input datasets&#x2F;data&#x2F;VOCdevkit&#x2F;VOC2007&#x2F;JPEGImages --dataset voc --model deeplabv3plus_mobilenet --ckpt checkpoints&#x2F;best_deeplabv3plus_mobilenet_voc_os16.pth --save_val_results_to test_results2 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>使用deeplabv3plus_resnet50模型</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python predict.py --input datasets&#x2F;data&#x2F;VOCdevkit&#x2F;VOC2007&#x2F;JPEGImages --dataset voc --model deeplabv3plus_resnet50 --ckpt checkpoints&#x2F;best_deeplabv3plus_resnet50_voc_os16.pth --save_val_results_to test_results2 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/blog/blog/2022/01/26/deeplabsv3-xun-lian-zong-jie/Deeplabsv3-%E8%AE%AD%E7%BB%83%E6%80%BB%E7%BB%93/1571518-20220126144552840-1848277601.png"></p><p>参考材料：<br>[1] 白勇老师课程及课件:<a href="https://edu.csdn.net/course/detail/36456">https://edu.csdn.net/course/detail/36456</a><br>[2] <a href="https://paperswithcode.com/method/deeplabv3">https://paperswithcode.com/method/deeplabv3</a><br>[3] Code:<a href="https://github.com/VainF/DeepLabV3Plus-Pytorch">https://github.com/VainF/DeepLabV3Plus-Pytorch</a></p>]]></content>
      
      
      <categories>
          
          <category> Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 炼丹术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kaggle介绍</title>
      <link href="/blog/2022/01/23/kaggle-jie-shao/"/>
      <url>/blog/2022/01/23/kaggle-jie-shao/</url>
      
        <content type="html"><![CDATA[<h1 id="Kaggle竞赛介绍"><a href="#Kaggle竞赛介绍" class="headerlink" title="Kaggle竞赛介绍"></a>Kaggle竞赛介绍</h1><p><img src="/blog/blog/2022/01/23/kaggle-jie-shao/Kaggle%E4%BB%8B%E7%BB%8D/1571518-20220123153721692-2108218298.jpg"></p><hr><h2 id="Kaggle简介"><a href="#Kaggle简介" class="headerlink" title="Kaggle简介"></a>Kaggle简介</h2><p><img src="/blog/blog/2022/01/23/kaggle-jie-shao/Kaggle%E4%BB%8B%E7%BB%8D/1571518-20220123153835837-1513570413.jpg"></p><p>如果Github相当于是软件项目和工程的国际开源代码平台，<br>那么Kaggle就等同于是针对数据科学相关的国际集智与比赛平台。<br>一些大厂和研究机构等面临一些难题的时候，就会以类似的比赛形式众包出去发布在Kaggle。<br>用户会以个人或组队team的方式进行解决方案的挑战。</p><p>对于数据科学和人工智能平台，国内有阿里背书的TIANCHI天池竞赛、与CCF等合作的Datafountain以及DC竞赛、kesci等。<br>但从影响力和受众人群等方面来看，Kaggle面向全球且用户分布广泛众多，Kaggle作为一个比赛社区、比赛平台与国内的一些平台比赛有其独特的优势。</p><p><strong>推荐使用！Kaggle的几个理由</strong>：</p><ul><li>1.数据量和使用用户广泛</li><li>2.影响力大=&gt;拿奖项的含金量高，认可度高</li><li>3.数据集和解决方案多，有庞大的交流社区</li></ul><h3 id="Kaggle的几种常见赛题："><a href="#Kaggle的几种常见赛题：" class="headerlink" title="Kaggle的几种常见赛题："></a>Kaggle的几种常见赛题：</h3><p><img src="/blog/blog/2022/01/23/kaggle-jie-shao/Kaggle%E4%BB%8B%E7%BB%8D/1571518-20220123154730841-948366610.png"></p><ul><li><p><strong>01.Featured</strong></p><p>商业或科研难题，奖金一般较为丰厚<br><img src="/blog/blog/2022/01/23/kaggle-jie-shao/Kaggle%E4%BB%8B%E7%BB%8D/1571518-20220123154849556-936947805.png"></p></li><li><p><strong>02.Analytics</strong><br>主要针对数据分析与数据挖掘等任务<br><img src="/blog/blog/2022/01/23/kaggle-jie-shao/Kaggle%E4%BB%8B%E7%BB%8D/1571518-20220123155153772-1877653863.png"></p></li><li><p><strong>03.Research</strong><br>科研和学术性较强的比赛，一般需要较强的领域和专业知识<br><img src="/blog/blog/2022/01/23/kaggle-jie-shao/Kaggle%E4%BB%8B%E7%BB%8D/1571518-20220123154935824-971773830.png"></p></li><li><p><strong>04.PlayGround</strong><br>提供一些简单的任务用于熟悉平台和比赛<br><img src="/blog/blog/2022/01/23/kaggle-jie-shao/Kaggle%E4%BB%8B%E7%BB%8D/1571518-20220123155100269-1856444684.png"></p></li><li><p><strong>05.Getting Started</strong><br>提供一些简单的任务用于熟悉平台和必死爱<br><img src="/blog/blog/2022/01/23/kaggle-jie-shao/Kaggle%E4%BB%8B%E7%BB%8D/1571518-20220123155039074-812900123.png"></p></li><li><p><strong>06.Community</strong><br>适用于任何人都可以举办的社区比赛<br><img src="/blog/blog/2022/01/23/kaggle-jie-shao/Kaggle%E4%BB%8B%E7%BB%8D/1571518-20220123155534851-1784812919.png"></p></li></ul><p>核心的打比赛主要是前三种：Featured、Recruitment、Research.</p><p>从难度上来看的排序是这样的：06&lt;05&lt;04&lt;02&lt;01=03</p><h3 id="赛题分类2：根据比赛的提交方式进行分类"><a href="#赛题分类2：根据比赛的提交方式进行分类" class="headerlink" title="赛题分类2：根据比赛的提交方式进行分类"></a>赛题分类2：根据比赛的提交方式进行分类</h3><ul><li><p><strong>在线提交比赛</strong><br>多数比赛为避免人工打标或作弊的情况，为保证公平性，以在线提交方式为主。<br>一般以Notebook的方式进行提交，测试集对于参赛者是不可见的。<br>提交之后以评估分数进行排名分为A榜、B榜，<br>其中A榜是带有验证测试性质的，是支持实时更新的，可能会存在过拟合的情况。<br>一般最终结果为B榜为主，<br>但也会按一定比例参考A榜的数值进行最终成绩的统计。</p></li><li><p><strong>离线提交比赛</strong><br>离线给出数据集的相关预测和比赛评估参数，一般以CSV文件进行上传提交。</p></li></ul><h3 id="赛题分类3：根据比赛的任务进行分类"><a href="#赛题分类3：根据比赛的任务进行分类" class="headerlink" title="赛题分类3：根据比赛的任务进行分类"></a>赛题分类3：根据比赛的任务进行分类</h3><ul><li><strong>数据挖掘</strong><br>对于结构化的数据进行算法挖掘与信息提取</li><li>** 图像相关**：计算机视觉领域相关任务<br>例如图像分类、目标定位、目标检测、图像分割、实例分割等</li><li><strong>语音相关</strong>：语音信号处理相关<br>例如语音识别、声纹识别等</li><li><strong>自然语言</strong><br>自然语言处理、自然语言理解、自然语言分析等</li></ul><h3 id="称号和奖牌"><a href="#称号和奖牌" class="headerlink" title="称号和奖牌"></a>称号和奖牌</h3><p>在打比赛的过程当中，如果能够排上榜单，就会得到相应的奖牌和称号：<br><img src="/blog/blog/2022/01/23/kaggle-jie-shao/Kaggle%E4%BB%8B%E7%BB%8D/1571518-20220123154525174-1495667619.png"></p><p>其中奖牌分为：金牌、银牌、铜牌<br>而称号则按照经验值和积分等，依次排为：<br>User =&gt; Nov =&gt; Con =&gt; EX =&gt; M =&gt; GM</p><h3 id="初学者怎么用Kaggle"><a href="#初学者怎么用Kaggle" class="headerlink" title="初学者怎么用Kaggle"></a>初学者怎么用Kaggle</h3><ul><li><strong>Overview</strong>: 比赛方对这次比赛整体概况的介绍，主要需要解决什么样的问题以及难点是什么，<br>并且说明比赛的评估方式和提交的时间节点等信息。<br><img src="/blog/blog/2022/01/23/kaggle-jie-shao/Kaggle%E4%BB%8B%E7%BB%8D/1571518-20220123153932008-2018434341.png"></li></ul><ul><li><p><strong>Data</strong>: 对数据进行基本的介绍，提供数据预览和下载的地址<br><img src="/blog/blog/2022/01/23/kaggle-jie-shao/Kaggle%E4%BB%8B%E7%BB%8D/1571518-20220123154015229-665276875.png"></p></li><li><p><strong>Code</strong>: 参赛者在比赛中开源分享出来的代码,一般以Notebook为主<br><img src="/blog/blog/2022/01/23/kaggle-jie-shao/Kaggle%E4%BB%8B%E7%BB%8D/1571518-20220123154221007-1071251655.png"></p></li><li><p><strong>Discussion</strong>：讨论交流社区<br><img src="/blog/blog/2022/01/23/kaggle-jie-shao/Kaggle%E4%BB%8B%E7%BB%8D/1571518-20220123154258677-989243007.png"></p></li><li><p><strong>Leaderboard</strong>: 排行榜：A榜|B榜<br>A榜表现!=B榜表现<br><img src="/blog/blog/2022/01/23/kaggle-jie-shao/Kaggle%E4%BB%8B%E7%BB%8D/1571518-20220123154331167-990290117.png"></p></li><li><p><strong>Rules</strong>: 比赛相关细则<br><img src="/blog/blog/2022/01/23/kaggle-jie-shao/Kaggle%E4%BB%8B%E7%BB%8D/1571518-20220123154352118-587651214.png"></p></li><li><p><strong>Team</strong>: 参赛队伍</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法竞赛 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kaggle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EfficientDet训练模型总结</title>
      <link href="/blog/2022/01/20/efficientdet-xun-lian-mo-xing-zong-jie/"/>
      <url>/blog/2022/01/20/efficientdet-xun-lian-mo-xing-zong-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="EfficientDet训练模型学习总结"><a href="#EfficientDet训练模型学习总结" class="headerlink" title="EfficientDet训练模型学习总结"></a>EfficientDet训练模型学习总结</h2><h2 id="1-Introduction简介"><a href="#1-Introduction简介" class="headerlink" title="1.Introduction简介"></a>1.Introduction简介</h2><p>pytorch用SOTA实时重新实现官方<a href="https://github.com/google/automl/tree/master/efficientdet">EfficientDet</a>，原文链接：<a href="https://arxiv.org/abs/1911.09070">https</a> : //arxiv.org/abs/1911.09070</p><p>关于 EfficientNetV2：</p><blockquote><p>EfficientNetV2 是一个新的卷积网络家族，与之前的模型相比，具有更快的训练速度和更好的参数效率。为了开发这一系列模型，我们结合使用训练感知神经架构搜索和缩放，共同优化训练速度和参数效率。这些模型是从富含新操作（例如 Fused-MBConv）的搜索空间中搜索的。</p></blockquote><p>这里是一个比较：</p><p><img src="/blog/blog/2022/01/20/efficientdet-xun-lian-mo-xing-zong-jie/EfficientDet%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/1571518-20211107145805791-919840886.png"></p><ul><li>结果表现：</li></ul><table><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td><img src="/blog/blog/2022/01/20/efficientdet-xun-lian-mo-xing-zong-jie/EfficientDet%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/1571518-20211107150127161-877118924.jpg"></td><td><img src="/blog/blog/2022/01/20/efficientdet-xun-lian-mo-xing-zong-jie/EfficientDet%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/1571518-20211107150114556-1712376125.jpg"></td><td><img src="/blog/blog/2022/01/20/efficientdet-xun-lian-mo-xing-zong-jie/EfficientDet%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/1571518-20211107150109564-2126745531.jpg"></td></tr><tr><td><img src="/blog/blog/2022/01/20/efficientdet-xun-lian-mo-xing-zong-jie/EfficientDet%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/1571518-20211107150202931-849803245.jpg"></td><td><img src="/blog/blog/2022/01/20/efficientdet-xun-lian-mo-xing-zong-jie/EfficientDet%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/1571518-20211107150153680-1502926812.jpg"></td><td><img src="/blog/blog/2022/01/20/efficientdet-xun-lian-mo-xing-zong-jie/EfficientDet%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/1571518-20211107150132022-942779202.jpg" alt="sss"></td></tr><tr><td></td><td></td><td></td></tr></tbody></table><h3 id="1-1-Requirements"><a href="#1-1-Requirements" class="headerlink" title="1.1 Requirements"></a>1.1 Requirements</h3><ul><li><strong>python 3.6</strong></li><li><strong>pytorch 1.2</strong></li><li><strong>opencv (cv2)</strong></li><li><strong>tensorboard</strong></li><li><strong>tensorboardX</strong> (This library could be skipped if you do not use SummaryWriter)</li><li><strong>pycocotools</strong></li><li><strong>efficientnet_pytorch</strong></li></ul><h3 id="1-2-预训练权重和基准"><a href="#1-2-预训练权重和基准" class="headerlink" title="1.2 预训练权重和基准"></a>1.2 预训练权重和基准</h3><p>性能非常接近论文，仍然是SOTA。</p><p>速度/FPS 测试包括没有jit/数据精度技巧的后处理时间。</p><table><thead><tr><th>coefficient</th><th>pth_download</th><th>GPU Mem(MB)</th><th>FPS</th><th>Extreme FPS (Batchsize 32)</th><th>mAP 0.5:0.95(this repo)</th><th>mAP 0.5:0.95(official)</th></tr></thead><tbody><tr><td>D0</td><td><a href="https://github.com/zylo117/Yet-Another-Efficient-Pytorch/releases/download/1.0/efficientdet-d0.pth">efficientdet-d0.pth</a></td><td>1049</td><td>36.20</td><td>163.14</td><td>33.1</td><td>33.8</td></tr><tr><td>D1</td><td><a href="https://github.com/zylo117/Yet-Another-Efficient-Pytorch/releases/download/1.0/efficientdet-d1.pth">efficientdet-d1.pth</a></td><td>1159</td><td>29.69</td><td>63.08</td><td>38.8</td><td>39.6</td></tr><tr><td>D2</td><td><a href="https://github.com/zylo117/Yet-Another-Efficient-Pytorch/releases/download/1.0/efficientdet-d2.pth">efficientdet-d2.pth</a></td><td>1321</td><td>26.50</td><td>40.99</td><td>42.1</td><td>43.0</td></tr><tr><td>D3</td><td><a href="https://github.com/zylo117/Yet-Another-Efficient-Pytorch/releases/download/1.0/efficientdet-d3.pth">efficientdet-d3.pth</a></td><td>1647</td><td>22.73</td><td>-</td><td>45.6</td><td>45.8</td></tr><tr><td>D4</td><td><a href="https://github.com/zylo117/Yet-Another-Efficient-Pytorch/releases/download/1.0/efficientdet-d4.pth">efficientdet-d4.pth</a></td><td>1903</td><td>14.75</td><td>-</td><td>48.8</td><td>49.4</td></tr><tr><td>D5</td><td><a href="https://github.com/zylo117/Yet-Another-Efficient-Pytorch/releases/download/1.0/efficientdet-d5.pth">efficientdet-d5.pth</a></td><td>2255</td><td>7.11</td><td>-</td><td>50.2</td><td>50.7</td></tr><tr><td>D6</td><td><a href="https://github.com/zylo117/Yet-Another-Efficient-Pytorch/releases/download/1.0/efficientdet-d6.pth">efficientdet-d6.pth</a></td><td>2985</td><td>5.30</td><td>-</td><td>50.7</td><td>51.7</td></tr><tr><td>D7</td><td><a href="https://github.com/zylo117/Yet-Another-Efficient-Pytorch/releases/download/1.2/efficientdet-d7.pth">efficientdet-d7.pth</a></td><td>3819</td><td>3.73</td><td>-</td><td>52.7</td><td>53.7</td></tr><tr><td>D7X</td><td><a href="https://github.com/zylo117/Yet-Another-Efficient-Pytorch/releases/download/1.2/efficientdet-d8.pth">efficientdet-d8.pth</a></td><td>3983</td><td>2.39</td><td>-</td><td>53.9</td><td>55.1</td></tr></tbody></table><h3 id="1-3-数据集-COCO格式"><a href="#1-3-数据集-COCO格式" class="headerlink" title="1.3 数据集(COCO格式)"></a>1.3 数据集(COCO格式)</h3><table><thead><tr><th>数据集</th><th>类别数量</th><th>#训练图像</th><th>#验证图片</th></tr></thead><tbody><tr><td>COCO2017</td><td>80</td><td>118k</td><td>5k</td></tr></tbody></table><p>在存储库下创建一个数据文件夹，</p><pre class="line-numbers language-none"><code class="language-none">cd &#123;repo_root&#125;mkdir data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li><p>COCO:从coco网站下载coco图片和注释。确保将文件按以下结构放置：</p><pre class="line-numbers language-none"><code class="language-none">COCO├── annotations│   ├── instances_train2017.json│   └── instances_val2017.json│── images    ├── train2017    └── val2017<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h2 id><a href="#" class="headerlink" title></a></h2><h2 id="2-Demo演示测试"><a href="#2-Demo演示测试" class="headerlink" title="2. Demo演示测试"></a>2. Demo演示测试</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 安装需求pip install pycocotools numpy opencv-python tqdm tensorboard tensorboardX pyyaml webcolors pip install torch&#x3D;&#x3D;1.4.0 pip install torchvision&#x3D;&#x3D;0.5.0 # 运行简单的推理脚本python Effectivedet_test.py <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-Training训练"><a href="#3-Training训练" class="headerlink" title="3. Training训练"></a>3. Training训练</h2><h3 id="3-1-准备数据集"><a href="#3-1-准备数据集" class="headerlink" title="3.1 准备数据集"></a>3.1 准备数据集</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 你的数据集结构应该是这样的datasets&#x2F;     -your_project_name&#x2F;         -train_set_name&#x2F;             -*.jpg         -val_set_name&#x2F;             -*.jpg         -annotations             -instances_&#123;train_set_name&#125;             .json -instances_&#123;val_set_name&#125;.json＃例如，coco2017数据集&#x2F;     -coco2017 &#x2F;         -train2017 &#x2F;             -000000000001.jpg             -000000000002.jpg             -000000000003.jpg         -val2017 &#x2F;             -000000000004.jpg             -000000000005.jpg             -000000000006.jpg         -annotations             -instances_train2017.json             -instances_val2017.json<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-2-手动设置项目的具体参数"><a href="#3-2-手动设置项目的具体参数" class="headerlink" title="3.2 手动设置项目的具体参数"></a>3.2 手动设置项目的具体参数</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 在&#39;projects&#39;文件夹下创建一个yml文件&#123;your_project_name&#125;.yml # 修改为&#39;coco.yml&#39; # 例如project_name: coco train_set: train2017 val_set: val2017 num_gpus: 4 # 0 表示使用cpu，1-N 表示使用显卡 # RGB顺序的mean和std，实际上这部分应该保持不变，只要你的数据集类似于coco。mean：[0.485, 0.456, 0.406]std：[0.229, 0.224, 0.225]# 这是coco anchors，如有必要，请更改，一般不建议修改anchors_scales: &#39;[2 ** 0, 2 ** (1.0 &#x2F; 3.0), 2 ** (2.0 &#x2F; 3.0)]&#39; anchors_ratios: &#39;[(1.0, 1.0), (1.4) , 0.7), (0.7, 1.4)]&#39;# 来自数据集中所有标签的对象，其顺序来自您的注释。# 它的索引必须与您的数据集的 category_id 匹配。# category_id 是one_indexed, # 比如这里&#39;car&#39;的index是2，category_id是3 obj_list: [&#39;person&#39;, &#39;bicycle&#39;, &#39;car&#39;, ...]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-3-开始训练"><a href="#3-3-开始训练" class="headerlink" title="3.3 开始训练"></a>3.3 开始训练</h3><h4 id="3-3-a-从头开始训练-不建议"><a href="#3-3-a-从头开始训练-不建议" class="headerlink" title="3.3.a 从头开始训练(不建议)"></a>3.3.a 从头开始训练(不建议)</h4><blockquote><p>从头开始在 coco 上训练高效 det-d0<br>若批量大小batch_size为 12，这将需要大量时间并且需要每隔几个小时更改一次超参数。<br>如果你有几个月的时间要花费，那就去做吧。<br>这不像有人会获得比论文中的分数更好的分数。<br>前几个 epoch 会比较不稳定，这在从头开始训练是很正常的。</p></blockquote><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python train.py -c 0 --batch_size 64 --optim sgd --lr 8e-2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="3-3-b-从头开始训练自定义数据集"><a href="#3-3-b-从头开始训练自定义数据集" class="headerlink" title="3.3.b 从头开始训练自定义数据集"></a>3.3.b 从头开始训练自定义数据集</h4><blockquote><p>在自定义数据集上训练efficientdet-d1<br>批量大小batch_size为 8，学习率lr为 1e-5</p></blockquote><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python train.py -c 1 -p your_project_name --batch_size 8 --lr 1e-5<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="3-3-c-使用预训练权重训练自定义数据集（强烈推荐）"><a href="#3-3-c-使用预训练权重训练自定义数据集（强烈推荐）" class="headerlink" title="3.3.c 使用预训练权重训练自定义数据集（强烈推荐）"></a>3.3.c 使用预训练权重训练自定义数据集（强烈推荐）</h4><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 在具有预训练权重的自定义数据集上训练高效 det-d2 # 批量大小batch_size为 8，学习率lr为 1e-3，用于 10 个epoch时期python train.py -c 2 -p your_project_name --batch_size 8 --lr 1e-3 --num_epochs 10 \ --load_weights &#x2F;path&#x2F;to&#x2F;your&#x2F;weights&#x2F;efficientdet-d2.pth<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 使用 coco-pretrained，您甚至可以冻结主干和仅训练头部# 以加快训练并帮助收敛。python train.py -c 2 -p your_project_name --batch_size 8 --lr 1e-3 --num_epochs 10 \ --load_weights &#x2F;path&#x2F;to&#x2F;your&#x2F;weights&#x2F;efficientdet-d2.pth \ --head_only True<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-4-提前停止训练"><a href="#3-4-提前停止训练" class="headerlink" title="3.4 提前停止训练"></a>3.4 提前停止训练</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 训练时，按Ctrl+c，程序会捕获KeyboardInterrupt # 并停止训练，保存当前检查点。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="3-5-恢复训练"><a href="#3-5-恢复训练" class="headerlink" title="3.5 恢复训练"></a>3.5 恢复训练</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#如果说你开始了下面这样的训练任务。python train.py -c 2 -p your_project_name --batch_size 8 --lr 1e-3 \ --load_weights &#x2F;path&#x2F;to&#x2F;your&#x2F;weights&#x2F;efficientdet-d2.pth \ --head_only True # 然后你用Ctrl+c，它以检查点退出 # 现在你想从最后一个检查点恢复训练# 只需将 load_weights 设置为 &#39;last&#39;python train.py -c 2 -p your_project_name --batch_size 8 --lr 1e-3 \ --load_weights last \ --head_only True<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-6-评估模型性能"><a href="#3-6-评估模型性能" class="headerlink" title="3.6 评估模型性能"></a>3.6 评估模型性能</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 在 your_project 上进行评估，高效 det-d5python coco_eval.py -p your_project_name -c 5 \ -w &#x2F;path&#x2F;to&#x2F;your&#x2F;weights<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-7-调试训练（可选）"><a href="#3-7-调试训练（可选）" class="headerlink" title="3.7 调试训练（可选）"></a>3.7 调试训练（可选）</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 当你得到不好的结果时，你需要调试训练结果。python train.py -c 2 -p your_project_name --batch_size 8 --lr 1e-3 --debug True# 然后检查 test&#x2F; 文件夹，在那里你可以在训练期间可视化预测的框# 如果你看到无数的错误框，不要惊慌，它发生在训练的早期阶段。# 但是，如果您在几次 epoch 之后仍然看不到一个正常的框，甚至在所有图像中都看不到一个，# 那么可能是锚点配置不合适或地面实况已损坏。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-自定义数据集的训练教程"><a href="#4-自定义数据集的训练教程" class="headerlink" title="4.自定义数据集的训练教程"></a>4.自定义数据集的训练教程</h2><h3 id="4-1-安装环境要求"><a href="#4-1-安装环境要求" class="headerlink" title="4.1 安装环境要求"></a>4.1 安装环境要求</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">pip install pycocotools numpy&#x3D;&#x3D;1.16.0 opencv-python tqdm tensorboard tensorboardX pyyaml webcolors matplotlib<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-2-准备自定义数据集-预训练权重"><a href="#4-2-准备自定义数据集-预训练权重" class="headerlink" title="4.2 准备自定义数据集/预训练权重"></a>4.2 准备自定义数据集/预训练权重</h3><p>这里以官方提供的自定义数据集为例，目录结构如下图：</p><p><img src="/blog/blog/2022/01/20/efficientdet-xun-lian-mo-xing-zong-jie/EfficientDet%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/1571518-20211107153525820-892485254.png"></p><p>├── annotations<br>│   ├── instances_train.json<br>│   └── instances_val.json<br>├── train<br>│   ├── 0.jpg<br>│   ├── 1000.jpg<br>│   ├── 1001.jpg<br>……<br>└── val<br>    ├── 1111.jpg<br>    ├── 1112.jpg<br>    └── 1359.jpg</p><p>其中，<code>annatations</code>是存放coco格式json数据的路径，<code>train</code>为训练集的图片路径，<code>val</code>为测试集的图片路径。<br>这里的文件夹与路径名称是与对应项目的.yml配置文件相匹配的，否则会出现读取不到数据的错误。</p><h3 id="4-3-训练模型"><a href="#4-3-训练模型" class="headerlink" title="4.3 训练模型"></a>4.3 训练模型</h3><p>简单数据集，直接训练头部即可。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 考虑到这是一个简单的数据集，训练头部就足够支持拟合了。python train.py -c 0 -p birdview_vehicles --head_only True --lr 5e-3 --batch_size 32 --load_weights weights&#x2F;efficientdet-d0.pth  --num_epochs 10 --save_interval 100#训练一开始损失会很大，不要惊慌，要有耐心，再等一会儿<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><img src="/blog/blog/2022/01/20/efficientdet-xun-lian-mo-xing-zong-jie/EfficientDet%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/1571518-20211107155105263-1682277245.png"><br><img src="/blog/blog/2022/01/20/efficientdet-xun-lian-mo-xing-zong-jie/EfficientDet%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/1571518-20211107155110998-1828640763.png"></p><p>从最后的一次保存的模型文件开始训练，并且不只训练头部，还训练骨干与其他部分。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python train.py -c 0 -p birdview_vehicles --head_only False --lr 1e-3 --batch_size 16 --load_weights last  --num_epochs 16 --save_interval 100<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-4-评估训练结果"><a href="#4-4-评估训练结果" class="headerlink" title="4.4 评估训练结果"></a>4.4 评估训练结果</h3><p>训练产生的模型与日志结果，存储在log文件目录下，由于模型是随着Loss下降，精度提高，递进式的产生，因此只需要评估最后一个模型文件即可。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">weight_file[-1] &#x3D; &#39;efficientdet-d0_9_2770.pth&#39;python coco_eval.py -c 0 -p birdview_vehicles -w &quot;logs&#x2F;birdview_vehicles&#x2F;&#123;weight_file[-1]&#125;&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python coco_eval.py -c 0 -p birdview_vehicles -w &quot;logs&#x2F;birdview_vehicles&#x2F;efficientdet-d0_9_2770.pth&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/blog/blog/2022/01/20/efficientdet-xun-lian-mo-xing-zong-jie/EfficientDet%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/1571518-20211107155616894-814217117.png"></p><h3 id="4-5-可视化预测"><a href="#4-5-可视化预测" class="headerlink" title="4.5 可视化预测"></a>4.5 可视化预测</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">put images here datasets&#x2F;your_project_name&#x2F;val_set_name&#x2F;*.jpgput annotations here datasets&#x2F;your_project_name&#x2F;annotations&#x2F;instances_&#123;val_set_name&#125;.jsonput weights here &#x2F;path&#x2F;to&#x2F;your&#x2F;weights&#x2F;*.pth<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python detect.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/blog/blog/2022/01/20/efficientdet-xun-lian-mo-xing-zong-jie/EfficientDet%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/1571518-20211107161229673-15805960.png"></p>]]></content>
      
      
      <categories>
          
          <category> Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 炼丹术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通过kaggle-api下载数据集</title>
      <link href="/blog/2022/01/20/tong-guo-kaggle-api-xia-zai-shu-ju-ji/"/>
      <url>/blog/2022/01/20/tong-guo-kaggle-api-xia-zai-shu-ju-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="Kaggle-API使用教程"><a href="#Kaggle-API使用教程" class="headerlink" title="Kaggle API使用教程"></a>Kaggle API使用教程</h2><hr><p><a href="https://www.kaggle.com/">https://www.kaggle.com 的</a>官方 API ，可使用 Python 3 中实现的命令行工具访问。</p><p>Beta 版 - Kaggle 保留修改当前提供的 API 功能的权利。</p><p>重要提示：使用 1.5.0 之前的 API 版本提交的比赛可能无法正常工作。如果您在提交竞赛时遇到困难，请使用 来检查您的版本<code>kaggle --version</code>。如果低于 1.5.0，请更新<code>pip install kaggle --upgrade</code>.</p><h3 id="一、安装Kaggle环境并配置"><a href="#一、安装Kaggle环境并配置" class="headerlink" title="一、安装Kaggle环境并配置"></a>一、安装Kaggle环境并配置</h3><h4 id="1-1-安装Kaggle-Package"><a href="#1-1-安装Kaggle-Package" class="headerlink" title="1.1 安装Kaggle Package"></a>1.1 安装Kaggle Package</h4><p>确保您安装了 Python 3 和包管理<code>pip</code>器。</p><p>运行以下命令以使用命令行访问 Kaggle API：</p><p><code>pip install kaggle</code>（您可能需要<code>pip install --user kaggle</code>在 Mac/Linux 上执行。如果在安装过程中出现问题，建议这样做。）<code>sudo pip install kaggle</code>除非您了解自己在做什么，否则通过 root 用户（即）完成的安装将无法正常工作。即便如此，它们仍然可能不起作用。如果出现权限错误，强烈建议用户安装。</p><p>您现在可以使用<code>kaggle</code>以下示例中所示的命令。</p><p>如果遇到<code>kaggle: command not found</code>错误，请确保您的 Python 二进制文件在您的路径上。您可以<code>kaggle</code>通过执行<code>pip uninstall kaggle</code>并查看二进制文件的位置来查看安装位置。对于 Linux 上的本地用户安装，默认位置是<code>~/.local/bin</code>. 在 Windows 上，默认位置是<code>$PYTHON_HOME/Scripts</code>.</p><p>重要提示：我们不提供 Python 2 支持。在报告任何问题之前，请确保您使用的是 Python 3。</p><h4 id="1-2-API-Token配置"><a href="#1-2-API-Token配置" class="headerlink" title="1.2 API Token配置"></a>1.2 API Token配置</h4><p>要使用 Kaggle API，请在<a href="https://www.kaggle.com/">https://www.kaggle.com</a>注册一个 Kaggle 帐户。</p><p>注册成功后登录kaggle</p><ul><li>点击右上角头像处，会弹出相关侧边栏设置，如下</li></ul><img src="/blog/blog/2022/01/20/tong-guo-kaggle-api-xia-zai-shu-ju-ji/1571518-20211027105535901-405781655.png" style="zoom:67%;"><ul><li><p>点击Your Profile，进入设置</p><p><img src="/blog/blog/2022/01/20/tong-guo-kaggle-api-xia-zai-shu-ju-ji/%E9%80%9A%E8%BF%87kaggle-api%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20211027105906204-15480096.png"></p></li><li><p>在上面的页面找到API对应的设置，点击Create New Token，这将触发下载包含您的 API 凭据的文件<code>kaggle.json</code>。对应的kagge.json如下</p><p><img src="/blog/blog/2022/01/20/tong-guo-kaggle-api-xia-zai-shu-ju-ji/%E9%80%9A%E8%BF%87kaggle-api%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20211027110505259-1558319378.png"></p></li></ul><p>kaggle配置<br>本机安装kaggle api</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">pip install kaggle<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>将此文件放在该位置<code>~/.kaggle/kaggle.json</code></p><p>若没有这个目录，则在根目录下创建.kaggle文件夹，再把kaggle.json放入</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cd ~mkdir .kagglecd ~&#x2F;.kaggle&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（在 Windows 上的该位置<code>C:\Users\&lt;Windows-username&gt;\.kaggle\kaggle.json</code>- 您可以检查确切位置，无驱动器，使用<code>echo %HOMEPATH%</code>）。您可以定义一个 shell 环境变量<code>KAGGLE_CONFIG_DIR</code>来将此位置更改为<code>$KAGGLE_CONFIG_DIR/kaggle.json</code>（在 Windows 上为<code>%KAGGLE_CONFIG_DIR%\kaggle.json</code>）。</p><p>为了您的安全，请确保您计算机的其他用户对您的凭据没有读取权限。在基于 Unix 的系统上，您可以使用以下命令执行此操作：</p><pre class="line-numbers language-none"><code class="language-none">chmod 600 ~&#x2F;.kaggle&#x2F;kaggle.json<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>您还可以选择将您的 Kaggle 用户名和令牌导出到环境中：</p><pre class="line-numbers language-none"><code class="language-none">导出KAGGLE_USERNAME&#x3D;datadinosaur导出KAGGLE_KEY&#x3D;xxxxxxxxxxxxxx<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>此外，您可以导出通常采用<code>$HOME/.kaggle/kaggle.json</code>“KAGGLE_”格式（注意大写）的任何其他配置值。<br>例如，如果文件具有变量“proxy”，您将导出<code>KAGGLE_PROXY</code> 并由客户端查看。</p><h3 id="二、Kaggle-Command命令使用"><a href="#二、Kaggle-Command命令使用" class="headerlink" title="二、Kaggle Command命令使用"></a>二、Kaggle Command命令使用</h3><p>命令行工具支持以下命令：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle competitions &#123;list, files, download, submit, submissions, leaderboard&#125;kaggle datasets &#123;list, files, download, create, version, init&#125;kaggle kernels &#123;list, init, push, pull, output, status&#125;kaggle config &#123;view, set, unset&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>有关使用这些命令中的每一个，请参阅下面的更多详细信息。</p><h4 id="2-1-Competitions比赛"><a href="#2-1-Competitions比赛" class="headerlink" title="2.1 Competitions比赛"></a>2.1 Competitions比赛</h4><p>该 API 支持以下用于 Kaggle 比赛的命令。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle competitions [-h]                           &#123;list,files,download,submit,submissions,leaderboard&#125;                           ...optional arguments:  -h, --help            show this help message and exitcommands:  &#123;list,files,download,submit,submissions,leaderboard&#125;    list                List available competitions    files               List competition files    download            Download competition files    submit              Make a new competition submission    submissions         Show your competition submissions    leaderboard         Get competition leaderboard information<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="2-1-1-列出比赛"><a href="#2-1-1-列出比赛" class="headerlink" title="2.1.1 列出比赛"></a>2.1.1 列出比赛</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle competitions list [-h] [--group GROUP] [--category CATEGORY] [--sort-by SORT_BY] [-p PAGE] [-s SEARCH] [-v]optional arguments:  -h, --help            show this help message and exit  --group GROUP         Search for competitions in a specific group. Default is &#39;general&#39;. Valid options are &#39;general&#39;, &#39;entered&#39;, and &#39;inClass&#39;  --category CATEGORY   Search for competitions of a specific category. Default is &#39;all&#39;. Valid options are &#39;all&#39;, &#39;featured&#39;, &#39;research&#39;, &#39;recruitment&#39;, &#39;gettingStarted&#39;, &#39;masters&#39;, and &#39;playground&#39;  --sort-by SORT_BY     Sort list results. Default is &#39;latestDeadline&#39;. Valid options are &#39;grouped&#39;, &#39;prize&#39;, &#39;earliestDeadline&#39;, &#39;latestDeadline&#39;, &#39;numberOfTeams&#39;, and &#39;recentlyCreated&#39;  -p PAGE, --page PAGE  Page number for results paging. Page size is 20 by default   -s SEARCH, --search SEARCH                        Term(s) to search for  -v, --csv             Print results in CSV format                        (if not set print in table format)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例:</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle competitions list -s health<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle competitions list --category gettingStarted<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h5 id="2-1-2-列出比赛文件"><a href="#2-1-2-列出比赛文件" class="headerlink" title="2.1.2 列出比赛文件"></a>2.1.2 列出比赛文件</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle competitions files [-h] [-v] [-q] [competition]optional arguments:  -h, --help   show this help message and exit  competition  Competition URL suffix (use &quot;kaggle competitions list&quot; to show options)               If empty, the default competition will be used (use &quot;kaggle config set competition&quot;)&quot;  -v, --csv    Print results in CSV format (if not set print in table format)  -q, --quiet  Suppress printing information about the upload&#x2F;download progress<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle competitions files favorita-grocery-sales-forecasting<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h5 id="2-1-3-下载比赛文件"><a href="#2-1-3-下载比赛文件" class="headerlink" title="2.1.3 下载比赛文件"></a>2.1.3 下载比赛文件</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle competitions download [-h] [-f FILE_NAME] [-p PATH] [-w] [-o]                                    [-q]                                    [competition]optional arguments:  -h, --help            show this help message and exit  competition           Competition URL suffix (use &quot;kaggle competitions list&quot; to show options)                        If empty, the default competition will be used (use &quot;kaggle config set competition&quot;)&quot;  -f FILE_NAME, --file FILE_NAME                        File name, all files downloaded if not provided                        (use &quot;kaggle competitions files -c &lt;competition&gt;&quot; to show options)  -p PATH, --path PATH  Folder where file(s) will be downloaded, defaults to current working directory  -w, --wp              Download files to current working path  -o, --force           Skip check whether local version of file is up to date, force file download  -q, --quiet           Suppress printing information about the upload&#x2F;download progress<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-none"><code class="language-none">kaggle competitions download favorita-grocery-sales-forecastingkaggle competitions download favorita-grocery-sales-forecasting -f test.csv.7z<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>注意：您需要在 接受比赛规则<code>https://www.kaggle.com/c/&lt;competition-name&gt;/rules</code>。</p><h5 id="2-1-4-提交比赛"><a href="#2-1-4-提交比赛" class="headerlink" title="2.1.4 提交比赛"></a>2.1.4 提交比赛</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle competitions submit [-h] -f FILE_NAME -m MESSAGE [-q]                                  [competition]required arguments:  -f FILE_NAME, --file FILE_NAME                        File for upload (full path)  -m MESSAGE, --message MESSAGE                        Message describing this submissionoptional arguments:  -h, --help            show this help message and exit  competition           Competition URL suffix (use &quot;kaggle competitions list&quot; to show options)                        If empty, the default competition will be used (use &quot;kaggle config set competition&quot;)&quot;  -q, --quiet           Suppress printing information about the upload&#x2F;download progress<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-none"><code class="language-none">kaggle competitions submit favorita-grocery-sales-forecasting -f sample_submission_favorita.csv.7z -m &quot;My submission message&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意：您需要在 接受比赛规则<code>https://www.kaggle.com/c/&lt;competition-name&gt;/rules</code>。</p><h5 id="2-1-5-列出参赛作品"><a href="#2-1-5-列出参赛作品" class="headerlink" title="2.1.5 列出参赛作品"></a>2.1.5 列出参赛作品</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle competitions submissions [-h] [-v] [-q] [competition]optional arguments:  -h, --help   show this help message and exit  competition  Competition URL suffix (use &quot;kaggle competitions list&quot; to show options)               If empty, the default competition will be used (use &quot;kaggle config set competition&quot;)&quot;  -v, --csv    Print results in CSV format (if not set print in table format)  -q, --quiet  Suppress printing information about the upload&#x2F;download progress<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：<br>kaggle competitions submissions favorita-grocery-sales-forecasting</p><p>注意：您需要在 接受比赛规则<a href="https://www.kaggle.com/c/">https://www.kaggle.com/c/</a><competition-name>/rules。</competition-name></p><h5 id="2-1-6-获取比赛排行榜"><a href="#2-1-6-获取比赛排行榜" class="headerlink" title="2.1.6 获取比赛排行榜"></a>2.1.6 获取比赛排行榜</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle competitions leaderboard [-h] [-s] [-d] [-p PATH] [-v] [-q]                                       [competition]optional arguments:  -h, --help            show this help message and exit  competition           Competition URL suffix (use &quot;kaggle competitions list&quot; to show options)                        If empty, the default competition will be used (use &quot;kaggle config set competition&quot;)&quot;  -s, --show            Show the top of the leaderboard  -d, --download        Download entire leaderboard  -p PATH, --path PATH  Folder where file(s) will be downloaded, defaults to current working directory  -v, --csv             Print results in CSV format (if not set print in table format)  -q, --quiet           Suppress printing information about the upload&#x2F;download progress<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>例子：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle competitions leaderboard favorita-grocery-sales-forecasting -s<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="2-2-数据集"><a href="#2-2-数据集" class="headerlink" title="2.2 数据集"></a>2.2 数据集</h4><p>API 支持以下用于 Kaggle 数据集的命令。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle datasets [-h]                       &#123;list,files,download,create,version,init,metadata,status&#125; ...optional arguments:  -h, --help            show this help message and exitcommands:  &#123;list,files,download,create,version,init,metadata, status&#125;    list                List available datasets    files               List dataset files    download            Download dataset files    create              Create a new dataset    version             Create a new dataset version    init                Initialize metadata file for dataset creation    metadata            Download metadata about a dataset    status              Get the creation status for a dataset<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="2-2-1-列出数据集"><a href="#2-2-1-列出数据集" class="headerlink" title="2.2.1 列出数据集"></a>2.2.1 列出数据集</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle datasets list [-h] [--sort-by SORT_BY] [--size SIZE] [--file-type FILE_TYPE] [--license LICENSE_NAME] [--tags TaG_IDS] [-s SEARCH] [-m] [--user USER] [-p PAGE] [-v]optional arguments:  -h, --help            show this help message and exit  --sort-by SORT_BY     Sort list results. Default is &#39;hottest&#39;. Valid options are &#39;hottest&#39;, &#39;votes&#39;, &#39;updated&#39;, and &#39;active&#39;  --size SIZE           Search for datasets of a specific size. Default is &#39;all&#39;. Valid options are &#39;all&#39;, &#39;small&#39;, &#39;medium&#39;, and &#39;large&#39;  --file-type FILE_TYPE Search for datasets with a specific file type. Default is &#39;all&#39;. Valid options are &#39;all&#39;, &#39;csv&#39;, &#39;sqlite&#39;, &#39;json&#39;, and &#39;bigQuery&#39;. Please note that bigQuery datasets cannot be downloaded  --license LICENSE_NAME                         Search for datasets with a specific license. Default is &#39;all&#39;. Valid options are &#39;all&#39;, &#39;cc&#39;, &#39;gpl&#39;, &#39;odb&#39;, and &#39;other&#39;  --tags TAG_IDS        Search for datasets that have specific tags. Tag list should be comma separated                        -s SEARCH, --search SEARCH                        Term(s) to search for  -m, --mine            Display only my items  --user USER           Find public datasets owned by a specific user or organization  -p PAGE, --page PAGE  Page number for results paging. Page size is 20 by default  -v, --csv             Print results in CSV format (if not set print in table format)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle datasets list -s demographicskaggle datasets list --sort-by votes<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h5 id="2-2-2-列出数据集的文件"><a href="#2-2-2-列出数据集的文件" class="headerlink" title="2.2.2 列出数据集的文件"></a>2.2.2 列出数据集的文件</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle datasets files [-h] [-v] [dataset]optional arguments:  -h, --help  show this help message and exit  dataset     Dataset URL suffix in format &lt;owner&gt;&#x2F;&lt;dataset-name&gt; (use &quot;kaggle datasets list&quot; to show options)  -v, --csv   Print results in CSV format (if not set print in table format)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle datasets files zillow&#x2F;zecon<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h5 id="2-2-3-下载数据集文件"><a href="#2-2-3-下载数据集文件" class="headerlink" title="2.2.3 下载数据集文件"></a>2.2.3 下载数据集文件</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle datasets download [-h] [-f FILE_NAME] [-p PATH] [-w] [--unzip]                                [-o] [-q]                                [dataset]optional arguments:  -h, --help            show this help message and exit  dataset               Dataset URL suffix in format &lt;owner&gt;&#x2F;&lt;dataset-name&gt; (use &quot;kaggle datasets list&quot; to show options)  -f FILE_NAME, --file FILE_NAME                        File name, all files downloaded if not provided                        (use &quot;kaggle datasets files -d &lt;dataset&gt;&quot; to show options)  -p PATH, --path PATH  Folder where file(s) will be downloaded, defaults to current working directory  -w, --wp              Download files to current working path  --unzip               Unzip the downloaded file. Will delete the zip file when completed.  -o, --force           Skip check whether local version of file is up to date, force file download  -q, --quiet           Suppress printing information about the upload&#x2F;download progress<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle datasets download zillow&#x2F;zeconkaggle datasets download zillow&#x2F;zecon -f State_time_series.csv<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>请注意，无法下载 BigQuery 数据集。</p><p>在对应数据集上找到API command，复制到剪切板</p><p><img src="/blog/blog/2022/01/20/tong-guo-kaggle-api-xia-zai-shu-ju-ji/%E9%80%9A%E8%BF%87kaggle-api%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20211027111605969-1361034724.png"></p><p>如上面这个数据集的命令就是：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle datasets download -d cisautomotiveapi&#x2F;large-car-dataset<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/blog/blog/2022/01/20/tong-guo-kaggle-api-xia-zai-shu-ju-ji/%E9%80%9A%E8%BF%87kaggle-api%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20211027113501374-515641469.png"></p><h5 id="2-2-4-初始化元数据文件以创建数据集"><a href="#2-2-4-初始化元数据文件以创建数据集" class="headerlink" title="2.2.4 初始化元数据文件以创建数据集"></a>2.2.4 初始化元数据文件以创建数据集</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle datasets init [-h] [-p FOLDER]optional arguments:  -h, --help            show this help message and exit  -p FOLDER, --path FOLDER                        Folder for upload, containing data files and a special dataset-metadata.json file (https:&#x2F;&#x2F;github.com&#x2F;Kaggle&#x2F;kaggle-api&#x2F;wiki&#x2F;Dataset-Metadata). Defaults to current working directory<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle datasets init -p &#x2F;path&#x2F;to&#x2F;dataset<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h5 id="2-2-5-创建新数据集"><a href="#2-2-5-创建新数据集" class="headerlink" title="2.2.5 创建新数据集"></a>2.2.5 创建新数据集</h5><p>如果要创建新的数据集，首先需要启动元数据文件。您可以通过<code>kaggle datasets init</code>如上所述运行来实现这一点。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle datasets create [-h] [-p FOLDER] [-u] [-q] [-t] [-r &#123;skip,zip,tar&#125;]optional arguments:  -h, --help            show this help message and exit  -p FOLDER, --path FOLDER                        Folder for upload, containing data files and a special dataset-metadata.json file (https:&#x2F;&#x2F;github.com&#x2F;Kaggle&#x2F;kaggle-api&#x2F;wiki&#x2F;Dataset-Metadata). Defaults to current working directory  -u, --public          Create publicly (default is private)  -q, --quiet           Suppress printing information about the upload&#x2F;download progress  -t, --keep-tabular    Do not convert tabular files to CSV (default is to convert)  -r &#123;skip,zip,tar&#125;, --dir-mode &#123;skip,zip,tar&#125;                        What to do with directories: &quot;skip&quot; - ignore; &quot;zip&quot; - compressed upload; &quot;tar&quot; - uncompressed upload<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle datasets create -p &#x2F;path&#x2F;to&#x2F;dataset<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h5 id="2-2-6-创建新的数据集版本"><a href="#2-2-6-创建新的数据集版本" class="headerlink" title="2.2.6 创建新的数据集版本"></a>2.2.6 创建新的数据集版本</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle datasets version [-h] -m VERSION_NOTES [-p FOLDER] [-q] [-t]                               [-r &#123;skip,zip,tar&#125;] [-d]required arguments:  -m VERSION_NOTES, --message VERSION_NOTES                        Message describing the new versionoptional arguments:  -h, --help            show this help message and exit  -p FOLDER, --path FOLDER                        Folder for upload, containing data files and a special dataset-metadata.json file (https:&#x2F;&#x2F;github.com&#x2F;Kaggle&#x2F;kaggle-api&#x2F;wiki&#x2F;Dataset-Metadata). Defaults to current working directory  -q, --quiet           Suppress printing information about the upload&#x2F;download progress  -t, --keep-tabular    Do not convert tabular files to CSV (default is to convert)  -r &#123;skip,zip,tar&#125;, --dir-mode &#123;skip,zip,tar&#125;                        What to do with directories: &quot;skip&quot; - ignore; &quot;zip&quot; - compressed upload; &quot;tar&quot; - uncompressed upload  -d, --delete-old-versions                        Delete old versions of this dataset<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle datasets version -p &#x2F;path&#x2F;to&#x2F;dataset -m &quot;Updated data&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h5 id="2-2-7-下载现有数据集的元数据"><a href="#2-2-7-下载现有数据集的元数据" class="headerlink" title="2.2.7 下载现有数据集的元数据"></a>2.2.7 下载现有数据集的元数据</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle datasets metadata [-h] [-p PATH] [dataset]optional arguments:  -h, --help            show this help message and exit  dataset               Dataset URL suffix in format &lt;owner&gt;&#x2F;&lt;dataset-name&gt; (use &quot;kaggle datasets list&quot; to show options)  -p PATH, --path PATH  Location to download dataset metadata to. Defaults to current working directory<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle datasets metadata -p &#x2F;path&#x2F;to&#x2F;download zillow&#x2F;zecon<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h5 id="2-2-8-取数据集创建状态"><a href="#2-2-8-取数据集创建状态" class="headerlink" title="2.2.8 取数据集创建状态"></a>2.2.8 取数据集创建状态</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle datasets status [-h] [dataset]optional arguments:  -h, --help  show this help message and exit  dataset     Dataset URL suffix in format &lt;owner&gt;&#x2F;&lt;dataset-name&gt; (use &quot;kaggle datasets list&quot; to show options)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle datasets status zillow&#x2F;zecon<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="2-3-kernel内核"><a href="#2-3-kernel内核" class="headerlink" title="2.3 kernel内核"></a>2.3 kernel内核</h4><p>该 API 支持 Kaggle 内核的以下命令。</p><pre class="line-numbers language-none"><code class="language-none">usage: kaggle kernels [-h] &#123;list,init,push,pull,output,status&#125; ...optional arguments:  -h, --help            show this help message and exitcommands:  &#123;list,init,push,pull,output,status&#125;    list                List available kernels    init                Initialize metadata file for a kernel    push                Push new code to a kernel and run the kernel    pull                Pull down code from a kernel    output              Get data output from the latest kernel run    status              Display the status of the latest kernel run<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="2-3-1-列出内核"><a href="#2-3-1-列出内核" class="headerlink" title="2.3.1 列出内核"></a>2.3.1 列出内核</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle kernels list [-h] [-m] [-p PAGE] [--page-size PAGE_SIZE] [-s SEARCH] [-v]                           [--parent PARENT] [--competition COMPETITION]                           [--dataset DATASET]                           [--user USER] [--language LANGUAGE]                           [--kernel-type KERNEL_TYPE]                           [--output-type OUTPUT_TYPE] [--sort-by SORT_BY]optional arguments:  -h, --help            show this help message and exit  -m, --mine            Display only my items  -p PAGE, --page PAGE  Page number for results paging. Page size is 20 by default  --page-size PAGE_SIZE Number of items to show on a page. Default size is 20, max is 100  -s SEARCH, --search SEARCH                        Term(s) to search for  -v, --csv             Print results in CSV format (if not set print in table format)  --parent PARENT       Find children of the specified parent kernel  --competition COMPETITION                        Find kernels for a given competition  --dataset DATASET     Find kernels for a given dataset  --user USER           Find kernels created by a given user  --language LANGUAGE   Specify the language the kernel is written in. Default is &#39;all&#39;. Valid options are &#39;all&#39;, &#39;python&#39;, &#39;r&#39;, &#39;sqlite&#39;, and &#39;julia&#39;  --kernel-type KERNEL_TYPE                        Specify the type of kernel. Default is &#39;all&#39;. Valid options are &#39;all&#39;, &#39;script&#39;, and &#39;notebook&#39;  --output-type OUTPUT_TYPE                        Search for specific kernel output types. Default is &#39;all&#39;. Valid options are &#39;all&#39;, &#39;visualizations&#39;, and &#39;data&#39;  --sort-by SORT_BY     Sort list results. Default is &#39;hotness&#39;.  Valid options are &#39;hotness&#39;, &#39;commentCount&#39;, &#39;dateCreated&#39;, &#39;dateRun&#39;, &#39;relevance&#39;, &#39;scoreAscending&#39;, &#39;scoreDescending&#39;, &#39;viewCount&#39;, and &#39;voteCount&#39;. &#39;relevance&#39; is only applicable if a search term is specified.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle kernels list -s titanickaggle kernels list --language python<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h5 id="2-3-2-为内核初始化元数据文件"><a href="#2-3-2-为内核初始化元数据文件" class="headerlink" title="2.3.2 为内核初始化元数据文件"></a>2.3.2 为内核初始化元数据文件</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle kernels init [-h] [-p FOLDER]optional arguments:  -h, --help            show this help message and exit  -p FOLDER, --path FOLDER                        Folder for upload, containing data files and a special kernel-metadata.json file (https:&#x2F;&#x2F;github.com&#x2F;Kaggle&#x2F;kaggle-api&#x2F;wiki&#x2F;Kernel-Metadata). Defaults to current working directory<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle kernels init -p &#x2F;path&#x2F;to&#x2F;kernel<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h5 id="2-3-3-推送内核"><a href="#2-3-3-推送内核" class="headerlink" title="2.3.3 推送内核"></a>2.3.3 推送内核</h5><pre class="line-numbers language-none"><code class="language-none">usage: kaggle kernels push [-h] -p FOLDERoptional arguments:  -h, --help            show this help message and exit  -p FOLDER, --path FOLDER                        Folder for upload, containing data files and a special kernel-metadata.json file (https:&#x2F;&#x2F;github.com&#x2F;Kaggle&#x2F;kaggle-api&#x2F;wiki&#x2F;Kernel-Metadata). Defaults to current working directory<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle kernels push -p &#x2F;path&#x2F;to&#x2F;kernel<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h5 id="2-3-4-拉一个内核"><a href="#2-3-4-拉一个内核" class="headerlink" title="2.3.4 拉一个内核"></a>2.3.4 拉一个内核</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle kernels pull [-h] [-p PATH] [-w] [-m] [kernel]optional arguments:  -h, --help            show this help message and exit  kernel                Kernel URL suffix in format &lt;owner&gt;&#x2F;&lt;kernel-name&gt; (use &quot;kaggle kernels list&quot; to show options)  -p PATH, --path PATH  Folder where file(s) will be downloaded, defaults to current working directory  -w, --wp              Download files to current working path  -m, --metadata        Generate metadata when pulling kernel<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle kernels pull rtatman&#x2F;list-of-5-day-challenges -p &#x2F;path&#x2F;to&#x2F;dest<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h5 id="2-3-5-检索内核的输出"><a href="#2-3-5-检索内核的输出" class="headerlink" title="2.3.5 检索内核的输出"></a>2.3.5 检索内核的输出</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle kernels output [-h] [-p PATH] [-w] [-o] [-q] [kernel]optional arguments:  -h, --help            show this help message and exit  kernel                Kernel URL suffix in format &lt;owner&gt;&#x2F;&lt;kernel-name&gt; (use &quot;kaggle kernels list&quot; to show options)  -p PATH, --path PATH  Folder where file(s) will be downloaded, defaults to current working directory  -w, --wp              Download files to current working path  -o, --force           Skip check whether local version of file is up to date, force file download  -q, --quiet           Suppress printing information about the upload&#x2F;download progress<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle kernels output mrisdal&#x2F;exploring-survival-on-the-titanic -p &#x2F;path&#x2F;to&#x2F;dest<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h5 id="2-4-6-获取最新内核运行的状态"><a href="#2-4-6-获取最新内核运行的状态" class="headerlink" title="2.4.6 获取最新内核运行的状态"></a>2.4.6 获取最新内核运行的状态</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle kernels status [-h] [kernel]optional arguments:  -h, --help  show this help message and exit  kernel      Kernel URL suffix in format &lt;owner&gt;&#x2F;&lt;kernel-name&gt; (use &quot;kaggle kernels list&quot; to show options)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle kernels status mrisdal&#x2F;exploring-survival-on-the-titanic<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="2-4-Config配置"><a href="#2-4-Config配置" class="headerlink" title="2.4 Config配置"></a>2.4 Config配置</h4><p>API 支持以下命令进行配置。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle config [-h] &#123;view,set,unset&#125; ...optional arguments:  -h, --help        show this help message and exitcommands:  &#123;view,set,unset&#125;    view            View current config values    set             Set a configuration value    unset           Clear a configuration value<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="2-4-1-查看当前配置值"><a href="#2-4-1-查看当前配置值" class="headerlink" title="2.4.1 查看当前配置值"></a>2.4.1 查看当前配置值</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle config path [-h] [-p PATH]optional arguments:  -h, --help            show this help message and exit  -p PATH, --path PATH  folder where file(s) will be downloaded, defaults to current working directory<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle config path -p C:\<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h5 id="2-4-2-查看当前配置值"><a href="#2-4-2-查看当前配置值" class="headerlink" title="2.4.2 查看当前配置值"></a>2.4.2 查看当前配置值</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle config view [-h]optional arguments:  -h, --help  show this help message and exit<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle config view<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h5 id="2-4-3-设置配置值"><a href="#2-4-3-设置配置值" class="headerlink" title="2.4.3 设置配置值"></a>2.4.3 设置配置值</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle config set [-h] -n NAME -v VALUErequired arguments:  -n NAME, --name NAME  Name of the configuration parameter                        (one of competition, path, proxy)  -v VALUE, --value VALUE                        Value of the configuration parameter, valid values depending on name                        - competition: Competition URL suffix (use &quot;kaggle competitions list&quot; to show options)                        - path: Folder where file(s) will be downloaded, defaults to current working directory                        - proxy: Proxy for HTTP requests<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle config set -n competition -v titanic<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h5 id="2-4-4-清除配置值"><a href="#2-4-4-清除配置值" class="headerlink" title="2.4.4 清除配置值"></a>2.4.4 清除配置值</h5><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">usage: kaggle config unset [-h] -n NAMErequired arguments:  -n NAME, --name NAME  Name of the configuration parameter                        (one of competition, path, proxy)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用实例：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">kaggle config unset -n competition<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 算法竞赛 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kaggle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YOLOv5训练自定义数据集</title>
      <link href="/blog/2022/01/20/yolov5-xun-lian-zi-ding-yi-shu-ju-ji/"/>
      <url>/blog/2022/01/20/yolov5-xun-lian-zi-ding-yi-shu-ju-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="YOLOv5训练自定义数据"><a href="#YOLOv5训练自定义数据" class="headerlink" title="YOLOv5训练自定义数据"></a>YOLOv5训练自定义数据</h2><h3 id="一、开始之前的准备工作"><a href="#一、开始之前的准备工作" class="headerlink" title="一、开始之前的准备工作"></a>一、开始之前的准备工作</h3><p>克隆 repo 并在<a href="https://www.python.org/"><strong>Python&gt;=3.6.0</strong></a>环境中安装<a href="https://github.com/ultralytics/yolov5/blob/master/requirements.txt">requirements.txt</a>，包括<a href="https://pytorch.org/get-started/locally/"><strong>PyTorch&gt;=1.7</strong></a>。<a href="https://github.com/ultralytics/yolov5/tree/master/models">模型</a>和<a href="https://github.com/ultralytics/yolov5/tree/master/data">数据</a>集会从最新的 YOLOv5<a href="https://github.com/ultralytics/yolov5/releases">版本中</a>自动下载。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git clone https:&#x2F;&#x2F;github.com&#x2F;ultralytics&#x2F;yolov5cd yolov5pip install -r requirements.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="二、训练自定义数据"><a href="#二、训练自定义数据" class="headerlink" title="二、训练自定义数据"></a>二、训练自定义数据</h3><h4 id="2-1-创建my-dataset-yaml"><a href="#2-1-创建my-dataset-yaml" class="headerlink" title="2.1 创建my_dataset.yaml"></a>2.1 创建my_dataset.yaml</h4><p><a href="https://www.kaggle.com/ultralytics/coco128">COCO128</a>是一个示例小教程数据集，由<a href="http://cocodataset.org/#home">COCO</a> train2017中的前 128 张图像组成。这些相同的 128 张图像用于训练和验证，以验证我们的训练管道是否能够过拟合。<a href="https://github.com/ultralytics/yolov5/blob/master/data/coco128.yaml">数据/ coco128.yaml</a>，如下所示，是数据集的配置文件，它定义1）数据集根目录<code>path</code>和相对路径<code>train</code>/ <code>val</code>/<code>test</code>图像目录（或* .txt与图像文件的路径），2）的类的数量<code>nc</code>和3）类列表<code>names</code>：</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token comment"># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]</span><span class="token key atrule">path</span><span class="token punctuation">:</span> ../datasets/coco128  <span class="token comment"># dataset root dir 数据集根目录</span><span class="token key atrule">train</span><span class="token punctuation">:</span> images/train2017  <span class="token comment"># train images (relative to 'path') 128 images #训练图像（相对于“path”）</span><span class="token key atrule">val</span><span class="token punctuation">:</span> images/train2017  <span class="token comment"># val images (relative to 'path') 128 images # val 图像（相对于“path”）</span><span class="token key atrule">test</span><span class="token punctuation">:</span>  <span class="token comment"># test images (optional) #测试图像（可选）</span><span class="token comment"># Classes</span><span class="token key atrule">nc</span><span class="token punctuation">:</span> <span class="token number">80</span>  <span class="token comment"># number of classes</span><span class="token key atrule">names</span><span class="token punctuation">:</span> <span class="token punctuation">[</span> <span class="token string">'person'</span><span class="token punctuation">,</span> <span class="token string">'bicycle'</span><span class="token punctuation">,</span> <span class="token string">'car'</span><span class="token punctuation">,</span> <span class="token string">'motorcycle'</span><span class="token punctuation">,</span> <span class="token string">'airplane'</span><span class="token punctuation">,</span> <span class="token string">'bus'</span><span class="token punctuation">,</span> <span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'truck'</span><span class="token punctuation">,</span> <span class="token string">'boat'</span><span class="token punctuation">,</span> <span class="token string">'traffic light'</span><span class="token punctuation">,</span>         <span class="token string">'fire hydrant'</span><span class="token punctuation">,</span> <span class="token string">'stop sign'</span><span class="token punctuation">,</span> <span class="token string">'parking meter'</span><span class="token punctuation">,</span> <span class="token string">'bench'</span><span class="token punctuation">,</span> <span class="token string">'bird'</span><span class="token punctuation">,</span> <span class="token string">'cat'</span><span class="token punctuation">,</span> <span class="token string">'dog'</span><span class="token punctuation">,</span> <span class="token string">'horse'</span><span class="token punctuation">,</span> <span class="token string">'sheep'</span><span class="token punctuation">,</span> <span class="token string">'cow'</span><span class="token punctuation">,</span>         <span class="token string">'elephant'</span><span class="token punctuation">,</span> <span class="token string">'bear'</span><span class="token punctuation">,</span> <span class="token string">'zebra'</span><span class="token punctuation">,</span> <span class="token string">'giraffe'</span><span class="token punctuation">,</span> <span class="token string">'backpack'</span><span class="token punctuation">,</span> <span class="token string">'umbrella'</span><span class="token punctuation">,</span> <span class="token string">'handbag'</span><span class="token punctuation">,</span> <span class="token string">'tie'</span><span class="token punctuation">,</span> <span class="token string">'suitcase'</span><span class="token punctuation">,</span> <span class="token string">'frisbee'</span><span class="token punctuation">,</span>         <span class="token string">'skis'</span><span class="token punctuation">,</span> <span class="token string">'snowboard'</span><span class="token punctuation">,</span> <span class="token string">'sports ball'</span><span class="token punctuation">,</span> <span class="token string">'kite'</span><span class="token punctuation">,</span> <span class="token string">'baseball bat'</span><span class="token punctuation">,</span> <span class="token string">'baseball glove'</span><span class="token punctuation">,</span> <span class="token string">'skateboard'</span><span class="token punctuation">,</span> <span class="token string">'surfboard'</span><span class="token punctuation">,</span>         <span class="token string">'tennis racket'</span><span class="token punctuation">,</span> <span class="token string">'bottle'</span><span class="token punctuation">,</span> <span class="token string">'wine glass'</span><span class="token punctuation">,</span> <span class="token string">'cup'</span><span class="token punctuation">,</span> <span class="token string">'fork'</span><span class="token punctuation">,</span> <span class="token string">'knife'</span><span class="token punctuation">,</span> <span class="token string">'spoon'</span><span class="token punctuation">,</span> <span class="token string">'bowl'</span><span class="token punctuation">,</span> <span class="token string">'banana'</span><span class="token punctuation">,</span> <span class="token string">'apple'</span><span class="token punctuation">,</span>         <span class="token string">'sandwich'</span><span class="token punctuation">,</span> <span class="token string">'orange'</span><span class="token punctuation">,</span> <span class="token string">'broccoli'</span><span class="token punctuation">,</span> <span class="token string">'carrot'</span><span class="token punctuation">,</span> <span class="token string">'hot dog'</span><span class="token punctuation">,</span> <span class="token string">'pizza'</span><span class="token punctuation">,</span> <span class="token string">'donut'</span><span class="token punctuation">,</span> <span class="token string">'cake'</span><span class="token punctuation">,</span> <span class="token string">'chair'</span><span class="token punctuation">,</span> <span class="token string">'couch'</span><span class="token punctuation">,</span>         <span class="token string">'potted plant'</span><span class="token punctuation">,</span> <span class="token string">'bed'</span><span class="token punctuation">,</span> <span class="token string">'dining table'</span><span class="token punctuation">,</span> <span class="token string">'toilet'</span><span class="token punctuation">,</span> <span class="token string">'tv'</span><span class="token punctuation">,</span> <span class="token string">'laptop'</span><span class="token punctuation">,</span> <span class="token string">'mouse'</span><span class="token punctuation">,</span> <span class="token string">'remote'</span><span class="token punctuation">,</span> <span class="token string">'keyboard'</span><span class="token punctuation">,</span> <span class="token string">'cell phone'</span><span class="token punctuation">,</span>         <span class="token string">'microwave'</span><span class="token punctuation">,</span> <span class="token string">'oven'</span><span class="token punctuation">,</span> <span class="token string">'toaster'</span><span class="token punctuation">,</span> <span class="token string">'sink'</span><span class="token punctuation">,</span> <span class="token string">'refrigerator'</span><span class="token punctuation">,</span> <span class="token string">'book'</span><span class="token punctuation">,</span> <span class="token string">'clock'</span><span class="token punctuation">,</span> <span class="token string">'vase'</span><span class="token punctuation">,</span> <span class="token string">'scissors'</span><span class="token punctuation">,</span> <span class="token string">'teddy bear'</span><span class="token punctuation">,</span>         <span class="token string">'hair drier'</span><span class="token punctuation">,</span> <span class="token string">'toothbrush'</span> <span class="token punctuation">]</span>  <span class="token comment"># class names</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里设置数据集路径有三种方式：</p><ul><li><ol><li>dir: path/to/imgs, </li></ol></li><li><ol start="2"><li>file: path/to/imgs.txt, 或 </li></ol></li><li><ol start="3"><li>list: [path/to/imgs1, path/to/imgs2, .. ] </li></ol></li></ul><h4 id="2-2-创建label标签"><a href="#2-2-创建label标签" class="headerlink" title="2.2 创建label标签"></a>2.2 创建label标签</h4><p>使用<a href="https://github.com/opencv/cvat">CVAT</a>或<a href="https://www.makesense.ai/">makeense.ai</a>等工具标记图像后，将标签导出为<strong>YOLO 格式</strong>，<code>*.txt</code>每个图像一个文件（如果图像中没有对象，则不需要<code>*.txt</code>文件）。该<code>*.txt</code>文件规格有：</p><ul><li>每个对象一行</li><li>每一行都是<code>class x_center y_center width height</code>格式。</li><li>框坐标必须采用<strong>标准化 xywh</strong>格式（从 0 - 1）。如果您的箱子以像素为单位，划分<code>x_center</code>并<code>width</code>通过图像宽度，<code>y_center</code>并<code>height</code>通过图像高度。</li><li>类号是零索引的（从 0 开始）。</li></ul><p><img src="/blog/blog/2022/01/20/yolov5-xun-lian-zi-ding-yi-shu-ju-ji/YOLOv5%E8%AE%AD%E7%BB%83%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20210923105201993-1410184774.png"></p><p>上图对应的标签文件包含2个人（class 0）和一条领带（class 27）：</p><p><img src="/blog/blog/2022/01/20/yolov5-xun-lian-zi-ding-yi-shu-ju-ji/YOLOv5%E8%AE%AD%E7%BB%83%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20210923105227621-579287813.png"></p><h4 id="2-3-整理目录"><a href="#2-3-整理目录" class="headerlink" title="2.3 整理目录"></a>2.3 整理目录</h4><p>根据以下示例组织您的训练和验证图像和标签。YOLOv5 假设 <code>/coco128</code>在<code>/datasets</code>目录<strong>旁边</strong>的<code>/yolov5</code>目录中。<strong>YOLOv5</strong>通过将<code>/images/</code>每个图像路径中的最后一个实例替换为<code>/labels/</code>. 例如：</p><pre class="line-numbers language-file" data-language="file"><code class="language-file">..&#x2F;datasets&#x2F;coco128&#x2F;images&#x2F;im0.jpg   #图像..&#x2F;datasets&#x2F;coco128&#x2F;labels&#x2F;im0.txt   #标签<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>文件结构，如下图所示：</p><p><img src="/blog/blog/2022/01/20/yolov5-xun-lian-zi-ding-yi-shu-ju-ji/YOLOv5%E8%AE%AD%E7%BB%83%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20210923105345948-636890929.png"></p><h4 id="2-4-选择模型型号"><a href="#2-4-选择模型型号" class="headerlink" title="2.4 选择模型型号"></a>2.4 选择模型型号</h4><p>选择一个预训练模型开始训练。这里我们选择<a href="https://github.com/ultralytics/yolov5/blob/master/models/yolov5s.yaml">YOLOv5s</a>，这是可用的最小和最快的模型。有关所有模型的完整比较，请参阅 README<a href="https://github.com/ultralytics/yolov5#pretrained-checkpoints">表</a>。</p><p><img src="/blog/blog/2022/01/20/yolov5-xun-lian-zi-ding-yi-shu-ju-ji/YOLOv5%E8%AE%AD%E7%BB%83%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20210923105602866-1013468094.png"></p><p><img src="/blog/blog/2022/01/20/yolov5-xun-lian-zi-ding-yi-shu-ju-ji/YOLOv5%E8%AE%AD%E7%BB%83%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20210923105615221-285667046.png"></p><h4 id="2-5-train训练"><a href="#2-5-train训练" class="headerlink" title="2.5 train训练"></a>2.5 train训练</h4><p>通过指定数据集、批量大小、图像大小以及预训练<code>--weights yolov5s.pt</code>（推荐）或随机初始化<code>--weights &#39;&#39; --cfg yolov5s.yaml</code>（不推荐），在 COCO128 上训练 YOLOv5s 模型。预训练权重是从<a href="https://github.com/ultralytics/yolov5/releases">最新的 YOLOv5 版本</a>自动下载的。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 在COCO128 上训练YOLOv5s 3 epochs $ python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>所有训练结果都保存在<code>runs/train/</code>递增的运行目录中，即<code>runs/train/exp2</code>，<code>runs/train/exp3</code>等。有关更多详细信息，请参阅我们的 Google Colab Notebook 的训练部分。</p><p><a href="https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="在 Colab 中打开"></a></p><p><a href="https://www.kaggle.com/ultralytics/yolov5"><img src="https://camo.githubusercontent.com/a08ca511178e691ace596a95d334f73cf4ce06e83a5c4a5169b8bb68cac27bef/68747470733a2f2f6b6167676c652e636f6d2f7374617469632f696d616765732f6f70656e2d696e2d6b6167676c652e737667" alt="在 Kaggle 中打开"></a></p><h3 id="三、可视化"><a href="#三、可视化" class="headerlink" title="三、可视化"></a>三、可视化</h3><h4 id="权重和偏差记录（🚀-新）"><a href="#权重和偏差记录（🚀-新）" class="headerlink" title="权重和偏差记录（🚀 新）"></a>权重和偏差记录（🚀 新）</h4><p><a href="https://wandb.ai/site?utm_campaign=repo_yolo_traintutorial">权重和偏差</a>(W&amp;B) 现在与 YOLOv5 集成，用于训练运行的实时可视化和云记录。这允许更好地运行比较和内省，以及提高团队成员之间的可见性和协作。要启用 W&amp;B 日志记录，请安装<code>wandb</code>，然后正常训练（首次使用时将指导您进行设置）。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">pip install wandb<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在训练过程期间，你将在<a href="https://wandb.ai/site?utm_campaign=repo_yolo_traintutorial">https://wandb.ai</a>看到实时更新，并且您可以使用 W&amp;B 报告工具创建结果的<a href="https://wandb.ai/glenn-jocher/yolov5_tutorial/reports/YOLOv5-COCO128-Tutorial-Results--VmlldzozMDI5OTY">详细报告</a>。</p><p><img src="/blog/blog/2022/01/20/yolov5-xun-lian-zi-ding-yi-shu-ju-ji/YOLOv5%E8%AE%AD%E7%BB%83%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20210923114832118-72834769.png"></p><h3 id="四、本地日志"><a href="#四、本地日志" class="headerlink" title="四、本地日志"></a>四、本地日志</h3><p>所有的结果都在默认情况下记录<code>runs/train</code>，为每个新的培训作为创建一个新的实验目录<code>runs/train/exp2</code>，<code>runs/train/exp3</code>等查看火车和Val JPG文件看马赛克，标签，预测和增强效果。请注意，使用 Ultralytics <strong>Mosaic Dataloader</strong>进行训练（如下所示），它在训练期间将 4 个图像组合成 1 个马赛克。</p><p><code>train_batch0.jpg</code> 显示训练批次 0 马赛克和标签：</p><blockquote><p>  <img src="/blog/blog/2022/01/20/yolov5-xun-lian-zi-ding-yi-shu-ju-ji/YOLOv5%E8%AE%AD%E7%BB%83%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20210923115145262-1608192963.png"></p></blockquote><p><code>val_batch0_labels.jpg</code> 显示 val 批次 0 标签：</p><blockquote><p> <img src="/blog/blog/2022/01/20/yolov5-xun-lian-zi-ding-yi-shu-ju-ji/YOLOv5%E8%AE%AD%E7%BB%83%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20210923115245233-2135381433.png"></p></blockquote><p><code>val_batch0_pred.jpg</code>显示 val 批次 0<em>预测</em>：</p><blockquote><p><img src="/blog/blog/2022/01/20/yolov5-xun-lian-zi-ding-yi-shu-ju-ji/YOLOv5%E8%AE%AD%E7%BB%83%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20210923115356355-1086532063.png"></p></blockquote><p>训练结果自动记录到<a href="https://www.tensorflow.org/tensorboard">Tensorboard</a>和<a href="https://github.com/ultralytics/yolov5/pull/4148">CSV</a>中<code>results.csv</code>，<code>results.png</code>训练完成后绘制为（下图）。您还可以<code>results.csv</code>手动绘制任何文件：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">from utils.plots import plot_results plot_results(&#39;path&#x2F;to&#x2F;results.csv&#39;)  # plot &#39;results.csv&#39; as &#39;results.png&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="/blog/blog/2022/01/20/yolov5-xun-lian-zi-ding-yi-shu-ju-ji/YOLOv5%E8%AE%AD%E7%BB%83%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86/1571518-20210923115501205-136492916.png"></p>]]></content>
      
      
      <categories>
          
          <category> 算法模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 炼丹术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>yolact训练模型学习总结</title>
      <link href="/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/"/>
      <url>/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="yolact训练模型学习总结"><a href="#yolact训练模型学习总结" class="headerlink" title="yolact训练模型学习总结"></a>yolact训练模型学习总结</h1><h2 id="一、YOLACT介绍-You-Only-Look-At-CoefficienTs"><a href="#一、YOLACT介绍-You-Only-Look-At-CoefficienTs" class="headerlink" title="一、YOLACT介绍(You Only Look At CoefficienTs)"></a>一、YOLACT介绍(<strong>Y</strong>ou <strong>O</strong>nly <strong>L</strong>ook <strong>A</strong>t <strong>C</strong>oefficien<strong>T</strong>s)</h2><p><img src="/blog/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/yolact%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210918183856980-2126594695.png"></p><h3 id="1-1-简要介绍"><a href="#1-1-简要介绍" class="headerlink" title="1.1 简要介绍"></a>1.1 简要介绍</h3><p>yolact是一种用于实时实例分割的简单、全卷积模型。<br>(A simple, fully convolutional model for real-time instance segmentation. </p><p>论文摘要介绍<strong>Abstract：</strong>我们提出了一个用于实时实例分割的简单全卷积模型，在单个Titan Xp上以33 fps在MS COCO上实现了<strong>29.8 mAP</strong>，这比以前的任何算法都要快得多。此外，我们只在一个GPU上训练后获得此结果。我们通过将实例分割分成两个并行子任务：（1）生成一组原型掩膜（prototype mask）；（2）预测每个实例的掩膜系数（mask coefficients）。然后我们通过将原型与掩模系数线性组合来生成实例掩膜（instance masks）。我们发现因为这个过程不依赖于 repooling，所以这种方法可以产生非常高质量的掩模。此外，我们分析了 the emergent behavior of our prototypes，并表明他们学会以 translation variant manner 定位实例，尽管是完全卷积的。最后，我们还提出了快速NMS（Fast NMS），比标准NMS快12 ms，只有一点点性能损失。</p><p><img src="/blog/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/yolact%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210919090752129-421635284.png"></p><h3 id="1-2-paper-with-code"><a href="#1-2-paper-with-code" class="headerlink" title="1.2 paper with code"></a>1.2 paper with code</h3><p>下面是官方团队论文和代码(两个白线)：</p><ul><li><a href="https://arxiv.org/abs/1904.02689">YOLACT：实时实例分割</a></li><li><a href="https://arxiv.org/abs/1912.06218">YOLACT++：更好的实时实例分割</a></li><li>paper：<a href="https://arxiv.org/pdf/1904.02689.pdf">yolact</a></li><li>github:<a href="https://github.com/dbolya/yolact">yolact</a></li></ul><p>YOLACT++ (v1.2) 发布！(<a href="https://gitee.com/moonharbour/yolact/blob/master/CHANGELOG.md">变更日志</a>)</p><p>YOLACT++ 的 resnet50 模型在 Titan Xp 上以 33.5 fps 运行，在 COCO 上达到 34.1 mAP <code>test-dev</code>（<a href="https://arxiv.org/abs/1912.06218">在此处</a>查看我们的期刊论文）。</p><p>为了使用 YOLACT++，请确保编译 DCNv2 代码。（可见<a href="https://github.com/dbolya/yolact#installation">安装</a>）</p><h3 id="1-3-测试效果"><a href="#1-3-测试效果" class="headerlink" title="1.3 测试效果"></a>1.3 测试效果</h3><p>下面是在coco数据集上的测试效果:</p><table><thead><tr><th align="center"><img src="/blog/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/1571518-20210918160449628-210667607.png" style="zoom:50%;"></th><th align="center"><img src="/blog/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/1571518-20210918160457322-1068890880.png" style="zoom:50%;"></th><th><img src="/blog/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/1571518-20210918160516160-1801174708.png" style="zoom:50%;"></th></tr></thead></table><p>此处是一个实时实例分割的Demo演示视频：</p><iframe src="//player.bilibili.com/player.html?aid=720581661&bvid=BV19Q4y1r78q&cid=410345635&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><h2 id="二、环境搭建及配置"><a href="#二、环境搭建及配置" class="headerlink" title="二、环境搭建及配置"></a>二、环境搭建及配置</h2><h3 id="2-1-搭建环境"><a href="#2-1-搭建环境" class="headerlink" title="2.1 搭建环境"></a>2.1 搭建环境</h3><ul><li><p>搭建Python3环境。<br>这里可以利用anaconda新建一个虚拟环境，然后参考<code>environment.yml</code>进行相应依赖库的安装。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 新建虚拟环境conda create -n train python&#x3D;3.7# 激活虚拟环境conda activate train# 安装相应依赖库pip install opencv-python&#x3D;&#x3D;4.5.3.56pip install matplotlib&#x3D;&#x3D;3.4.3pip install pillow&#x3D;&#x3D;8.3.1 pip install pycocotools&#x3D;&#x3D;2.0.2 pip install PyQt5&#x3D;&#x3D;5.9.2pip install torchvision&#x3D;&#x3D;0.8.1pip install pytorch&#x3D;&#x3D;1.7.1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>安装 <a href="http://pytorch.org/">Pytorch</a> 1.0.1（或更高版本）和 TorchVision。</p></li><li><p>安装一些其他软件包：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># Cython needs to be installed before pycocotoolspip install cythonpip install opencv-python pillow pycocotools matplotlib <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>克隆这个存储库并进入该目录：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git clone https:&#x2F;&#x2F;github.com&#x2F;dbolya&#x2F;yolact.gitcd yolact<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p>如果您想训练 YOLACT，请下载COCO数据集和2014/2017注释。请注意，此脚本将需要一段时间并将 21gb 的文件转储到 <code>./data/coco</code>.<br>下面是通过脚本下载COCO数据集的方式:</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">sh data&#x2F;scripts&#x2F;COCO.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>如果您想在 上评估 YOLACT <code>test-dev</code>，请<code>test-dev</code>使用此脚本下载。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">sh data&#x2F;scripts&#x2F;COCO_test.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>如果要使用 YOLACT++，请编译可变形卷积层（来自<a href="https://github.com/CharlesShang/DCNv2/tree/pytorch_1.0">DCNv2</a>）。确保您从<a href="https://developer.nvidia.com/cuda-toolkit">NVidia 的网站</a>安装了最新的 CUDA 工具包。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cd external&#x2F;DCNv2 python setup.py build develop<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这里以上几步基本可以参考官方readme的步骤。在数据集下载的时候，也可直接通过<code>COCO.sh</code>脚本下的下载地址利用wget进行下载，解压文件之后存放到指定位置就好。</p><p>在./data目录下执行：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 数据集与验证集下载wget http:&#x2F;&#x2F;images.cocodataset.org&#x2F;zips&#x2F;train2017.zipwget http:&#x2F;&#x2F;images.cocodataset.org&#x2F;zips&#x2F;val2017.zipunzip -qqjd ..&#x2F;images ..&#x2F;images&#x2F;train2017.zipunzip -qqjd ..&#x2F;images ..&#x2F;images&#x2F;val2017.zip# 数据集相关标注文件wget http:&#x2F;&#x2F;images.cocodataset.org&#x2F;annotations&#x2F;annotations_trainval2014.zipwget http:&#x2F;&#x2F;images.cocodataset.org&#x2F;annotations&#x2F;annotations_trainval2017.zipunzip -qqd .. .&#x2F;annotations_trainval2014.zipunzip -qqd .. .&#x2F;annotations_trainval2017.zip# 清除压缩文件rm ..&#x2F;images&#x2F;train2017.ziprm ..&#x2F;images&#x2F;val2017.ziprm .&#x2F;annotations_trainval2014.ziprm .&#x2F;annotations_trainval2017.zip<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="2-2-模型评估"><a href="#2-2-模型评估" class="headerlink" title="2.2 模型评估"></a>2.2 模型评估</h3><p>在搭建完成基本环境与源码之后，我们需要下载官方作者开放的对应权重模型，也即为weights下的.pth文件，若yolact根目录下没有weights文件目录则需要自行创建。</p><h4 id="2-2-1-评估数据预览及权重文件下载"><a href="#2-2-1-评估数据预览及权重文件下载" class="headerlink" title="2.2.1 评估数据预览及权重文件下载"></a>2.2.1 评估数据预览及权重文件下载</h4><p>下面给出了yolact在不同Image Size大小下的FPS 和 mAP评估数据以及相应的权重文件(截止到2019 年 4 月 5 日)：</p><table><thead><tr><th align="center">Image Size</th><th align="center">Backbone</th><th align="center">FPS</th><th align="center">mAP</th><th>Weights(谷歌云端硬盘)</th><th>镜像</th></tr></thead><tbody><tr><td align="center">550</td><td align="center">Resnet50-FPN</td><td align="center">42.5</td><td align="center">28.2</td><td><a href="https://drive.google.com/file/d/1yp7ZbbDwvMiFJEq4ptVKTYTI2VeRDXl0/view?usp=sharing">yolact_resnet50_54_800000.pth</a></td><td><a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EUVpxoSXaqNIlssoLKOEoCcB1m0RpzGq_Khp5n1VX3zcUw">Mirror</a></td></tr><tr><td align="center">550</td><td align="center">Darknet53-FPN</td><td align="center">40.0</td><td align="center">28.7</td><td><a href="https://drive.google.com/file/d/1dukLrTzZQEuhzitGkHaGjphlmRJOjVnP/view?usp=sharing">yolact_darknet53_54_800000.pth</a></td><td><a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/ERrao26c8llJn25dIyZPhwMBxUp2GdZTKIMUQA3t0djHLw">Mirror</a></td></tr><tr><td align="center">550</td><td align="center">Resnet101-FPN</td><td align="center">33.5</td><td align="center">29.8</td><td><a href="https://drive.google.com/file/d/1UYy3dMapbH1BnmtZU4WH1zbYgOzzHHf_/view?usp=sharing">yolact_base_54_800000.pth</a></td><td><a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EYRWxBEoKU9DiblrWx2M89MBGFkVVB_drlRd_v5sdT3Hgg">Mirror</a></td></tr><tr><td align="center">700</td><td align="center">Resnet101-FPN</td><td align="center">23.6</td><td align="center">31.2</td><td><a href="https://drive.google.com/file/d/1lE4Lz5p25teiXV-6HdTiOJSnS7u7GBzg/view?usp=sharing">yolact_im700_54_800000.pth</a></td><td><a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/Eagg5RSc5hFEhp7sPtvLNyoBjhlf2feog7t8OQzHKKphjw">Mirror</a></td></tr></tbody></table><p>同上，下面是YOLACT++模型 (2019 年 12 月 16 日发布):</p><table><thead><tr><th align="center">Image Size</th><th align="center">Backbone</th><th align="center">FPS</th><th align="center">mAP</th><th>Weights(谷歌云端硬盘地址)</th><th>镜像</th></tr></thead><tbody><tr><td align="center">550</td><td align="center">Resnet50-FPN</td><td align="center">33.5</td><td align="center">34.1</td><td><a href="https://drive.google.com/file/d/1ZPu1YR2UzGHQD0o1rEqy-j5bmEm3lbyP/view?usp=sharing">yolact_plus_resnet50_54_800000.pth</a></td><td><a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EcJAtMiEFlhAnVsDf00yWRIBUC4m8iE9NEEiV05XwtEoGw">Mirror</a></td></tr><tr><td align="center">550</td><td align="center">Resnet101-FPN</td><td align="center">27.3</td><td align="center">34.6</td><td><a href="https://drive.google.com/file/d/15id0Qq5eqRbkD-N3ZjDZXdCvRyIaHpFB/view?usp=sharing">yolact_plus_base_54_800000.pth</a></td><td><a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EVQ62sF0SrJPrl_68onyHF8BpG7c05A8PavV4a849sZgEA">Mirror</a></td></tr></tbody></table><p>要评估模型，请将相应的权重文件放在<code>./weights</code>目录中并运行以下命令之一。每个配置的名称是文件名中数字之前的所有内容（例如，<code>yolact_base</code>for <code>yolact_base_54_800000.pth</code>）。</p><p>下面提供一段脚本代码进行这部分内容的整合，(由于Weights列中为谷歌云端硬盘的下载链接，因此这里设置的是Mirror镜像链接地址)<br>为方便下载，这里提供腾讯微云的下载链接: <a href="https://share.weiyun.com/uH6hSGX5">yolact_weight</a></p><p>如何去评估一个模型的好坏及其训练后的性能，我们需要对其数值数据上进行定性、定量及其最基准的评估与分析，同时直接对其进行图像与视频的预测测试。</p><h4 id="2-2-2-评估COCO数据集下的定量结果"><a href="#2-2-2-评估COCO数据集下的定量结果" class="headerlink" title="2.2.2 评估COCO数据集下的定量结果"></a>2.2.2 评估COCO数据集下的定量结果</h4><p>杳在整个验证集上量化评估训练模型，需确保已按上述方式下载 COCO数据集。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">##mAP。#输出一个COCOEval json 提交给网站或者使用run_coco_eval.py 脚本。#此命令将分别创建 &#39;.&#x2F;results&#x2F;bbox_detections.json&#39; 和 &#39;.&#x2F;results&#x2F;mask_detections.json&#39; 用于检测和实例分割。python eval.py --trained_model&#x3D;weights&#x2F;yolact_base_54_800000.pth#您可以在上一个命令中创建的文件上运行 COCOEval。性能应该与在 eval.py 中的实现相匹配。python eval.py --trained_model&#x3D;weights&#x2F;yolact_base_54_800000.pth --output_coco_json#要为 test-dev 输出 coco json 文件，请确保您已经从上面下载了 test-dev 并转到对应位置python run_coco_eval.pypython eval.py --trained_model&#x3D;weights&#x2F;yolact_base_54_800000.pth --output_coco_json --dataset&#x3D;coco2017_testdev_dataset<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-2-3-评估COCO数据集下的定性结果"><a href="#2-2-3-评估COCO数据集下的定性结果" class="headerlink" title="2.2.3 评估COCO数据集下的定性结果"></a>2.2.3 评估COCO数据集下的定性结果</h4><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#在 COCO 上显示定性结果。从这里开始，我将使用 0.15 的置信阈值。python eval.py --trained_model&#x3D;weights&#x2F;yolact_base_54_800000.pth --score_threshold&#x3D;0.15 --top_k&#x3D;15 --display<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h4 id="2-2-4-评估COCO数据集下的基准测试"><a href="#2-2-4-评估COCO数据集下的基准测试" class="headerlink" title="2.2.4 评估COCO数据集下的基准测试"></a>2.2.4 评估COCO数据集下的基准测试</h4><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#只在验证集的前 1k 个图像上运行原始模型python eval.py --trained_model&#x3D;weights&#x2F;yolact_base_54_800000.pth --benchmark --max_images&#x3D;1000<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h4 id="2-2-5-在COCO预训练数据模型上进行图像测试"><a href="#2-2-5-在COCO预训练数据模型上进行图像测试" class="headerlink" title="2.2.5 在COCO预训练数据模型上进行图像测试"></a>2.2.5 在COCO预训练数据模型上进行图像测试</h4><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 在指定图像上显示定性结果# 对单张图片进行预测python eval.py --trained_model&#x3D;weights&#x2F;yolact_base_54_800000.pth --score_threshold&#x3D;0.15 --top_k&#x3D;15 --image&#x3D;my_image.png# 处理一个图像并将其保存到另一个文件中python eval.py --trained_model&#x3D;weights&#x2F;yolact_base_54_800000.pth --score_threshold&#x3D;0.15 --top_k&#x3D;15 --image&#x3D;input_image.png:output_image.png# 处理整个文件夹的图像python eval.py --trained_model&#x3D;weights&#x2F;yolact_base_54_800000.pth --score_threshold&#x3D;0.15 --top_k&#x3D;15 --images&#x3D;path&#x2F;to&#x2F;input&#x2F;folder:path&#x2F;to&#x2F;output&#x2F;folder<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>下面是COCO图像测试的结果：</p><table><thead><tr><th><img src="/blog/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/yolact%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210919103942949-2256516.png"></th><th><img src="/blog/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/yolact%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210919103951730-1426578431.png"></th><th><img src="/blog/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/yolact%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210919103957622-192417666.png"></th></tr></thead></table><h4 id="2-2-6-在COCO预训练数据模型上进行视频测试"><a href="#2-2-6-在COCO预训练数据模型上进行视频测试" class="headerlink" title="2.2.6 在COCO预训练数据模型上进行视频测试"></a>2.2.6 在COCO预训练数据模型上进行视频测试</h4><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#实时显示视频。“--video_multiframe”将一次处理那么多帧以提高性能。#如果需要，可以使用“--display_fps”直接在帧上绘制FPS。# 对指定视频进行推理预测python eval.py --trained_model&#x3D;weights&#x2F;yolact_base_54_800000.pth --score_threshold&#x3D;0.15 --top_k&#x3D;15 --video_multiframe&#x3D;4 --video&#x3D;my_video.mp4# 实时显示网络摄像头提要。如果您有多个网络摄像头，请传递您想要的网络摄像头索引而不是 0。python eval.py --trained_model&#x3D;weights&#x2F;yolact_base_54_800000.pth --score_threshold&#x3D;0.15 --top_k&#x3D;15 --video_multiframe&#x3D;4 --video&#x3D;0# 处理视频并将其保存到另一个文件。现在使用与上面相同的管道，所以速度很快！python eval.py --trained_model&#x3D;weights&#x2F;yolact_base_54_800000.pth --score_threshold&#x3D;0.15 --top_k&#x3D;15 --video_multiframe&#x3D;4 --video&#x3D;input_video.mp4:output_video.mp4<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>下面是针对大象视频做图像分割的预测示例:</p><iframe src="//player.bilibili.com/player.html?aid=805515750&bvid=BV1K34y1X7W6&cid=410761259&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><h3 id="2-3-训练参数及其文件配置"><a href="#2-3-训练参数及其文件配置" class="headerlink" title="2.3 训练参数及其文件配置"></a>2.3 训练参数及其文件配置</h3><p>默认情况下，我们在 COCO 上训练。训练前需确保已下载所用的整个数据集。</p><h4 id="2-3-1-训练说明"><a href="#2-3-1-训练说明" class="headerlink" title="2.3.1 训练说明"></a>2.3.1 训练说明</h4><ul><li>要进行训练，请获取一个 imagenet 预训练模型并将其放入<code>./weights</code><ul><li>对于 Resnet101，请<code>resnet101_reducedfc.pth</code>从<a href="https://drive.google.com/file/d/1tvqFPd4bJtakOlmn-uIA492g2qurRChj/view?usp=sharing">这里</a>下载。</li><li>对于 Resnet50，<code>resnet50-19c8e357.pth</code>从<a href="https://drive.google.com/file/d/1Jy3yCdbatgXa5YYIdTCRrSV0S9V5g1rn/view?usp=sharing">这里</a>下载。</li><li>对于 Darknet53，请<code>darknet53.pth</code>从<a href="https://drive.google.com/file/d/17Y431j4sagFpSReuPNoFcj9h7azDTZFf/view?usp=sharing">这里</a>下载。</li></ul></li><li>运行以下训练命令之一。<ul><li>请注意，您可以在训练时按 ctrl+c，它将<code>*_interrupt.pth</code>在当前迭代中保存一个文件。</li><li><code>./weights</code>默认情况下，所有权重都以文件名保存在目录中<code>&lt;config&gt;_&lt;epoch&gt;_&lt;iter&gt;.pth</code>。</li></ul></li></ul><h4 id="2-3-2-训练操作及参数配置"><a href="#2-3-2-训练操作及参数配置" class="headerlink" title="2.3.2 训练操作及参数配置"></a>2.3.2 训练操作及参数配置</h4><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 使用批量大小为 8（默认值）的基本配置进行训练。python train.py --config&#x3D;yolact_base_config# 训练 yolact_base_config 的 batch_size 为 5。# 对于 550px 模型，1 个批次占用大约 1.5 gig 的 VRAM，因此相应地指定。python train.py --config&#x3D;yolact_base_config --batch_size&#x3D;5# 使用特定的权重文件恢复训练 yolact_base，并从权重文件名中指定的迭代开始。python train.py --config&#x3D;yolact_base_config --resume&#x3D;weights&#x2F;yolact_base_10_32100.pth --start_iter&#x3D;-1# 使用帮助选项查看所有可用命令行参数的描述python train.py --help<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-3-3-使用多GPU支持训练"><a href="#2-3-3-使用多GPU支持训练" class="headerlink" title="2.3.3 使用多GPU支持训练"></a>2.3.3 使用多GPU支持训练</h4><p>YOLACT 现在在训练期间无缝支持多个 GPU：</p><ul><li><p>在运行任何脚本之前，运行： </p><pre class="line-numbers language-none"><code class="language-none">export CUDA_VISIBLE_DEVICES&#x3D;[gpus]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>您应该将 [gpus] 替换为您要使用的每个 GPU 的索引的逗号分隔列表（例如，0,1,2,3）。</li><li>如果只使用 1 个 GPU，您仍然应该这样做。</li><li>您可以使用<code>nvidia-smi</code>.</li></ul></li><li><p>然后，只需</p><pre class="line-numbers language-none"><code class="language-none">8*num_gpus<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>使用上面的训练命令将批量大小设置为。训练脚本会自动将超参数缩放到正确的值。</p><ul><li>如果您有空闲内存，您可以进一步增加批量大小，但将其保持为您正在使用的 GPU 数量的倍数。</li><li>如果要为每个 GPU 分配特定于不同 GPU 的图像，可以使用<code>--batch_alloc=[alloc]</code>其中 [alloc] 是逗号分隔的列表，其中包含每个 GPU 上的图像数量。这必须总和为<code>batch_size</code>。</li></ul></li></ul><h4 id="2-3-4-训练日志记录"><a href="#2-3-4-训练日志记录" class="headerlink" title="2.3.4 训练日志记录"></a>2.3.4 训练日志记录</h4><p>YOLACT 现在默认记录训练和验证信息。您可以使用<code>--no_log</code>. 如何进行可视化，这里官方作者没有进一步说明，但不妨参考一下<code>LogVizualizer</code>在<code>utils/logger.py</code>寻求帮助。</p><h4 id="2-3-5-训练示例Pascal-SBD"><a href="#2-3-5-训练示例Pascal-SBD" class="headerlink" title="2.3.5 训练示例Pascal SBD"></a>2.3.5 训练示例Pascal SBD</h4><p>这里还包括一个用于训练 Pascal SBD 注释的配置（用于快速实验或与其他方法进行比较）。要在 Pascal SBD 上进行训练，请继续执行以下步骤：</p><ol><li>从<a href="http://home.bharathh.info/pubs/codes/SBD/download.html">这里</a>下载数据集。它是顶部“概述”部分中的第一个链接（该文件名为<code>benchmark.tgz</code>）。</li><li>将数据集提取到某处。在数据集中应该有一个名为<code>dataset/img</code>. 创建目录<code>./data/sbd</code>（<code>.</code>YOLACT 的根目录在哪里）并复制<code>dataset/img</code>到<code>./data/sbd/img</code>.</li><li>从<a href="https://drive.google.com/open?id=1ExrRSPVctHW8Nxrn0SofU1lVhK5Wn0_S">这里</a>下载 COCO 风格的注释。</li><li>将注释提取到<code>./data/sbd/</code>.</li><li>现在您可以使用<code>--config=yolact_resnet50_pascal_config</code>. 检查该配置以了解如何将其扩展到其他模型。</li></ol><p>转换注释的脚本位置在<code>./scripts/convert_sbd.py</code>.<br>如果想验证我们的结果，可以<code>yolact_resnet50_pascal_config</code>从<a href="https://drive.google.com/open?id=1yLVwtkRtNxyl0kxeMCtPXJsXFFyc_FHe">这里</a>下载我们的权重。<br>这个模型应该得到 72.3 掩码 AP_50 和 56.2 掩码 AP_70。请注意，“所有”AP 与其他论文中为 pascal 报告的“vol”AP 不同（它们使用<code>0.1 - 0.9</code>增量阈值的平均值，<code>0.1</code>而不是 COCO 使用的阈值）。</p><h4 id="2-3-6-自定义数据集训练"><a href="#2-3-6-自定义数据集训练" class="headerlink" title="2.3.6 自定义数据集训练"></a>2.3.6 自定义数据集训练</h4><p>您还可以按照以下步骤对自己的数据集进行训练：</p><h5 id="2-3-6-1-从预训练开始训练"><a href="#2-3-6-1-从预训练开始训练" class="headerlink" title="2.3.6.1 从预训练开始训练"></a>2.3.6.1 从预训练开始训练</h5><ul><li><p>为您的数据集创建一个 COCO 风格的对象检测 JSON 注释文件。可以在<a href="http://cocodataset.org/#format-data">此处</a>找到<a href="http://cocodataset.org/#format-data">此</a>规范。请注意，我们不使用某些字段，因此可以省略以下内容：</p><ul><li><code>info</code></li><li><code>liscense</code></li><li>Under <code>image</code>: <code>license, flickr_url, coco_url, date_captured</code></li><li><code>categories</code> (we use our own format for categories, see below)</li></ul></li><li><p>在<code>dataset_base</code>in下为您的数据集创建一个定义<code>data/config.py</code>（<code>dataset_base</code>有关每个字段的解释，请参阅 in中的注释）：</p></li></ul><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">my_custom_dataset = dataset_base.copy(<span class="token punctuation">&#123;</span>    <span class="token key atrule">'name'</span><span class="token punctuation">:</span> <span class="token string">'My Dataset'</span><span class="token punctuation">,</span>    <span class="token key atrule">'train_images'</span><span class="token punctuation">:</span> <span class="token string">'path_to_training_images'</span><span class="token punctuation">,</span>    <span class="token key atrule">'train_info'</span><span class="token punctuation">:</span>   <span class="token string">'path_to_training_annotation'</span><span class="token punctuation">,</span>    <span class="token key atrule">'valid_images'</span><span class="token punctuation">:</span> <span class="token string">'path_to_validation_images'</span><span class="token punctuation">,</span>    <span class="token key atrule">'valid_info'</span><span class="token punctuation">:</span>   <span class="token string">'path_to_validation_annotation'</span><span class="token punctuation">,</span>    <span class="token key atrule">'has_gt'</span><span class="token punctuation">:</span> <span class="token boolean important">True</span><span class="token punctuation">,</span>    <span class="token key atrule">'class_names'</span><span class="token punctuation">:</span> ('my_class_id_1'<span class="token punctuation">,</span> <span class="token string">'my_class_id_2'</span><span class="token punctuation">,</span> <span class="token string">'my_class_id_3'</span><span class="token punctuation">,</span> <span class="token punctuation">...</span>)<span class="token punctuation">&#125;</span>)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>需要注意的几点：<ul><li>注释文件中的类 ID 应从 1 开始并按<code>class_names</code>. 如果这不是您的注释文件（就像COCO）的情况下，看到现场<code>label_map</code>的<code>dataset_base</code>。</li><li>如果您不想创建验证拆分，请使用相同的图像路径和注释文件进行验证。默认情况下（参见<code>python train.py --help</code>），<code>train.py</code>将每 2 个时期输出数据集中前 5000 个图像的验证 mAP。</li></ul></li><li>最后，在<code>yolact_base_config</code>同一个文件中，更改<code>&#39;dataset&#39;</code>to<code>&#39;my_custom_dataset&#39;</code>或您在上面命名的配置对象的值。然后您可以使用上一节中的任何训练命令。</li></ul><h5 id="2-3-6-2从头开始创建自定义数据集"><a href="#2-3-6-2从头开始创建自定义数据集" class="headerlink" title="2.3.6.2从头开始创建自定义数据集"></a>2.3.6.2<strong>从头开始创建自定义数据集</strong></h5><p>这里使用Labelme 和 labelme2coco.py 可以很好地创建数据集。对于计划在自定义数据集上微调/训练模型的任何人，请按照以下步骤操作：</p><ul><li><p>下载并安装 Labelme ( <a href="https://github.com/wkentaro/labelme">https://github.com/wkentaro/labelme</a> )</p></li><li><p>使用提供的多边形标签( Create Polygons)选项进行标记</p></li><li><p>将为每个标记的图像创建一个 json 文件(与图像同名)</p><p><img src="/blog/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/yolact%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210919152333156-1308662815.png"></p></li><li><p>创建一个 label.txt</p><p>例如下面这样形式:</p><pre class="line-numbers language-txt" data-language="txt"><code class="language-txt">__ignore___background_elephantbirddogcatperson<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><p><img src="/blog/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/yolact%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210919154125368-721592120.png"></p><ul><li>转换生成coco风格的 json <pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">.&#x2F;labelme2coco.py &lt;labelled_data_folder&gt; &lt;out_folder&gt; --labels labels.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>生成的内容如下:<img src="/blog/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/1571518-20210919154227366-1129313568.png" style="zoom:67%;"></li></ul><p> <img src="/blog/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/yolact%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210919154021879-1532117218.png"></p><p>这部分的更多细节可见：<a href="https://github.com/wkentaro/labelme/tree/master/examples/instance_segmentation">链接</a></p><h5 id="2-3-6-3-配置configs数据集路径与训练参数"><a href="#2-3-6-3-配置configs数据集路径与训练参数" class="headerlink" title="2.3.6.3 配置configs数据集路径与训练参数"></a>2.3.6.3 配置configs数据集路径与训练参数</h5><p>数据集制作好了之后，还不能直接进一步训练，这里还缺了关键的一步，就是在config配置文件中修改对应的参数与指定路径。</p><p>这里主要修改两部分：my_custom_dataset与yolact_im400_custom_cfg<br>具体如下：</p><h6 id="2-3-6-3-1-配置数据集"><a href="#2-3-6-3-1-配置数据集" class="headerlink" title="2.3.6.3.1 配置数据集"></a>2.3.6.3.1 配置数据集</h6><p>在DATASETS下面相应的加上下面的部分，用来设置自己的数据集。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># TODO:setting my dataset</span>my_custom_dataset <span class="token operator">=</span> dataset_base<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">&#123;</span>    <span class="token string">'name'</span><span class="token punctuation">:</span> <span class="token string">'my_custom_dataset'</span><span class="token punctuation">,</span>    <span class="token string">'train_images'</span><span class="token punctuation">:</span> <span class="token string">'/home/hxzh02/WORK/yolact/data/data_tower/'</span><span class="token punctuation">,</span>    <span class="token string">'train_info'</span><span class="token punctuation">:</span> <span class="token string">'/home/hxzh02/WORK/yolact/data/data_animal/annations/instance_train.json'</span><span class="token punctuation">,</span>    <span class="token string">'valid_images'</span><span class="token punctuation">:</span> <span class="token string">'/home/hxzh02/WORK/yolact/data/data_animal/'</span><span class="token punctuation">,</span>    <span class="token string">'valid_info'</span><span class="token punctuation">:</span> <span class="token string">'/home/hxzh02/WORK/yolact/data/data_animal/annations/instance_train.json'</span><span class="token punctuation">,</span>    <span class="token string">'has_gt'</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token string">'class_names'</span><span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token string">'elephant'</span><span class="token punctuation">,</span><span class="token string">'bird'</span><span class="token punctuation">,</span><span class="token string">'dog'</span><span class="token punctuation">,</span><span class="token string">'cat'</span><span class="token punctuation">,</span><span class="token string">'person'</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>说明：此处的<code>&#39;name&#39;: &#39;my_custom_dataset&#39;</code>是与后面的 <code>&#39;dataset&#39;: my_custom_dataset</code>建立绑定关系的纽带，若设置出错则有可能读不到数据，或是出现其他类型报错等等异常。<br>另外需要说明的是，若需要检测的目标只有一类，也即为单分类时，需要将’class_names’的后面添加’,’作为区分，例如(‘dog’)，否则程序会默认将’d’, ‘o’, ‘g’三个字母作为实例目标分类名称，从而得到的分类和分类数量都是错误的。若为多分类则可以不用加’,’，正常设置即可。</p><p>如下图:</p><p><img src="/blog/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/yolact%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210919155329345-649948128.png"></p><h6 id="2-3-6-3-2-配置训练设置cfg"><a href="#2-3-6-3-2-配置训练设置cfg" class="headerlink" title="2.3.6.3.2 配置训练设置cfg"></a>2.3.6.3.2 配置训练设置cfg</h6><p>在YOLACT v1.0 CONFIGS下对应的位置加上此段部分，用来建立配置文件与训练的关系。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># TODO base_confid</span>yolact_im400_custom_cfg <span class="token operator">=</span> yolact_base_config<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">&#123;</span>    <span class="token string">'name'</span><span class="token punctuation">:</span> <span class="token string">'yolact_im400'</span><span class="token punctuation">,</span>    <span class="token comment"># Dataset stuff</span>    <span class="token string">'dataset'</span><span class="token punctuation">:</span> my_custom_dataset<span class="token punctuation">,</span> <span class="token comment"># 数据集定义</span>    <span class="token string">'num_classes'</span><span class="token punctuation">:</span> <span class="token builtin">len</span><span class="token punctuation">(</span>my_custom_dataset<span class="token punctuation">.</span>class_names<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token comment"># 类别数</span>    <span class="token string">'max_iter'</span><span class="token punctuation">:</span> <span class="token number">2000</span><span class="token punctuation">,</span>               <span class="token comment"># 最大迭代次数</span>    <span class="token string">'lr_steps'</span><span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token number">500</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">,</span><span class="token number">1500</span><span class="token punctuation">)</span><span class="token punctuation">,</span>   <span class="token comment"># 学习率衰减区间</span>    <span class="token string">'max_size'</span><span class="token punctuation">:</span> <span class="token number">416</span><span class="token punctuation">,</span>    <span class="token string">'backbone'</span><span class="token punctuation">:</span> yolact_base_config<span class="token punctuation">.</span>backbone<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">&#123;</span>        <span class="token string">'pred_scales'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> yolact_base_config<span class="token punctuation">.</span>max_size <span class="token operator">*</span> <span class="token number">400</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">for</span> x <span class="token keyword">in</span>                        yolact_base_config<span class="token punctuation">.</span>backbone<span class="token punctuation">.</span>pred_scales<span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>说明：此处的yolact_im400_custom_cfg即为config设置的可选项，定义后可直接在训练时进行调用。这里需要说明的是，程序中没有提供轮次的修改，可根据最大迭代次数进行计算。比如我的数据集只有2000张左右，批次给的是4，我设置了25000，大概迭代轮次为50轮，可以根据自己数据集数量依次类推设置，lr_steps是学习率衰减区间，也可以根据自己设置的最大迭代次数进行设置。至此，配置也修改完成了。</p><p>如下图：</p><p><img src="/blog/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/yolact%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210919155757053-1811199327.png"></p><h6 id="2-3-6-3-3-开始训练"><a href="#2-3-6-3-3-开始训练" class="headerlink" title="2.3.6.3.3 开始训练"></a>2.3.6.3.3 开始训练</h6><p>与config默认设置为yolact_base_config，训练时默认使用</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python train.py --config&#x3D;yolact_base_config<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>类似的，在训练自定义数据集时，只需要修改config的可选参数，再根据需要对其他参数进行相应的修改即可，如下所示：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python3 train.py --config&#x3D;yolact_im400_custom_cfg <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python train.py --config&#x3D;yolact_im400_custom_cfg --batch_size&#x3D;5<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>成功开始训练会出现以下信息：<br><img src="/blog/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/yolact%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210922110522079-342846004.png"><br>计算mAP<br><img src="/blog/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/yolact%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210922110609088-1054269276.png"></p><p>默认迭代10000次会保存一次模型，可以在传参时进行修改。如果训练中断可以使用–resume进行恢复训练：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python train.py --config&#x3D;yolact_im400_custom_cfg --batch_size 4 --resume<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>训练过程完成后，或者提前打断时，相应地会在./weights目录下根据迭代次数等参数生成对应的权重文件，如下图:</p><p><img src="/blog/blog/2022/01/20/yolact-xun-lian-mo-xing-xue-xi-zong-jie/yolact%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210919161609089-789737706.png"></p><h6 id="2-3-6-3-4-评估训练模型"><a href="#2-3-6-3-4-评估训练模型" class="headerlink" title="2.3.6.3.4 评估训练模型"></a>2.3.6.3.4 评估训练模型</h6><p>得到权重文件后，我们自然是希望评估模型在验证集上的性能及其他表现，那么类似上面的流程，我们可以通过以下命令来继续评估模型数值作为模型过程的参考。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python eval.py --trained_model&#x3D;weights&#x2F;yolact_im400_19_38_interrupt.pth<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h6 id="2-3-6-3-5-测试训练模型"><a href="#2-3-6-3-5-测试训练模型" class="headerlink" title="2.3.6.3.5 测试训练模型"></a>2.3.6.3.5 测试训练模型</h6><p>先来看默认设置是怎么测试单张图像：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python3 eval.py --trained_model&#x3D;weights&#x2F;yolact_im400_9999_50000.pth --score_threshold&#x3D;0.3 --top_k&#x3D;100 --image&#x3D;&#x2F;home&#x2F;hxzh02&#x2F;WORK&#x2F;yolact&#x2F;data&#x2F;data_tower&#x2F;000004.jpg<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>同理，我们也可以在自定义数据集训练后对其进行图像与视频测试，与默认设置的相比，区别只在于需要修改测试的图片路径以及权重的路径：<br>如下：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python3 eval.py --trained_model&#x3D;weights&#x2F;yolact_im400_7999_40000.pth --score_threshold&#x3D;0.3 --top_k&#x3D;100 --image&#x3D;image&#x2F;0000000000125.jpg<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 炼丹术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>UNet图像分割模型相关总结</title>
      <link href="/blog/2022/01/20/unet-tu-xiang-fen-ge-mo-xing-xiang-guan-zong-jie/"/>
      <url>/blog/2022/01/20/unet-tu-xiang-fen-ge-mo-xing-xiang-guan-zong-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="UNet图像分割模型相关总结"><a href="#UNet图像分割模型相关总结" class="headerlink" title="UNet图像分割模型相关总结"></a>UNet图像分割模型相关总结</h2><h3 id="1-制作图像分割数据集"><a href="#1-制作图像分割数据集" class="headerlink" title="1.制作图像分割数据集"></a>1.制作图像分割数据集</h3><h4 id="1-1使用labelme进行标注"><a href="#1-1使用labelme进行标注" class="headerlink" title="1.1使用labelme进行标注"></a>1.1使用labelme进行标注</h4><p>(注：labelme与labelImg类似，都属于对图像数据集进行标注的软件。但不同的是，labelme更关心对象的边缘和轮廓细节，也即通过生成和训练图像对应的mask来实现图像分割的目的。这里的分割一般使用的是闭合多边形折线来进行标注，每张图片标注完成后，按下Ctrl+S来进行保存，此时存储的文件是与图片同名的.json格式文件。)</p><p><img src="/blog/blog/2022/01/20/unet-tu-xiang-fen-ge-mo-xing-xiang-guan-zong-jie/UNet%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/1571518-20210827152523534-1020421557.png"></p><p>我们要得到的结果是mask，保存生成的.json文件还需要通过转换得到对应的mask图像。<br>(这里的转换有两种方式，一种是找到当前python环境下的labelme_json_to_dataset.py进行修改，二是直接在命令行中调用对应的接口labelme_json_to_dataset {file}生成mask，由于单命令行直接执行一个文件的生成，因此这里考虑编写对应的脚本，对当前目录下的.json进行批量处理。)</p><h4 id="1-2生成mask文件"><a href="#1-2生成mask文件" class="headerlink" title="1.2生成mask文件"></a>1.2生成mask文件</h4><p>使用第二种方式，步骤如下：<br>1.新建.sh脚本文件</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">touch json2mask.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2.编辑.sh脚本文件<br>将下列内容复制进.sh脚本文件中</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">gedit json2mask.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#！&#x2F;bin&#x2F;bashlet i&#x3D;1path&#x3D;.&#x2F;cd $&#123;path&#125;for file in *.jsondo     labelme_json_to_dataset $&#123;file&#125;     let i&#x3D;i+1 done<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3.执行脚本</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">bash json2mask.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>对.json文件进行转换生成之后，会得到对应名称的文件夹<br>如图所示</p><img src="/blog/blog/2022/01/20/unet-tu-xiang-fen-ge-mo-xing-xiang-guan-zong-jie/1571518-20210827152523534-1020421557.png"><p>查看文件夹，发现存在四个文件：<br><img src="/blog/blog/2022/01/20/unet-tu-xiang-fen-ge-mo-xing-xiang-guan-zong-jie/UNet%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/1571518-20210827152649325-1279716410.png"></p><p>分别为以下：</p><ul><li>img.png，源文件图像</li><li>label.png，标签图像</li><li>label_names.txt，标签中的各个类别的名称</li><li>label_viz.png，源文件与标签融合文件</li></ul><p>其中的label.png即是我们要的想要的标签文件。如果本来的源文件图像为jpg格式，我们会发现生成的png格式源文件图像大小会大很多，不必惊慌。JPG质量不会有变化，但大小通常会增加几倍左右，这是因为JPG是有损压缩，而PNG是无损压缩。</p><h4 id="1-3-转换二值图像并批量整理"><a href="#1-3-转换二值图像并批量整理" class="headerlink" title="1.3 转换二值图像并批量整理"></a>1.3 转换二值图像并批量整理</h4><ul><li><p>得到以上这些结果是不是意味着结束了呢？</p><blockquote><p>事实上，到这里才仅仅完成的一半，我们还需要对label.png图片进行转换为二值图片，最后我们可以遍历文件夹内所有小文件夹，分别对其中的img和转换后的label进行重命名存储到对应的imgs和masks文件目录下，到这一步整个数据集制作才算全部完成。</p></blockquote></li></ul><p>通过执行下面代码可以批量的对各个小文件夹下的图片进行重命名和整理：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">'''@author: linxu@contact: 17746071609@163.com@time: 2021-08-21 上午11:54@desc: 将多通道mask图像批量转换为单通道二值化图像并存放到指定位置'''</span><span class="token keyword">import</span> cv2<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> os<span class="token keyword">import</span> os<span class="token keyword">def</span> <span class="token function">os_mkdir</span><span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 去除首位空格</span>    path <span class="token operator">=</span> path<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># 去除尾部/符号</span>    path <span class="token operator">=</span> path<span class="token punctuation">.</span>rstrip<span class="token punctuation">(</span><span class="token string">"/"</span><span class="token punctuation">)</span>    <span class="token comment"># 判断路径是否存在</span>    isExists <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>path<span class="token punctuation">)</span>    <span class="token comment"># 判断结果</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> isExists<span class="token punctuation">:</span>        <span class="token comment"># 如果不存在则创建目录</span>        <span class="token comment"># 创建目录操作函数</span>        os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span>path<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>path <span class="token operator">+</span> <span class="token string">' 创建成功'</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token boolean">True</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token comment"># 如果目录存在则不创建，并提示目录已存在</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>path <span class="token operator">+</span> <span class="token string">' 目录已存在'</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token boolean">False</span>        <span class="token keyword">def</span> <span class="token function">mask2binimg</span><span class="token punctuation">(</span>path<span class="token punctuation">,</span>show<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> root<span class="token punctuation">,</span> dirs<span class="token punctuation">,</span> files <span class="token keyword">in</span> os<span class="token punctuation">.</span>walk<span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'################################################################'</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> name <span class="token keyword">in</span> files<span class="token punctuation">:</span>            <span class="token comment"># 遍历label生成的&#123;x&#125;_json目录</span>            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>dirs<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                <span class="token comment"># print('root', root)</span>                <span class="token comment"># 字符分割,得到label排序序号</span>                filepath <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>split<span class="token punctuation">(</span>root<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                numname <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>split<span class="token punctuation">(</span>root<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>                n_name <span class="token operator">=</span> numname<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'_json'</span><span class="token punctuation">,</span><span class="token string">''</span><span class="token punctuation">)</span>                      <span class="token comment"># 处理原图img</span>            <span class="token keyword">if</span> name <span class="token operator">==</span> <span class="token string">'img.png'</span><span class="token punctuation">:</span>                fname <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>root<span class="token punctuation">,</span> name<span class="token punctuation">)</span>                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'INFO[img]'</span><span class="token punctuation">,</span> fname<span class="token punctuation">)</span>                img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>fname<span class="token punctuation">)</span>                img_dst <span class="token operator">=</span> cv2<span class="token punctuation">.</span>resize<span class="token punctuation">(</span>img<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">640</span><span class="token punctuation">,</span> <span class="token number">480</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token comment"># img = cv2.resize(img, (0, 0), fx=0.3, fy=0.3, interpolation=cv2.INTER_NEAREST)</span>                <span class="token keyword">if</span> show<span class="token punctuation">:</span>                    cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">'img'</span><span class="token punctuation">,</span> img_dst<span class="token punctuation">)</span>                    cv2<span class="token punctuation">.</span>waitKey<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token comment"># 根据指定路径存取二值化图片</span>                img_path <span class="token operator">=</span> filepath <span class="token operator">+</span> <span class="token string">'/imgs/'</span>                os_mkdir<span class="token punctuation">(</span>img_path<span class="token punctuation">)</span>                cv2<span class="token punctuation">.</span>imwrite<span class="token punctuation">(</span>img_path <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>n_name<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'.png'</span><span class="token punctuation">,</span> img_dst<span class="token punctuation">)</span>            <span class="token comment"># 处理label标签图</span>            <span class="token keyword">if</span> name <span class="token operator">==</span> <span class="token string">'label.png'</span><span class="token punctuation">:</span>                fname <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>root<span class="token punctuation">,</span> name<span class="token punctuation">)</span>                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'INFO[label]'</span><span class="token punctuation">,</span> fname<span class="token punctuation">)</span>                label <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>fname<span class="token punctuation">)</span>                label <span class="token operator">=</span> cv2<span class="token punctuation">.</span>resize<span class="token punctuation">(</span>label<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">640</span><span class="token punctuation">,</span> <span class="token number">480</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                gray <span class="token operator">=</span> cv2<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>label<span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>COLOR_BGR2GRAY<span class="token punctuation">)</span>                retVal<span class="token punctuation">,</span> dst <span class="token operator">=</span> cv2<span class="token punctuation">.</span>threshold<span class="token punctuation">(</span>gray<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>THRESH_OTSU<span class="token punctuation">)</span>                <span class="token comment"># 显示图片</span>                <span class="token keyword">if</span> show<span class="token punctuation">:</span>                    cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">'label'</span><span class="token punctuation">,</span> label<span class="token punctuation">)</span>                    cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">'dst'</span><span class="token punctuation">,</span> dst<span class="token punctuation">)</span>                    <span class="token keyword">if</span> cv2<span class="token punctuation">.</span>waitKey<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token number">0xff</span> <span class="token operator">==</span> <span class="token builtin">ord</span><span class="token punctuation">(</span><span class="token string">"q"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                        <span class="token keyword">break</span>                <span class="token comment"># 根据指定路径存取二值化图片</span>                mask_path <span class="token operator">=</span> filepath <span class="token operator">+</span> <span class="token string">'/masks/'</span>                os_mkdir<span class="token punctuation">(</span>mask_path<span class="token punctuation">)</span>                cv2<span class="token punctuation">.</span>imwrite<span class="token punctuation">(</span>mask_path <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>n_name<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'.png'</span><span class="token punctuation">,</span> dst<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'当前图片转换完成...'</span><span class="token punctuation">)</span><span class="token keyword">pass</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    path <span class="token operator">=</span> <span class="token string">'/home/linxu/下载/flow_dataset/image/'</span>    mask2binimg<span class="token punctuation">(</span>path<span class="token punctuation">,</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>运行结束后，会发现目录下多了两个文件夹，一个是imgs，用来存放原图；另外一个是masks，用来存放二值化标注图像。<br><img src="/blog/blog/2022/01/20/unet-tu-xiang-fen-ge-mo-xing-xiang-guan-zong-jie/UNet%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/1571518-20210827154341850-1236280524.png"><br>文件目录imgs下内容如下图所示：<br><img src="/blog/blog/2022/01/20/unet-tu-xiang-fen-ge-mo-xing-xiang-guan-zong-jie/UNet%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/1571518-20210827154421797-1096449726.png"><br>文件目录masks下内容如下图所示：<br><img src="/blog/blog/2022/01/20/unet-tu-xiang-fen-ge-mo-xing-xiang-guan-zong-jie/UNet%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/1571518-20210827154429235-1144234536.png"></p><p>确认imgs与masks内容无误之后，将这两个文件夹拷贝到UNet模型源码目录下的data路径，如下图所示：<br><img src="/blog/blog/2022/01/20/unet-tu-xiang-fen-ge-mo-xing-xiang-guan-zong-jie/UNet%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/1571518-20210827154819303-1073061395.png"><br>至此，数据集制作完毕并放置到指定训练路径下。</p><h3 id="2-Train训练"><a href="#2-Train训练" class="headerlink" title="2.Train训练"></a>2.Train训练</h3><p>制作完数据集之后，下一步就是对数据集进行训练</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python train.py -h<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="2-1-用法"><a href="#2-1-用法" class="headerlink" title="2.1 用法"></a>2.1 用法</h4><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">train.py [-h] [-e E] [-b [B]] [-l [LR]] [-f LOAD] [-s SCALE] [-v VAL]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在图像和目标掩码上训练 UNet</p><p>可选参数：</p><ul><li>-h , –help 显示此帮助信息并退出</li><li>-e E, –epochs E 时期数（默认值：5）</li><li>-b [B], –batch-size [B]<pre><code>                    批量大小（默认值：1）</code></pre></li><li>-l [LR], –learning-rate [LR]<pre><code>                    学习率（默认：0.1）</code></pre></li><li>-f LOAD, –load LOAD 从 .pth 文件加载模型（默认：False）</li><li>-s SCALE, –scale SCALE<pre><code>                    图像的缩小因子（默认值：0.5）</code></pre></li><li>-v VAL, –validation VAL<pre><code>                    用作验证的数据百分比 (0-100)                    （默认值：10.0）</code></pre></li></ul><p>默认情况下，该<code>scale</code>值为 0.5，因此如果您希望获得更好的结果（但使用更多内存），请将其设置为 1。</p><p>输入图像和目标掩码应分别位于<code>data/imgs</code>和<code>data/masks</code>文件夹中。</p><h4 id="2-2-调用示例"><a href="#2-2-调用示例" class="headerlink" title="2.2 调用示例"></a>2.2 调用示例</h4><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python train.py -e 200 -b 1 -l 0.1 -s 0.5 -v 15.0<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-Predict预测"><a href="#3-Predict预测" class="headerlink" title="3.Predict预测"></a>3.Predict预测</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python predict.py -h<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="3-1-用法"><a href="#3-1-用法" class="headerlink" title="3.1 用法"></a>3.1 用法</h4><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">predict.py [-h] [--model FILE] --input INPUT [INPUT ...]                   [--output INPUT [INPUT ...]] [--viz] [ --no-save]                   [--mask-threshold MASK_THRESHOLD] [--scale SCALE]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>可选参数：-h , –help 显示此帮助消息并退出</p><ul><li><p>–model FILE, -m FILE<br>指定文件在该模型被存储（默认值：MODEL.pth）</p></li><li><p>–input INPUT [INPUT …]，-i INPUT [INPUT …]<br>的输入图像的文件名（默认值：无）</p></li><li><p>–output INPUT [INPUT …], -o INPUT [INPUT …]<br>输出图像的文件名（默认值：无）–<br>viz，-v 在处理图像时可视化（默认值：False）</p></li><li><p>– no -save, -n 不保存输出掩码 (默认: False) </p></li><li><p>–mask-threshold MASK_THRESHOLD, -t MASK_THRESHOLD<br>考虑掩码像素 白色的最小概率值(默认: 0.5) </p></li><li><p>–scale SCALE, -s SCALE 比例因子对于输入图像（默认值：0.5）</p></li></ul><h4 id="3-2-调用示例"><a href="#3-2-调用示例" class="headerlink" title="3.2 调用示例"></a>3.2 调用示例</h4><ul><li>要预测单个图像并保存它：</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python predict.py -i image.jpg -o output.jpg<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>要预测多个图像并显示它们而不保存它们：</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python predict.py -i image1.jpg image2.jpg --viz --no-save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="3-3-融合预览"><a href="#3-3-融合预览" class="headerlink" title="3.3 融合预览"></a>3.3 融合预览</h4><p>为更加直观地感受分割后得到的结果，下面采用图像融合的方式进行预览<br>(说明：其中img1为图像原图，img2为预测的二值图像，image为两者根据一定比例融合之后得到的结果。)</p><p><img src="/blog/blog/2022/01/20/unet-tu-xiang-fen-ge-mo-xing-xiang-guan-zong-jie/UNet%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/1571518-20210828085642273-1274153587.png"></p><ul><li>下面一并附上图像融合代码</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> cv2              <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np                              src <span class="token operator">=</span> <span class="token string">"/home/linxu/下载/flow_dataset/image/30.jpg"</span>mask <span class="token operator">=</span> <span class="token string">"/home/linxu/下载/flow_dataset/output.png"</span>                        <span class="token comment"># 使用opencv叠加图片          </span>img1 <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>src<span class="token punctuation">)</span>  img2 <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>mask<span class="token punctuation">)</span>                         alpha <span class="token operator">=</span> <span class="token number">1</span>               meta <span class="token operator">=</span> <span class="token number">0.4</span>              gamma <span class="token operator">=</span> <span class="token number">0</span>               cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">'img1'</span><span class="token punctuation">,</span> img1<span class="token punctuation">)</span>cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">'img2'</span><span class="token punctuation">,</span> img2<span class="token punctuation">)</span> image <span class="token operator">=</span> cv2<span class="token punctuation">.</span>addWeighted<span class="token punctuation">(</span>img1<span class="token punctuation">,</span>alpha<span class="token punctuation">,</span>img2<span class="token punctuation">,</span>meta<span class="token punctuation">,</span>gamma<span class="token punctuation">)</span>                       cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">'image'</span><span class="token punctuation">,</span> image<span class="token punctuation">)</span>                                               cv2<span class="token punctuation">.</span>waitKey<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>                                                          <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>参考：<a href="https://blog.csdn.net/jcfszxc/article/details/106289555">https://blog.csdn.net/jcfszxc/article/details/106289555</a></p>]]></content>
      
      
      <categories>
          
          <category> Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 炼丹术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YOLOv5目标检测学习总结</title>
      <link href="/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/"/>
      <url>/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="Yolov5目标检测训练模型学习总结"><a href="#Yolov5目标检测训练模型学习总结" class="headerlink" title="Yolov5目标检测训练模型学习总结"></a>Yolov5目标检测训练模型学习总结</h1><h2 id="一、YOLOv5介绍"><a href="#一、YOLOv5介绍" class="headerlink" title="一、YOLOv5介绍"></a>一、YOLOv5介绍</h2><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/1571518-20210821230430746-1580738715.png" style="zoom:67%;"><p>YOLOv5是一系列在 COCO 数据集上预训练的对象检测架构和模型，代表<a href="https://ultralytics.com/">Ultralytics</a> 对未来视觉 AI 方法的开源研究，结合了在数千小时的研究和开发中获得的经验教训和最佳实践。</p><p>下面是YOLOv5的具体表现：</p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210821232820583-1349441236.png"></p><p>我们可以看到上面图像中，除了灰色折线为EfficientDet模型，剩余的四种都是YOLOv5系列的不同网络模型。<br>其中5s是最小的网络模型，5x是最大的网络模型，而5m与5l则介于两者之间。<br>相应地，5s的精度小模型小易于移植，而5x的精度高模型大比较臃肿。</p><h3 id="1-1-Pretrained-Checkpoints-预检查点"><a href="#1-1-Pretrained-Checkpoints-预检查点" class="headerlink" title="1.1 Pretrained Checkpoints 预检查点"></a>1.1 Pretrained Checkpoints 预检查点</h3><table><thead><tr><th>Model</th><th>size(pixels)</th><th>$mAP^{val}$0.5:0.95</th><th>$mAP^{test}$</th><th>$mAP^{val}$0.5</th><th>Speed v100</th><th>params(M)</th><th>FLOPs  640(B)</th></tr></thead><tbody><tr><td>[YOLOv5s][assets]</td><td>640</td><td>36.7</td><td>36.7</td><td>55.4</td><td><strong>2.0</strong></td><td>7.3</td><td>17.0</td></tr><tr><td>[YOLOv5m][assets]</td><td>640</td><td>44.5</td><td>44.5</td><td>63.1</td><td>2.7</td><td>21.4</td><td>51.3</td></tr><tr><td>[YOLOv5l][assets]</td><td>640</td><td>48.2</td><td>48.2</td><td>66.9</td><td>3.8</td><td>47.0</td><td>115.4</td></tr><tr><td>[YOLOv5x][assets]</td><td>640</td><td><strong>50.4</strong></td><td><strong>50.4</strong></td><td><strong>68.8</strong></td><td>6.1</td><td>87.7</td><td>218.8</td></tr><tr><td>[YOLOv5s6][assets]</td><td>1280</td><td>43.3</td><td>43.3</td><td>61.9</td><td><strong>4.3</strong></td><td>12.7</td><td>17.4</td></tr><tr><td>[YOLOv5m6][assets]</td><td>1280</td><td>50.5</td><td>50.5</td><td>68.7</td><td>8.4</td><td>35.9</td><td>52.4</td></tr><tr><td>[YOLOv5l6][assets]</td><td>1280</td><td>53.4</td><td>53.4</td><td>71.1</td><td>12.3</td><td>77.2</td><td>117.7</td></tr><tr><td>[YOLOv5x6][assets]</td><td>1280</td><td><strong>54.4</strong></td><td><strong>54.4</strong></td><td><strong>72.0</strong></td><td>22.4</td><td>141.8</td><td>222.9</td></tr><tr><td>[YOLOv5x6][assets] TTA</td><td>1280</td><td><strong>55.0</strong></td><td><strong>55.0</strong></td><td><strong>72.0</strong></td><td>70.8</td><td></td><td></td></tr></tbody></table><ul><li>YOLOv5训练与预测技巧</li></ul><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210831150210855-1681831343.png"></p><ul><li>YOLOv5功能增加</li></ul><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210831150234738-1904360579.png"></p><ul><li>课程内容</li></ul><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210831151209117-2013386745.png"></p><ul><li>YOLOv5网络模型图</li></ul><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210831151334768-1342579777.png"></p><h2 id="二、目标检测相关基础"><a href="#二、目标检测相关基础" class="headerlink" title="二、目标检测相关基础"></a>二、目标检测相关基础</h2><h3 id="2-1-目标检测任务理解与总结"><a href="#2-1-目标检测任务理解与总结" class="headerlink" title="2.1 目标检测任务理解与总结"></a>2.1 目标检测任务理解与总结</h3><p><a href="https://www.cnblogs.com/isLinXu/p/15893539.html">目标检测任务理解与总结</a></p><h3 id="2-2-目标检测之常用数据集"><a href="#2-2-目标检测之常用数据集" class="headerlink" title="2.2 目标检测之常用数据集"></a>2.2 目标检测之常用数据集</h3><p><a href="https://www.cnblogs.com/isLinXu/p/15893506.html">目标检测之常用数据集</a></p><h3 id="2-3-目标检测之性能指标"><a href="#2-3-目标检测之性能指标" class="headerlink" title="2.3 目标检测之性能指标"></a>2.3 目标检测之性能指标</h3><p><a href="https://www.cnblogs.com/isLinXu/p/15893489.html">目标检测之性能指标</a></p><hr><h2 id="三、YOLO目标检测系列发展史"><a href="#三、YOLO目标检测系列发展史" class="headerlink" title="三、YOLO目标检测系列发展史"></a>三、YOLO目标检测系列发展史</h2><h3 id="3-1-目标检测的里程碑"><a href="#3-1-目标检测的里程碑" class="headerlink" title="3.1 目标检测的里程碑"></a>3.1 目标检测的里程碑</h3><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210831171236932-486340528.png"></p><p>从整个时间轴上我们可以看到，在2012年之前，目标检测的主要算法还是建立在传统视觉方式之上的，AI也不曾像今日这般火热，这里将这段时期称为“冷兵器的时代”。而在2012年之后，就开始了j基于深度学习+卷积网络的方式尝试与探索，这里又根据识别阶段分为两类，一类是以YOLO、SSD等为代表的单阶段检测器，另一类是以Faster RCNN等为代表的双阶段检测器。</p><p>从原理上区分，我们又将双阶段检测器归为基于候选框的方法，而将单阶段检测器称为不基于候选框的方法。<br>如下图：</p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210831172456754-947156285.png"></p><p>我们可以看到，两种不同方式的检测方法都在近些年得到了不断的发展，并且相互促进和进步。<br>这里重点关注YOLO系列的发展，最近的版本即为YOLOv4与YOLOv5，两者都是于2020年同期发布，并且较前几个版本效果优化差异明显。</p><h3 id="3-2-Darknet简要介绍"><a href="#3-2-Darknet简要介绍" class="headerlink" title="3.2 Darknet简要介绍"></a>3.2 <strong>Darknet</strong>简要介绍</h3><hr><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210831172920472-1915634673.png"></p><p>追根溯源，YOLO系列均是基于darknet这个框架来进行开发的。<br>Darknet is an open source neural network framework written in C and CUDA. It is fast, easy to install, and supports CPU and GPU computation.<br>Darknet是一个用C和CUDA编写的开源神经网络框架。它是快速，易于安装，并支持CPU和GPU计算。</p><p>Yolov4 paper：<a href="https://arxiv.org/abs/2004.10934">https://arxiv.org/abs/2004.10934</a><br>Yolov4 sourcecode：<a href="https://github.com/alexeyab/darknet">https://github.com/alexeyab/darknet</a> </p><h3 id="3-3-YOLO-算法基本思想"><a href="#3-3-YOLO-算法基本思想" class="headerlink" title="3.3 YOLO 算法基本思想"></a>3.3 YOLO 算法基本思想</h3><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210831173309026-1551074729.png"></p><p>YOLO的基本思想是，将一副图像进行细化的网格划分，在划分的网格基础上进行边界框的预测，同时计算出目标的置信度以及类别的概率图，综合两者来进行最终的检测。</p><p>下面接着来看，假设我们要检测的对象还是那只狗，可以看到在黄色边界框中有不断迭代移动的红色网格，这个红色网格就负责处理对象的检测，在迭代过程中，红色网格被预测特征图(Prediction Feature Map)处理，其中这个预测特征图分为三个Box(不同尺度的边界框)，可用于计算不同类型的单元——(Box Co-ordinates 边界框坐标信息)、(Objectness Score置信度分数)、(Class Scores分类得分)</p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210831174655569-106936744.png"></p><p>具体地来说，下面再来看一个过程。<br>假设我们要处理的图像为下图,尺寸大小为(608×608)，要检测的对象是图中这辆车，经过YOLO的Deep CNN(DCNN,深度卷积神经网络)之后，会对其进行32倍的下采样，从而可得到一个(19×19)的网格图，对应的每个小格子分别计算出其相应的坐标信息、目标性得分以及每个类别的分类概率，同时，还可以通过Box得到三个不同尺度的边界框的数据。</p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210831175456180-1206286368.png"></p><h3 id="3-4-YOLO-v3-YOLO-v4算法的基本思想"><a href="#3-4-YOLO-v3-YOLO-v4算法的基本思想" class="headerlink" title="3.4 YOLO v3/YOLO v4算法的基本思想"></a>3.4 YOLO v3/YOLO v4算法的基本思想</h3><ul><li>首先，通过特征提取网络对输入图像提取特征，得到一定大小的特征图，比如19×19(相当于608×608图片大小)，然后将输入图像分成19×19个grid cells网格单元，如果GT(Ground Truth标定边界框)中某个目标的中心坐标落在哪一个Grid cell中，那么就由该grid cell来预测该目标。</li><li>预测得到的输出特征图有两个维度是提取到的特征的维度，比如19×19，还有一个维度(深度)是B×(5+C)。<br>其中<strong>B</strong>表示每个grid cell预测的边界框的数量(YOLO v3/v4中是3个，即B=3)；<br><strong>C</strong>表示边界框的类别数(没有背景类，所以对于VOC数据集是20)；<br><strong>5</strong>表示4个坐标信息和一个目标性得分。</li></ul><p><strong>类别预测(Class Prediction)</strong></p><ul><li><p>大多数分类器假设输出标签是互斥的。如果输出是互斥的目标类别，则确实如此。因此，YOLO应用softmax函数将得分转换为总和为1的概率。而YOLOv3/v4使用多标签分类。例如，输出标签可以是“行人”和“儿童”，它们不是非排他性的。(现在输出的总和可以大于1)。</p></li><li><p>YOLOv3/v4用多个独立的逻辑(logistic)分类器替换softmax函数，以计算输出属于特定标签的可能性。在计算分类损失时，YOLOv3/v4对每个标签使用<strong>二元交叉熵损失</strong>。这也避免使用softmax函数而降低了计算复杂度。</p></li></ul><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210901090412496-1511491229.png"></p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210901090553320-1946483796.png"></p><h3 id="3-5-YOLOv3网络架构"><a href="#3-5-YOLOv3网络架构" class="headerlink" title="3.5 YOLOv3网络架构"></a>3.5 YOLOv3网络架构</h3><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210901092800057-820659352.png"></p><p>YOLOv3的主干网络是Darknet53，经过采样与卷积等计算，传到头部进行处理。</p><h3 id="3-6-YOLOv4网络架构"><a href="#3-6-YOLOv4网络架构" class="headerlink" title="3.6 YOLOv4网络架构"></a>3.6 YOLOv4网络架构</h3><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210901092957060-345572692.png"></p><p>YOLOv4的主干网络是CSP组件+Darknet53，网络传播方式使用SPP+PANet，最终传输到YOLO Head。</p><h2 id="四、YOLOv5网络架构及组件及Loss函数"><a href="#四、YOLOv5网络架构及组件及Loss函数" class="headerlink" title="四、YOLOv5网络架构及组件及Loss函数"></a>四、YOLOv5网络架构及组件及Loss函数</h2><p>YOLO系列属于单阶段目标探测器，与RCNN不同，它没有单独的区域建议网络（RPN），并且依赖于不同尺度的锚框。</p><p><strong>架构</strong>可分为三个部分：骨架、颈部和头部。利用CSP（Cross-Stage Partial Networks）作为主干，从输入图像中提取特征。PANet被用作收集特征金字塔的主干，头部是最终的检测层，它使用特征上的锚框来检测对象。</p><p>YOLO架构使用的<strong>激活函数</strong>是Google Brains在2017年提出的Swish的变体，它看起来与ReLU非常相同，但与ReLU不同，它在x=0附近是平滑的。</p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210901094857223-869940034.png"></p><p>通过上图我们可以看到，<br>YOLOv5的Backbone骨干网络为{VGG16、ResNet-50、ResNet-101、DarkNet53，…};<br>颈部网络Backbone为{FPN、PANet、Bi-FPN，…};<br>头部网络Head为:<br>     Dense Prediction：{RPN、YOLO、SSD、RetinaNet、FCOS、…}<br>     Sparse Prediction：{Faster R-CNN、R-FCN、…}</p><p>YOLOv5包括：</p><ul><li>Backbone：Focus、BottleneckCSP、SPP</li><li>Head：PANet+Detect(YOLOv3/v4 Head)</li></ul><h3 id="4-1-网络可视化工具：Netron"><a href="#4-1-网络可视化工具：Netron" class="headerlink" title="4.1 网络可视化工具：Netron"></a>4.1 网络可视化工具：Netron</h3><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210901102430722-182698806.png"></p><p>在线版本链接：<a href="https://lutzroeder.github.io/netron/">https://lutzroeder.github.io/netron/</a><br>netron官方的Github链接：<a href="https://github.com/lutzroeder/netron">https://github.com/lutzroeder/netron</a></p><p>netron对pt格式的权重文件兼容性不好，直接使用netron工具打开，无法现实整个网络。<br>可使用YOLOv5代码中models/export.py脚本将pt权重文件转换为onnx格式，再用netron工具打开，就可以看YOLOv5的整体架构。</p><p>导出ONNX文件</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">pip install onnx&gt;&#x3D;1.7.0 -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple #for ONNX export <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">pip install coremltools&#x3D;&#x3D;4.0-i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple #for Coreml export <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python models.export.py --weights weights.yolov5s.pt --img 640 --batch 1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>预览YOLOv5在netron中的网络结构图：<a href="https://img2020.cnblogs.com/blog/1571518/202109/1571518-20210901105121131-1343162288.png">链接</a>.</p><p>借助于上面的Netron工具得到的网络结构图，我们可以画出下面这样的网络架构图，对此进行做一个全局把握。</p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210831151334768-1342579777.png"></p><p>其中Conv表示由Conv(卷积)、BN(Batch Normalization归一化)和Hard-swish(非线性激活)三个操作组合成的运算。<br>Bottleneck为瓶颈部分运算操作，若Bottleneck为True时，本操作为两个Conv操作相加得到的结果，若Bottleneck为False，则操作为两个Conv卷积。<br>同时，BCSPn为Conv-&gt;Bottleneck-&gt;conv与conv拼接，再进行BN(Batch Normalization)和后续的Relu(非饱和激活函数)卷积，防止“梯度消失”，加快收敛速度。<br>而Focus操作，需要对其进行四个Slice分片，再进行重新拼接后卷积。<br>SPP操作前后都需要conv，中间有三种不同尺度大小的层级进行选择处理。</p><p>图片从输入层开始传入模型，经过复杂的主干网络，最终流向Head头部网络的DarkNet去做处理。</p><h3 id="4-2-灵活配置不同复杂度的模型"><a href="#4-2-灵活配置不同复杂度的模型" class="headerlink" title="4.2 灵活配置不同复杂度的模型"></a>4.2 灵活配置不同复杂度的模型</h3><p>(应用类似EfficientNet的channel和layer控制因子)</p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210901112212002-1053152801.png"></p><p>YOLOv5的四种网络结构是depth_multiple和width_multiple两个参数，来进行控制网络的深度和宽度。其中，depth_multiple控制网络的深度(BottleneckCSP数)，width_multiple控制网络的宽度(卷积核数量)。</p><h3 id="4-3-Focus机制"><a href="#4-3-Focus机制" class="headerlink" title="4.3 Focus机制"></a>4.3 Focus机制</h3><p>把数据切分为4份，每份数据都是相当于2倍下采样得到的，然后在channel维度进行拼接，最后进行卷积操作。</p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210901113844135-2037854205.png"></p><p>这里可以做一个小实验：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 新增一个tensor x</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">,</span><span class="token number">12</span><span class="token punctuation">,</span><span class="token number">13</span><span class="token punctuation">,</span><span class="token number">14</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">21</span><span class="token punctuation">,</span><span class="token number">22</span><span class="token punctuation">,</span><span class="token number">23</span><span class="token punctuation">,</span><span class="token number">24</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">31</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">33</span><span class="token punctuation">,</span><span class="token number">34</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">41</span><span class="token punctuation">,</span><span class="token number">42</span><span class="token punctuation">,</span><span class="token number">43</span><span class="token punctuation">,</span><span class="token number">44</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>拆分后，得到四份数据</p><pre class="line-numbers language-none"><code class="language-none">tensor([[[[11,13],[31,33]],[[21,23],[41,43]],[[12,14],[32,34]],[[22,24],[42,44]]]])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p> Focus() module模块被设计是用来减少FLOPS和增加速度，但不增加mAP。<br>在YOLOV5中，作者希望降低二维卷积Conv2d计算的成本，并实际使用张量reshape来减少空间（分辨率）和增加深度（通道数）</p><p>输入将按如下方式转换：[b，c，h，w]-&gt;[b，c*4，h//2，w//2]</p><p>以YOLOv5s的结构为例，原始640×640×3的图像输入Focus结构，采用切片结构，先变成320×320×12的特征图，再经过一次32个卷积核的卷积操作，最终变成320×320×32的特征图。<br>而YOLOv5m的Focus结构中的卷积操作使用了48个卷积核，因此Focus结构后的特征图变成320×320×48。YOLOv5l，YOLOv5x也是同样的道理。</p><h3 id="4-4-SPP-Spatial-Pyramid-Pooling-空间金字塔池化"><a href="#4-4-SPP-Spatial-Pyramid-Pooling-空间金字塔池化" class="headerlink" title="4.4 SPP(Spatial Pyramid Pooling)空间金字塔池化"></a>4.4 SPP(Spatial Pyramid Pooling)空间金字塔池化</h3><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210901140029868-1777328369.png"></p><p>在CSP上添加SPP块，因为它显著增加了感受野，分离出最重要的上下文特征，并且几乎不会降低网络运行速度</p><h3 id="4-5-Hard-Swish激活函数"><a href="#4-5-Hard-Swish激活函数" class="headerlink" title="4.5 Hard Swish激活函数"></a>4.5 Hard Swish激活函数</h3><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210901142241145-517792325.png"><br><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210901142255502-1857305914.png"></p><h3 id="4-6-PANet-Path-Aggregation-Network-路径聚合网络"><a href="#4-6-PANet-Path-Aggregation-Network-路径聚合网络" class="headerlink" title="4.6 PANet(Path-Aggregation Network)路径聚合网络"></a>4.6 PANet(Path-Aggregation Network)路径聚合网络</h3><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210901140511312-2101957633.png"></p><p>对上图进行以下说明：(注意，为了简洁起见，我们省略了(a)和(b)中特征映射的通道维度)<br>a) FPN backbone FPN主干网络<br>b) Bottom-up path augmentation 自底向上路径扩充<br>c) Adaptive feature pooling 自适应特征池<br>d) Box branch Box分支<br>e) Fully-connected fusion 全连通结合</p><h3 id="4-7-YOLOv5损失函数"><a href="#4-7-YOLOv5损失函数" class="headerlink" title="4.7 YOLOv5损失函数"></a>4.7 YOLOv5损失函数</h3><p>YOLOv5损失函数包括：</p><ul><li>classification loss，分类损失</li><li>localization loss，定位损失(预测框与GT(Ground Truth)框之间的误差)</li><li>confidence loss，置信度损失(框的目标性；objectness of the box)</li></ul><p>总的损失函数：<br>classification loss + localization loss + confidence loss</p><blockquote><p>YOLOv5使用二元交叉熵损失函数计算类别概率和目标置信度得分的损失。<br>YOLOv5使用C-LoU Loss作为bounding box回归的损失。</p></blockquote><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210901142423034-717276604.png"></p><ul><li><p>针对包围盒回归提出了一种距离lou损失，即Dlou损失，它比lou损失和Glou损失具有更快的收敛速度。</p></li><li><p>通过考虑重叠面积、中心点距离和纵横比这三个几何度量，进一步提出了完整的lou损失，即Clou损失，它更好地描述了矩形盒的回归。</p></li></ul><h2 id="五、YOLOv5-实战训练自己的数据集"><a href="#五、YOLOv5-实战训练自己的数据集" class="headerlink" title="五、YOLOv5 实战训练自己的数据集"></a>五、YOLOv5 实战训练自己的数据集</h2><h3 id="5-1-软件安装及环境配置"><a href="#5-1-软件安装及环境配置" class="headerlink" title="5.1 软件安装及环境配置"></a>5.1 软件安装及环境配置</h3><h4 id="5-1-1-安装Anaconda"><a href="#5-1-1-安装Anaconda" class="headerlink" title="5.1.1 安装Anaconda"></a>5.1.1 安装Anaconda</h4><p>Anaconda是一个用于科学计算的Python发行版，支持Linux、Mac、Windows，包含了众多流行的科学计算、数据分析的Python包。<br>安装步骤:<br>1.先去官方地址下载好对应的安装包<br>下载地址：<a href="https://www.anaconda.com/download/#linux">https://www.anaconda.com/download/#linux</a></p><p>2.然后，安装anaconda</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">bash Anaconda3-2021.05-Linux-x86_64.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>anaconda会自动将环境变量添加到PATH里面，如果后面你发现输入conda提示没有该命令，那么<br>你需要执行命令 <code>source ~/.bashrc</code> 更新环境变量，就可以正常使用了。<br>如果发现这样还是没用，那么需要添加环境变量。<br>编辑<code>~/.bashrc</code> 文件，在最后面加上</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">export PATH&#x3D;&#x2F;home&#x2F;user&#x2F;anaconda3&#x2F;bin:$PATH<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意：路径应改为自己机器上的路径<br>保存退出后执行： <code>source ~/.bashrc</code><br>再次输入 <code>conda list</code> 测试看看，应该没有问题。</p><h4 id="5-1-2-安装Anaconda国内镜像配置"><a href="#5-1-2-安装Anaconda国内镜像配置" class="headerlink" title="5.1.2 安装Anaconda国内镜像配置"></a>5.1.2 安装Anaconda国内镜像配置</h4><p>清华TUNA提供了 Anaconda 仓库的镜像，运行以下三个命令:</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda config --set show_channel_urls yes<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="5-1-3-安装pytorch"><a href="#5-1-3-安装pytorch" class="headerlink" title="5.1.3 安装pytorch"></a>5.1.3 安装pytorch</h4><p>更新：如果使用yolov5版本v5.0以上的代码，使用pytorch1.8<br>首先为pytorch创建一个anaconda虚拟环境，环境名字可自己确定，这里本人使用pytrain作为环境<br>名:</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda create -n pytrain python&#x3D;3.8<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>安装成功后激活pytrain环境：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda activate pytrain<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在所创建的pytorch环境下安装pytorch的1.8版本, 执行命令：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda install pytorch torchvision cudatoolkit&#x3D;10.2 -c pytorch<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意：10.2处应为cuda的安装版本号<br>编辑<code>~/.bashrc</code> 文件，设置使用pytrain环境下的python3.8</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">alias python&#x3D;&#39;&#x2F;home&#x2F;bai&#x2F;anaconda3&#x2F;envs&#x2F;pytrain&#x2F;bin&#x2F;python3.8&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意：python路径应改为自己机器上的路径<br>保存退出后执行： <code>source ~/.bashrc</code><br>该命令将自动回到base环境，再执行 <code>conda activate pytrain</code> 到pytorch环境。</p><h3 id="5-2-YOLOv5项目克隆和安装"><a href="#5-2-YOLOv5项目克隆和安装" class="headerlink" title="5.2 YOLOv5项目克隆和安装"></a>5.2 YOLOv5项目克隆和安装</h3><h4 id="5-2-1-克隆YOLOv5项目"><a href="#5-2-1-克隆YOLOv5项目" class="headerlink" title="5.2.1 克隆YOLOv5项目"></a>5.2.1 克隆YOLOv5项目</h4><p>网址: <a href="https://github.com/ultralytics/yolov5">https://github.com/ultralytics/yolov5</a></p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git clone https:&#x2F;&#x2F;github.com&#x2F;ultralytics&#x2F;yolov5.git<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>或者直接下载YOLOv5的5.0版本的代码。<br>下载后可重命名项目文件夹</p><h4 id="5-2-2-安装所需库"><a href="#5-2-2-安装所需库" class="headerlink" title="5.2.2 安装所需库"></a>5.2.2 安装所需库</h4><p>在yolov5目录下执行：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">pip install -r requirements.txt -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意：simple 不能少, 是 https 而不是 http</p><h4 id="5-2-3-下载预训练权重文件"><a href="#5-2-3-下载预训练权重文件" class="headerlink" title="5.2.3 下载预训练权重文件"></a>5.2.3 下载预训练权重文件</h4><p>下载yolov5s.pt，yolov5m.pt，yolov5l.pt，yolov5x.pt权重文件，并放置在weights文件夹下<br>百度网盘下载链接：<br>链接：<a href="https://pan.baidu.com/s/1p1HS0gpWZy55dShj3ihLRQ">https://pan.baidu.com/s/1p1HS0gpWZy55dShj3ihLRQ</a><br>提取码：0sao<br>更新：如果使用yolov5版本v5.0以上的代码，下载相应的权重。</p><p>7.2.4 安装测试</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python detect.py --source .&#x2F;data&#x2F;images&#x2F; --weights weights&#x2F;yolov5s.pt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="5-3-标注自己的数据集"><a href="#5-3-标注自己的数据集" class="headerlink" title="5.3 标注自己的数据集"></a>5.3 标注自己的数据集</h3><h4 id="5-3-1-安装图像标注工具labelImg"><a href="#5-3-1-安装图像标注工具labelImg" class="headerlink" title="5.3.1 安装图像标注工具labelImg"></a>5.3.1 安装图像标注工具labelImg</h4><p>这里的安装大致有两种方式，一种是直接pip install，另一种是克隆源码进行编译，推荐优先使用第一种。<br>第一种安装方式：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda create -n label python&#x3D;3.8<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda activate label<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">pip install labelImg<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>打开方式，在label环境下命令行输入labelImg打开即可。</p><p>第二种安装方式：<br>克隆labelImg</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git clone https:&#x2F;&#x2F;github.com&#x2F;tzutalin&#x2F;labelImg.git<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>使用Anaconda安装<br>到labelImg路径下执行命令</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda install pyqt&#x3D;5pip install lxmlpyrcc5 -o libs&#x2F;resources.py resources.qrcpython labelImg.py<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="5-3-2-添加自定义类别"><a href="#5-3-2-添加自定义类别" class="headerlink" title="5.3.2 添加自定义类别"></a>5.3.2 添加自定义类别</h4><p>修改文件<code>labelImg/data/predefined_classes.txt</code>：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">ballmessitrophy<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="5-3-3-使用labelImg进行图像标注"><a href="#5-3-3-使用labelImg进行图像标注" class="headerlink" title="5.3.3 使用labelImg进行图像标注"></a>5.3.3 使用labelImg进行图像标注</h4><p>打开labelImg，可以看到如下界面：</p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210902091413384-990933187.png"></p><p>说明：由于这里是直接pip install安装，且系统语言又是中文，因此界面语言也保持一致。<br>最左边一栏相当于是标注的操作栏，我们通过创建区块，也即为标注边界框并界定分类标签名。<br>对一张样本图片操作完毕后，Ctrl+S进行保存，这时会采用默认的<code>PascalVOC</code>的格式，也即将标记信息存在同名的xml文件。</p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210902093323461-739044492.png"></p><p>需要说明的是，对上图来说，图像的坐标原点在左上角，水平方向为X轴，竖直方向为Y轴。图中边界框由几个要素组成，即x_min,y_min,x_max,y_max,这些通过xml的结构存储为标注信息文件。</p><p>查看xml对应的格式，如下</p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210902092301419-449327492.png"></p><p>但由于YOLO系列对于标注文件的要求格式为YOLO类型的，因此还需要将其转换为对应的txt格式。</p><blockquote><p>YOLO格式的txt标记文件如下：<br>class_id x y w h<br>class_id: 类别的id编号<br>x: 目标的中心点x坐标（横向）/图片总宽度<br>y: 目标的中心的y坐标（纵向）/图片总高度<br>w: 目标框的宽带/图片总宽度<br>h: 目标框的高度/图片总高度</p></blockquote><p>如下图所示:<br><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210902093035961-1685849400.png"></p><ul><li>可以用python代码实现两种标记格式的转换：</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">convert</span><span class="token punctuation">(</span>size<span class="token punctuation">,</span> box<span class="token punctuation">)</span><span class="token punctuation">:</span>    dw <span class="token operator">=</span> <span class="token number">1.</span><span class="token operator">/</span>size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    dh <span class="token operator">=</span> <span class="token number">1.</span><span class="token operator">/</span>size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token comment"># box[0]：xmin,box[1]: xmax</span>    x <span class="token operator">=</span> <span class="token punctuation">(</span>box<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> box<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2.0</span><span class="token comment"># box[2]: ymin,box[3]: ymax</span>    y <span class="token operator">=</span> <span class="token punctuation">(</span>box<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> box<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2.0</span>    w <span class="token operator">=</span> box<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> box<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    h <span class="token operator">=</span> box<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">-</span> box<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>    x <span class="token operator">=</span> x<span class="token operator">*</span>dw    w <span class="token operator">=</span> w<span class="token operator">*</span>dw    y <span class="token operator">=</span> y<span class="token operator">*</span>dh    h <span class="token operator">=</span> h<span class="token operator">*</span>dh    <span class="token keyword">return</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">,</span>w<span class="token punctuation">,</span>h<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>当然，也可以在操作栏中直接更换保存为YOLO格式的txt文件，这样就可以直接使用，无需转换。</p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210902092907887-529087960.png"></p><h3 id="5-4-准备自己的数据集"><a href="#5-4-准备自己的数据集" class="headerlink" title="5.4 准备自己的数据集"></a>5.4 准备自己的数据集</h3><h4 id="5-4-1-下载项目文件"><a href="#5-4-1-下载项目文件" class="headerlink" title="5.4.1 下载项目文件"></a>5.4.1 下载项目文件</h4><p>从百度文件下载到yolov5目录下并解压</p><ul><li>VOCdevkit_ball.zip</li><li>testfiles.zip</li><li>prepare_data.py</li></ul><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210902095205302-253222246.png"></p><h4 id="5-4-2-解压建立或自行建立数据集"><a href="#5-4-2-解压建立或自行建立数据集" class="headerlink" title="5.4.2 解压建立或自行建立数据集"></a>5.4.2 解压建立或自行建立数据集</h4><p>使用PASCAL VOC数据集的目录结构：<br>建立文件夹层次为<code>yolov5/data/VOCdevkit/VOC2007</code><br>VOC2007下建立两个文件夹：<code>Annotations</code>和<code>JPEGImages</code><br><code>JPEGImages</code>放所有的训练和测试图片；<br><code>Annotations</code>放所有的xml标记文件。</p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210902104033909-1318379829.png"></p><h4 id="5-4-3-生成训练集和验证集文件"><a href="#5-4-3-生成训练集和验证集文件" class="headerlink" title="5.4.3 生成训练集和验证集文件"></a>5.4.3 生成训练集和验证集文件</h4><p>新建Python脚本</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">touch prepare_data.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">gedit prepare_data.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>prepare_data.py </li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> xml<span class="token punctuation">.</span>etree<span class="token punctuation">.</span>ElementTree <span class="token keyword">as</span> ET<span class="token keyword">import</span> pickle<span class="token keyword">import</span> os<span class="token keyword">from</span> os <span class="token keyword">import</span> listdir<span class="token punctuation">,</span> getcwd<span class="token keyword">from</span> os<span class="token punctuation">.</span>path <span class="token keyword">import</span> join<span class="token keyword">import</span> random<span class="token keyword">from</span> shutil <span class="token keyword">import</span> copyfile<span class="token comment"># 分类类别</span>classes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"ball"</span><span class="token punctuation">,</span> <span class="token string">"messi"</span><span class="token punctuation">]</span><span class="token comment"># classes=["ball"]</span><span class="token comment"># 划分训练集比率</span>TRAIN_RATIO <span class="token operator">=</span> <span class="token number">80</span><span class="token keyword">def</span> <span class="token function">clear_hidden_files</span><span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    清除目录下隐藏文件    :param path:    :return:    '''</span>    dir_list <span class="token operator">=</span> os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span>path<span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> dir_list<span class="token punctuation">:</span>        abspath <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>abspath<span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token punctuation">,</span> i<span class="token punctuation">)</span>        <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isfile<span class="token punctuation">(</span>abspath<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> i<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">"._"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                os<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>abspath<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            clear_hidden_files<span class="token punctuation">(</span>abspath<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">convert</span><span class="token punctuation">(</span>size<span class="token punctuation">,</span> box<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    转换格式    :param size:    :param box:    :return:    '''</span>    dw <span class="token operator">=</span> <span class="token number">1.</span> <span class="token operator">/</span> size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    dh <span class="token operator">=</span> <span class="token number">1.</span> <span class="token operator">/</span> size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    x <span class="token operator">=</span> <span class="token punctuation">(</span>box<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> box<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2.0</span>    y <span class="token operator">=</span> <span class="token punctuation">(</span>box<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> box<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2.0</span>    w <span class="token operator">=</span> box<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> box<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    h <span class="token operator">=</span> box<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">-</span> box<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>    x <span class="token operator">=</span> x <span class="token operator">*</span> dw    w <span class="token operator">=</span> w <span class="token operator">*</span> dw    y <span class="token operator">=</span> y <span class="token operator">*</span> dh    h <span class="token operator">=</span> h <span class="token operator">*</span> dh    <span class="token keyword">return</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> w<span class="token punctuation">,</span> h<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">convert_annotation</span><span class="token punctuation">(</span>image_id<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    转换annotation    :param image_id:    :return:    '''</span>    in_file <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'data/VOCdevkit/VOC2007/Annotations/%s.xml'</span> <span class="token operator">%</span> image_id<span class="token punctuation">)</span>    out_file <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'data/VOCdevkit/VOC2007/YOLOLabels/%s.txt'</span> <span class="token operator">%</span> image_id<span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>    tree <span class="token operator">=</span> ET<span class="token punctuation">.</span>parse<span class="token punctuation">(</span>in_file<span class="token punctuation">)</span>    root <span class="token operator">=</span> tree<span class="token punctuation">.</span>getroot<span class="token punctuation">(</span><span class="token punctuation">)</span>    size <span class="token operator">=</span> root<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'size'</span><span class="token punctuation">)</span>    w <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>size<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'width'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span>    h <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>size<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'height'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span>    <span class="token keyword">for</span> obj <span class="token keyword">in</span> root<span class="token punctuation">.</span><span class="token builtin">iter</span><span class="token punctuation">(</span><span class="token string">'object'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        difficult <span class="token operator">=</span> obj<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'difficult'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text        cls <span class="token operator">=</span> obj<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'name'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text        <span class="token keyword">if</span> cls <span class="token keyword">not</span> <span class="token keyword">in</span> classes <span class="token keyword">or</span> <span class="token builtin">int</span><span class="token punctuation">(</span>difficult<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>            <span class="token keyword">continue</span>        cls_id <span class="token operator">=</span> classes<span class="token punctuation">.</span>index<span class="token punctuation">(</span>cls<span class="token punctuation">)</span>        xmlbox <span class="token operator">=</span> obj<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'bndbox'</span><span class="token punctuation">)</span>        b <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">(</span>xmlbox<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'xmin'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span>xmlbox<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'xmax'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span>xmlbox<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'ymin'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">,</span>             <span class="token builtin">float</span><span class="token punctuation">(</span>xmlbox<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'ymax'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">)</span>        bb <span class="token operator">=</span> convert<span class="token punctuation">(</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span> h<span class="token punctuation">)</span><span class="token punctuation">,</span> b<span class="token punctuation">)</span>        out_file<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>cls_id<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">" "</span> <span class="token operator">+</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token keyword">for</span> a <span class="token keyword">in</span> bb<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>    in_file<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>    out_file<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>        wd <span class="token operator">=</span> os<span class="token punctuation">.</span>getcwd<span class="token punctuation">(</span><span class="token punctuation">)</span>    data_base_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>wd<span class="token punctuation">,</span> <span class="token string">"data/VOCdevkit/"</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span>data_base_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>        os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>data_base_dir<span class="token punctuation">)</span>    work_sapce_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>data_base_dir<span class="token punctuation">,</span> <span class="token string">"VOC2007/"</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span>work_sapce_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>        os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>work_sapce_dir<span class="token punctuation">)</span>    annotation_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>work_sapce_dir<span class="token punctuation">,</span> <span class="token string">"Annotations/"</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span>annotation_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>        os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>annotation_dir<span class="token punctuation">)</span>    clear_hidden_files<span class="token punctuation">(</span>annotation_dir<span class="token punctuation">)</span>    image_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>work_sapce_dir<span class="token punctuation">,</span> <span class="token string">"JPEGImages/"</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span>image_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>        os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>image_dir<span class="token punctuation">)</span>    clear_hidden_files<span class="token punctuation">(</span>image_dir<span class="token punctuation">)</span>    yolo_labels_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>work_sapce_dir<span class="token punctuation">,</span> <span class="token string">"YOLOLabels/"</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span>yolo_labels_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>        os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>yolo_labels_dir<span class="token punctuation">)</span>    clear_hidden_files<span class="token punctuation">(</span>yolo_labels_dir<span class="token punctuation">)</span>    yolov5_images_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>data_base_dir<span class="token punctuation">,</span> <span class="token string">"images/"</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span>yolov5_images_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>        os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>yolov5_images_dir<span class="token punctuation">)</span>    clear_hidden_files<span class="token punctuation">(</span>yolov5_images_dir<span class="token punctuation">)</span>    yolov5_labels_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>data_base_dir<span class="token punctuation">,</span> <span class="token string">"labels/"</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span>yolov5_labels_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>        os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>yolov5_labels_dir<span class="token punctuation">)</span>    clear_hidden_files<span class="token punctuation">(</span>yolov5_labels_dir<span class="token punctuation">)</span>    yolov5_images_train_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>yolov5_images_dir<span class="token punctuation">,</span> <span class="token string">"train/"</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span>yolov5_images_train_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>        os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>yolov5_images_train_dir<span class="token punctuation">)</span>    clear_hidden_files<span class="token punctuation">(</span>yolov5_images_train_dir<span class="token punctuation">)</span>    yolov5_images_test_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>yolov5_images_dir<span class="token punctuation">,</span> <span class="token string">"val/"</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span>yolov5_images_test_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>        os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>yolov5_images_test_dir<span class="token punctuation">)</span>    clear_hidden_files<span class="token punctuation">(</span>yolov5_images_test_dir<span class="token punctuation">)</span>    yolov5_labels_train_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>yolov5_labels_dir<span class="token punctuation">,</span> <span class="token string">"train/"</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span>yolov5_labels_train_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>        os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>yolov5_labels_train_dir<span class="token punctuation">)</span>    clear_hidden_files<span class="token punctuation">(</span>yolov5_labels_train_dir<span class="token punctuation">)</span>    yolov5_labels_test_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>yolov5_labels_dir<span class="token punctuation">,</span> <span class="token string">"val/"</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span>yolov5_labels_test_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>        os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>yolov5_labels_test_dir<span class="token punctuation">)</span>    clear_hidden_files<span class="token punctuation">(</span>yolov5_labels_test_dir<span class="token punctuation">)</span>        train_file <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>wd<span class="token punctuation">,</span> <span class="token string">"data/yolov5_train.txt"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>    test_file <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>wd<span class="token punctuation">,</span> <span class="token string">"data/yolov5_val.txt"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>    train_file<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>    test_file<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>    train_file <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>wd<span class="token punctuation">,</span> <span class="token string">"data/yolov5_train.txt"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'a'</span><span class="token punctuation">)</span>    test_file <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>wd<span class="token punctuation">,</span> <span class="token string">"data/yolov5_val.txt"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'a'</span><span class="token punctuation">)</span>    list_imgs <span class="token operator">=</span> os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span>image_dir<span class="token punctuation">)</span>  <span class="token comment"># list image files</span>    prob <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Probability: %d"</span> <span class="token operator">%</span> prob<span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>list_imgs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>image_dir<span class="token punctuation">,</span> list_imgs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isfile<span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token punctuation">:</span>            image_path <span class="token operator">=</span> image_dir <span class="token operator">+</span> list_imgs<span class="token punctuation">[</span>i<span class="token punctuation">]</span>            voc_path <span class="token operator">=</span> list_imgs<span class="token punctuation">[</span>i<span class="token punctuation">]</span>            <span class="token punctuation">(</span>nameWithoutExtention<span class="token punctuation">,</span> extention<span class="token punctuation">)</span> <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>splitext<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>basename<span class="token punctuation">(</span>image_path<span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token punctuation">(</span>voc_nameWithoutExtention<span class="token punctuation">,</span> voc_extention<span class="token punctuation">)</span> <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>splitext<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>basename<span class="token punctuation">(</span>voc_path<span class="token punctuation">)</span><span class="token punctuation">)</span>            annotation_name <span class="token operator">=</span> nameWithoutExtention <span class="token operator">+</span> <span class="token string">'.xml'</span>            annotation_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>annotation_dir<span class="token punctuation">,</span> annotation_name<span class="token punctuation">)</span>            label_name <span class="token operator">=</span> nameWithoutExtention <span class="token operator">+</span> <span class="token string">'.txt'</span>            label_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>yolo_labels_dir<span class="token punctuation">,</span> label_name<span class="token punctuation">)</span>        prob <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Probability: %d"</span> <span class="token operator">%</span> prob<span class="token punctuation">)</span>                <span class="token comment"># 训练集</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>prob <span class="token operator">&lt;</span> TRAIN_RATIO<span class="token punctuation">)</span><span class="token punctuation">:</span>              <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>annotation_path<span class="token punctuation">)</span><span class="token punctuation">:</span>                train_file<span class="token punctuation">.</span>write<span class="token punctuation">(</span>image_path <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>                <span class="token comment"># 转换label</span>                convert_annotation<span class="token punctuation">(</span>nameWithoutExtention<span class="token punctuation">)</span>                  copyfile<span class="token punctuation">(</span>image_path<span class="token punctuation">,</span> yolov5_images_train_dir <span class="token operator">+</span> voc_path<span class="token punctuation">)</span>                copyfile<span class="token punctuation">(</span>label_path<span class="token punctuation">,</span> yolov5_labels_train_dir <span class="token operator">+</span> label_name<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>             <span class="token comment"># 测试集</span>            <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>annotation_path<span class="token punctuation">)</span><span class="token punctuation">:</span>                test_file<span class="token punctuation">.</span>write<span class="token punctuation">(</span>image_path <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>                <span class="token comment"># 转换label</span>                convert_annotation<span class="token punctuation">(</span>nameWithoutExtention<span class="token punctuation">)</span>                  copyfile<span class="token punctuation">(</span>image_path<span class="token punctuation">,</span> yolov5_images_test_dir <span class="token operator">+</span> voc_path<span class="token punctuation">)</span>                copyfile<span class="token punctuation">(</span>label_path<span class="token punctuation">,</span> yolov5_labels_test_dir <span class="token operator">+</span> label_name<span class="token punctuation">)</span>    train_file<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>    test_file<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>执行Python脚本:</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python prepare_data.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意：classes=[“ball”,”messi”]要根据自己的数据集类别做相应的修改</p><ul><li><p>在VOCdevkit/VOC2007目录下可以看到生成了文件夹YOLOLabels<br>YOLOLabels下的文件是images文件夹下每一个图像的yolo格式的标注文件，这是由annotations的xml标注文件转换来的；</p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210902104319451-143180433.png"></p></li><li><p><strong>在VOCdevkit目录下生成了images和labels文件夹</strong><br><strong>images文件夹下有train和val文件夹，分别放置训练集和验证集图片；</strong><br><strong>labels文件夹有train和val文件夹，分别放置训练集和验证集标签（yolo格式）</strong><br><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210902104439563-1887179132.png"></p></li></ul><ul><li>在yolov5下生成了两个文件yolov5_train.txt和yolov5_val.txt。<br> yolov5_train.txt和yolov5_val.txt分别给出了训练图片文件和验证图片文件的列表，含有每个图片的路径和文件名。<br> <img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210902104810557-1723125006.png"></li></ul><h3 id="5-5-修改配置文件"><a href="#5-5-修改配置文件" class="headerlink" title="5.5 修改配置文件"></a>5.5 修改配置文件</h3><h4 id="5-5-1-新建文件data-voc-ball-yaml"><a href="#5-5-1-新建文件data-voc-ball-yaml" class="headerlink" title="5.5.1 新建文件data/voc_ball.yaml"></a>5.5.1 新建文件data/voc_ball.yaml</h4><p>为了对于定制数据集进行专门的训练，这里需要新建对应的配置文件。<br>可以复制data/voc.yaml再根据自己情况和需要进行修改；<br>将复制后得到的文件重命名为voc_ball.yaml<br>然后，修改配置参数</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token comment"># download command/URL (optional)</span><span class="token comment">#download: bash data/scripts/get_voc.sh</span><span class="token comment"># train and val data as 1) directory: path/images/, 2) file: path/images.txt, or </span><span class="token key atrule">3) list</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>path1/images/<span class="token punctuation">,</span> path2/images/<span class="token punctuation">]</span><span class="token key atrule">train</span><span class="token punctuation">:</span> ./VOCdevkit/images/train/   <span class="token key atrule">val</span><span class="token punctuation">:</span> ./VOCdevkit/images/val/  <span class="token comment"># number of classes</span><span class="token key atrule">nc</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token comment"># class names</span><span class="token key atrule">names</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'ball'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="5-5-2-新建文件models-yolov5s-ball-yaml"><a href="#5-5-2-新建文件models-yolov5s-ball-yaml" class="headerlink" title="5.5.2 新建文件models.yolov5s_ball.yaml"></a>5.5.2 新建文件models.yolov5s_ball.yaml</h4><p>可以复制models/yolov5s.yaml再根据自己的情况修改；<br>可以重命名为models/yolov5s_ball.yaml<br>然后，修改配置参数</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">nc</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token comment"># number of classes</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="5-6-使用wandb训练可视化工具"><a href="#5-6-使用wandb训练可视化工具" class="headerlink" title="5.6 使用wandb训练可视化工具"></a>5.6 使用wandb训练可视化工具</h3><p>wandb(Weight&amp;Biases)是一个类似于tensorboard的在线模型训练可视化工具。<br>YOLOv5(v4.0 release开始)集成了Weights&amp;Biases，可以方便的追踪模型训练的整个过程，包括模型的性能、超参数、GPU的使用情况、模型预测等。</p><h4 id="5-6-1-注册和安装wandb"><a href="#5-6-1-注册和安装wandb" class="headerlink" title="5.6.1 注册和安装wandb"></a>5.6.1 注册和安装wandb</h4><p><strong>注册wandb</strong><br>到其官网<a href="https://wandb.ai/home">https://wandb.ai/home</a> 注册<br><strong>安装wandb</strong><br>执行</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">pip install wandb<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>wandb: You can find your API key in your browser here: <a href="https://wandb.ai/authorize">https://wandb.ai/authorize</a><br>wandb: Paste an API key from your profile and hit enter:<br>登录后有 Copy this key and paste it into your command line to authorize it.</p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210902101443026-563645490.png"></p><p><strong>本地使用wandb:</strong><br>wandb网站有时挺卡，wandb也有本地使用方式。参考：<a href="https://docs.wandb.ai/self-hosted/local%EF%BC%8C">https://docs.wandb.ai/self-hosted/local，</a><br>配置好后可以本地访问。</p><p><strong>关闭wandb：</strong><br>如需要关闭wandb，可把代码文件utils/wandb_logging/wandb_utils.py中的</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">try</span><span class="token punctuation">:</span>    <span class="token keyword">import</span> wandb    <span class="token keyword">from</span> wandb <span class="token keyword">import</span> init<span class="token punctuation">,</span> finish<span class="token keyword">except</span> ImportError<span class="token punctuation">:</span>    wandb <span class="token operator">=</span> <span class="token boolean">None</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>修改为</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">wandb <span class="token operator">=</span> <span class="token boolean">None</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="5-6-2-在代码中修改项目名称"><a href="#5-6-2-在代码中修改项目名称" class="headerlink" title="5.6.2 在代码中修改项目名称"></a>5.6.2 在代码中修改项目名称</h4><p>在utils/wandb_logging/wandb_utils.py中</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>wandb_run <span class="token operator">=</span> wandb<span class="token punctuation">.</span>init<span class="token punctuation">(</span>config<span class="token operator">=</span>opt<span class="token punctuation">,</span>resume<span class="token operator">=</span><span class="token string">"allow"</span><span class="token punctuation">,</span>project<span class="token operator">=</span><span class="token string">'YOLOv5-Ball-Ubuntu'</span> <span class="token keyword">if</span> opt<span class="token punctuation">.</span>project<span class="token operator">==</span> <span class="token string">'runs/train'</span> <span class="token keyword">else</span> Path<span class="token punctuation">(</span>opt<span class="token punctuation">.</span>project<span class="token punctuation">)</span><span class="token punctuation">.</span>stem<span class="token punctuation">,</span>                                    name<span class="token operator">=</span>name<span class="token punctuation">,</span>                                    job_type<span class="token operator">=</span>job_type<span class="token punctuation">,</span>                                    <span class="token builtin">id</span><span class="token operator">=</span>run_id<span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token keyword">not</span> wandb<span class="token punctuation">.</span>run <span class="token keyword">else</span> wandb<span class="token punctuation">.</span>run<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="5-7-训练自己的数据集"><a href="#5-7-训练自己的数据集" class="headerlink" title="5.7 训练自己的数据集"></a>5.7 训练自己的数据集</h3><h4 id="5-7-1-训练命令"><a href="#5-7-1-训练命令" class="headerlink" title="5.7.1 训练命令"></a>5.7.1 训练命令</h4><p>在yolov5路径下执行：<br>使用YOLOv5s训练命令：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python train.py --data data&#x2F;voc_ball.yaml --cfg models&#x2F;yolov5s_ball.yaml --weights weights&#x2F;yolov5s.pt --batch-size 16 --epochs 50 --workers 4 --name bm-yolov5s<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>使用YOLOv5x训练命令：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python train.py --data data&#x2F;voc_ball.yaml --cfg models&#x2F;yolov5x_ball.yaml --weights weights&#x2F;yolov5x.pt --batch-size 8 --epochs 100 --workers 4 --name bm-yolov5x<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意：如果出现显存溢出，可减少batch-size</p><p>开始训练，如下所示</p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210902171909658-227298560.png"><br><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20210902171915410-1326918050.png"></p><h4 id="5-7-2-训练过程可视化"><a href="#5-7-2-训练过程可视化" class="headerlink" title="5.7.2 训练过程可视化"></a>5.7.2 训练过程可视化</h4><p>在yolov5路径下执行：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">tensorboard --logdir&#x3D;.&#x2F;runs<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20211208105615540-1923473733.png"></p><p><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20211208105725931-1383581352.png"></p><h4 id="7-7-3-训练结果的查看"><a href="#7-7-3-训练结果的查看" class="headerlink" title="7.7.3 训练结果的查看"></a>7.7.3 训练结果的查看</h4><p>查看runs目录下的文件<br><img src="/blog/blog/2022/01/20/yolov5-mu-biao-jian-ce-xue-xi-zong-jie/YOLOv5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/1571518-20211208105818579-1200494046.png"></p><h3 id="5-8-测试训练出的网络模型"><a href="#5-8-测试训练出的网络模型" class="headerlink" title="5.8 测试训练出的网络模型"></a>5.8 测试训练出的网络模型</h3><h4 id="5-8-1-测试图片"><a href="#5-8-1-测试图片" class="headerlink" title="5.8.1 测试图片"></a>5.8.1 测试图片</h4><p>yolo5路径下执行:<br>使用yolov5s训练出的权重</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python detect.py --source .&#x2F;testfiles&#x2F;img1.jpg --weights runs&#x2F;train&#x2F;bmyolov5s&#x2F;weights&#x2F;best.pt <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>使用yolov5x训练出的权重</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python detect.py --source .&#x2F;testfiles&#x2F;img1.jpg --weights runs&#x2F;train&#x2F;bmyolov5x&#x2F;weights&#x2F;best.pt <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="5-8-2-测试视频"><a href="#5-8-2-测试视频" class="headerlink" title="5.8.2 测试视频"></a>5.8.2 测试视频</h4><p>yolov5路径下执行：<br>使用yolov5s训练出的权重</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python detect.py --source .&#x2F;testfiles&#x2F;messi.mp4 --weights runs&#x2F;train&#x2F;bmyolov5s&#x2F;weights&#x2F;best.pt <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>使用yolov5x训练出的权重</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python detect.py --source .&#x2F;testfiles&#x2F;messi.mp4 --weights runs&#x2F;train&#x2F;bmyolov5x&#x2F;weights&#x2F;best.pt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注：</p><ul><li>1)批量处理文件夹下的图片和视频可以指定文件夹的名字，如–source ./testfiles</li><li>2)命令后可加上目标的置信度阈值，如–conf-thres 0.4</li></ul><h4 id="5-8-3-性能统计"><a href="#5-8-3-性能统计" class="headerlink" title="5.8.3 性能统计"></a>5.8.3 性能统计</h4><p>yolo5路径下执行</p><p>使用yolov5s训练出的权重</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python test.py --data data&#x2F;voc_bm.yaml --weights runs&#x2F;train&#x2F;bmyolov5s&#x2F;weights&#x2F;best.pt --batch-size 16<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>使用yolov5x训练出的权重</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python test.py --data data&#x2F;voc_bm.yaml --weights runs&#x2F;train&#x2F;bmyolov5x&#x2F;weights&#x2F;best.pt --batch-size 16<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 炼丹术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用MobileNetSSD进行对象检测</title>
      <link href="/blog/2022/01/20/shi-yong-mobilenetssd-jin-xing-dui-xiang-jian-ce/"/>
      <url>/blog/2022/01/20/shi-yong-mobilenetssd-jin-xing-dui-xiang-jian-ce/</url>
      
        <content type="html"><![CDATA[<h2 id="使用MobileNetSSD进行对象检测"><a href="#使用MobileNetSSD进行对象检测" class="headerlink" title="使用MobileNetSSD进行对象检测"></a>使用MobileNetSSD进行对象检测</h2><h3 id="1-单帧图片识别"><a href="#1-单帧图片识别" class="headerlink" title="1.单帧图片识别"></a>1.单帧图片识别</h3><ul><li>object_detection.py</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 导入必要的包</span><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> argparse<span class="token keyword">import</span> cv2<span class="token comment"># 构造参数 parse 并解析参数</span>ap <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>ap<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"-i"</span><span class="token punctuation">,</span> <span class="token string">"--image"</span><span class="token punctuation">,</span> required<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token builtin">help</span><span class="token operator">=</span><span class="token string">"path to input image"</span><span class="token punctuation">)</span>ap<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"-p"</span><span class="token punctuation">,</span> <span class="token string">"--prototxt"</span><span class="token punctuation">,</span> required<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token builtin">help</span><span class="token operator">=</span><span class="token string">"path to Caffe 'deploy' prototxt file"</span><span class="token punctuation">)</span>ap<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"-m"</span><span class="token punctuation">,</span> <span class="token string">"--model"</span><span class="token punctuation">,</span> required<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token builtin">help</span><span class="token operator">=</span><span class="token string">"path to Caffe pre-trained model"</span><span class="token punctuation">)</span>ap<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"-c"</span><span class="token punctuation">,</span> <span class="token string">"--confidence"</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">float</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span><span class="token builtin">help</span><span class="token operator">=</span><span class="token string">"minimum probability to filter weak detections"</span><span class="token punctuation">)</span>args <span class="token operator">=</span> <span class="token builtin">vars</span><span class="token punctuation">(</span>ap<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 初始化 MobileNet SSD 训练的类标签列表</span><span class="token comment"># 检测，然后为每个类生成一组边界框颜色</span>CLASSES <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"background"</span><span class="token punctuation">,</span> <span class="token string">"aeroplane"</span><span class="token punctuation">,</span> <span class="token string">"bicycle"</span><span class="token punctuation">,</span> <span class="token string">"bird"</span><span class="token punctuation">,</span> <span class="token string">"boat"</span><span class="token punctuation">,</span><span class="token string">"bottle"</span><span class="token punctuation">,</span> <span class="token string">"bus"</span><span class="token punctuation">,</span> <span class="token string">"car"</span><span class="token punctuation">,</span> <span class="token string">"cat"</span><span class="token punctuation">,</span> <span class="token string">"chair"</span><span class="token punctuation">,</span> <span class="token string">"cow"</span><span class="token punctuation">,</span> <span class="token string">"diningtable"</span><span class="token punctuation">,</span><span class="token string">"dog"</span><span class="token punctuation">,</span> <span class="token string">"horse"</span><span class="token punctuation">,</span> <span class="token string">"motorbike"</span><span class="token punctuation">,</span> <span class="token string">"person"</span><span class="token punctuation">,</span> <span class="token string">"pottedplant"</span><span class="token punctuation">,</span> <span class="token string">"sheep"</span><span class="token punctuation">,</span><span class="token string">"sofa"</span><span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">,</span> <span class="token string">"tvmonitor"</span><span class="token punctuation">]</span>COLORS <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>CLASSES<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 从磁盘加载我们的序列化模型</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[INFO] loading model..."</span><span class="token punctuation">)</span>net <span class="token operator">=</span> cv2<span class="token punctuation">.</span>dnn<span class="token punctuation">.</span>readNetFromCaffe<span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token string">"prototxt"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> args<span class="token punctuation">[</span><span class="token string">"model"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># 加载输入图像并为图像构造一个输入 blob</span><span class="token comment"># 将大小调整为固定的 300x300 像素，然后对其进行标准化</span><span class="token comment">#（注意：标准化是通过 MobileNet SSD 完成执行的</span>image <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token string">"image"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>h<span class="token punctuation">,</span> w<span class="token punctuation">)</span> <span class="token operator">=</span> image<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span>blob <span class="token operator">=</span> cv2<span class="token punctuation">.</span>dnn<span class="token punctuation">.</span>blobFromImage<span class="token punctuation">(</span>cv2<span class="token punctuation">.</span>resize<span class="token punctuation">(</span>image<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">300</span><span class="token punctuation">,</span> <span class="token number">300</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0.007843</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">300</span><span class="token punctuation">,</span> <span class="token number">300</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">127.5</span><span class="token punctuation">)</span><span class="token comment"># 通过网络传递blob并获得检测</span><span class="token comment"># 预测</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[INFO] computing object detections..."</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span>setInput<span class="token punctuation">(</span>blob<span class="token punctuation">)</span>detections <span class="token operator">=</span> net<span class="token punctuation">.</span>forward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 循环检测</span><span class="token keyword">for</span> i <span class="token keyword">in</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> detections<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment"># 提取与相关的置信度（即概率）</span>confidence <span class="token operator">=</span> detections<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token comment"># 通过确保置信度大于最小置信度来过滤无效检测</span><span class="token keyword">if</span> confidence <span class="token operator">></span> args<span class="token punctuation">[</span><span class="token string">"confidence"</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token comment"># 从类标签detections中提取索引，</span><span class="token comment"># 然后计算物体边界框的 (x, y) 坐标</span>idx <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>detections<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>box <span class="token operator">=</span> detections<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>w<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w<span class="token punctuation">,</span> h<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>startX<span class="token punctuation">,</span> startY<span class="token punctuation">,</span> endX<span class="token punctuation">,</span> endY<span class="token punctuation">)</span> <span class="token operator">=</span> box<span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">"int"</span><span class="token punctuation">)</span><span class="token comment"># 显示预测结果</span>label <span class="token operator">=</span> <span class="token string">"&#123;&#125;: &#123;:.2f&#125;%"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>CLASSES<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> confidence <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[INFO] &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>label<span class="token punctuation">)</span><span class="token punctuation">)</span>cv2<span class="token punctuation">.</span>rectangle<span class="token punctuation">(</span>image<span class="token punctuation">,</span> <span class="token punctuation">(</span>startX<span class="token punctuation">,</span> startY<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>endX<span class="token punctuation">,</span> endY<span class="token punctuation">)</span><span class="token punctuation">,</span>COLORS<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>y <span class="token operator">=</span> startY <span class="token operator">-</span> <span class="token number">15</span> <span class="token keyword">if</span> startY <span class="token operator">-</span> <span class="token number">15</span> <span class="token operator">></span> <span class="token number">15</span> <span class="token keyword">else</span> startY <span class="token operator">+</span> <span class="token number">15</span>cv2<span class="token punctuation">.</span>putText<span class="token punctuation">(</span>image<span class="token punctuation">,</span> label<span class="token punctuation">,</span> <span class="token punctuation">(</span>startX<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span>cv2<span class="token punctuation">.</span>FONT_HERSHEY_SIMPLEX<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> COLORS<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token comment"># 显示输出图像</span>cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">"Output"</span><span class="token punctuation">,</span> image<span class="token punctuation">)</span>cv2<span class="token punctuation">.</span>waitKey<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>调用方法:</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 用法</span>python object_detection<span class="token punctuation">.</span>py <span class="token operator">-</span><span class="token operator">-</span>image images<span class="token operator">/</span>example_01<span class="token punctuation">.</span>jpg <span class="token operator">-</span><span class="token operator">-</span>prototxt MobileNetSSD_deploy<span class="token punctuation">.</span>prototxt<span class="token punctuation">.</span>txt <span class="token operator">-</span><span class="token operator">-</span>model MobileNetSSD_deploy<span class="token punctuation">.</span>caffemodel<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>测试效果</li></ul><p><img src="/blog/blog/2022/01/20/shi-yong-mobilenetssd-jin-xing-dui-xiang-jian-ce/%E4%BD%BF%E7%94%A8MobileNetSSD%E8%BF%9B%E8%A1%8C%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/1571518-20210802144117159-1049441785.png"></p><h3 id="2-视频流实时检测对象"><a href="#2-视频流实时检测对象" class="headerlink" title="2.视频流实时检测对象"></a>2.视频流实时检测对象</h3><ul><li>real_time_object_detection.py</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 导入必要的包</span><span class="token keyword">from</span> imutils<span class="token punctuation">.</span>video <span class="token keyword">import</span> VideoStream<span class="token keyword">from</span> imutils<span class="token punctuation">.</span>video <span class="token keyword">import</span> FPS<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> argparse<span class="token keyword">import</span> imutils<span class="token keyword">import</span> time<span class="token keyword">import</span> cv2<span class="token comment"># 构造参数 parse 并解析参数</span>ap <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>ap<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"-p"</span><span class="token punctuation">,</span> <span class="token string">"--prototxt"</span><span class="token punctuation">,</span> required<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token builtin">help</span><span class="token operator">=</span><span class="token string">"path to Caffe 'deploy' prototxt file"</span><span class="token punctuation">)</span>ap<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"-m"</span><span class="token punctuation">,</span> <span class="token string">"--model"</span><span class="token punctuation">,</span> required<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token builtin">help</span><span class="token operator">=</span><span class="token string">"path to Caffe pre-trained model"</span><span class="token punctuation">)</span>ap<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"-c"</span><span class="token punctuation">,</span> <span class="token string">"--confidence"</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">float</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span><span class="token builtin">help</span><span class="token operator">=</span><span class="token string">"minimum probability to filter weak detections"</span><span class="token punctuation">)</span>args <span class="token operator">=</span> <span class="token builtin">vars</span><span class="token punctuation">(</span>ap<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 初始化 MobileNet SSD 训练的类标签列表</span><span class="token comment">#  检测，然后为每个类生成一组边界框颜色</span>CLASSES <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"background"</span><span class="token punctuation">,</span> <span class="token string">"aeroplane"</span><span class="token punctuation">,</span> <span class="token string">"bicycle"</span><span class="token punctuation">,</span> <span class="token string">"bird"</span><span class="token punctuation">,</span> <span class="token string">"boat"</span><span class="token punctuation">,</span><span class="token string">"bottle"</span><span class="token punctuation">,</span> <span class="token string">"bus"</span><span class="token punctuation">,</span> <span class="token string">"car"</span><span class="token punctuation">,</span> <span class="token string">"cat"</span><span class="token punctuation">,</span> <span class="token string">"chair"</span><span class="token punctuation">,</span> <span class="token string">"cow"</span><span class="token punctuation">,</span> <span class="token string">"diningtable"</span><span class="token punctuation">,</span><span class="token string">"dog"</span><span class="token punctuation">,</span> <span class="token string">"horse"</span><span class="token punctuation">,</span> <span class="token string">"motorbike"</span><span class="token punctuation">,</span> <span class="token string">"person"</span><span class="token punctuation">,</span> <span class="token string">"pottedplant"</span><span class="token punctuation">,</span> <span class="token string">"sheep"</span><span class="token punctuation">,</span><span class="token string">"sofa"</span><span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">,</span> <span class="token string">"tvmonitor"</span><span class="token punctuation">]</span>COLORS <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>CLASSES<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#  从磁盘加载我们的序列化模型</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[INFO] loading model..."</span><span class="token punctuation">)</span>net <span class="token operator">=</span> cv2<span class="token punctuation">.</span>dnn<span class="token punctuation">.</span>readNetFromCaffe<span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token string">"prototxt"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> args<span class="token punctuation">[</span><span class="token string">"model"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># 初始化视频流，允许摄像机传感器预加载，</span><span class="token comment"># 并初始化 FPS 计数器</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[INFO] starting video stream..."</span><span class="token punctuation">)</span>vs <span class="token operator">=</span> VideoStream<span class="token punctuation">(</span>src<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">2.0</span><span class="token punctuation">)</span>fps <span class="token operator">=</span> FPS<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 循环读取视频流中的帧</span><span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span><span class="token comment"># 从线程视频流中抓取帧并调整其大小</span><span class="token comment"># 最大宽度为 400 像素</span>frame <span class="token operator">=</span> vs<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>frame <span class="token operator">=</span> imutils<span class="token punctuation">.</span>resize<span class="token punctuation">(</span>frame<span class="token punctuation">,</span> width<span class="token operator">=</span><span class="token number">400</span><span class="token punctuation">)</span><span class="token comment"># 获取帧尺寸并将其转换为 blob</span><span class="token punctuation">(</span>h<span class="token punctuation">,</span> w<span class="token punctuation">)</span> <span class="token operator">=</span> frame<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span>blob <span class="token operator">=</span> cv2<span class="token punctuation">.</span>dnn<span class="token punctuation">.</span>blobFromImage<span class="token punctuation">(</span>cv2<span class="token punctuation">.</span>resize<span class="token punctuation">(</span>frame<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">300</span><span class="token punctuation">,</span> <span class="token number">300</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">0.007843</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">300</span><span class="token punctuation">,</span> <span class="token number">300</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">127.5</span><span class="token punctuation">)</span><span class="token comment"># 通过网络传递blob并获得检测</span><span class="token comment"># 预测</span>net<span class="token punctuation">.</span>setInput<span class="token punctuation">(</span>blob<span class="token punctuation">)</span>detections <span class="token operator">=</span> net<span class="token punctuation">.</span>forward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 循环检测</span><span class="token keyword">for</span> i <span class="token keyword">in</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> detections<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment"># 提取与相关联的置信度（即概率）用来预测</span>confidence <span class="token operator">=</span> detections<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token comment"># 通过确保置信度大于最小置信度来过滤无效检测</span><span class="token keyword">if</span> confidence <span class="token operator">></span> args<span class="token punctuation">[</span><span class="token string">"confidence"</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token comment"># 从类标签 detections中提取索引，然后计算物体的边界框 (x, y) 坐标</span>idx <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>detections<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>box <span class="token operator">=</span> detections<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>w<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w<span class="token punctuation">,</span> h<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>startX<span class="token punctuation">,</span> startY<span class="token punctuation">,</span> endX<span class="token punctuation">,</span> endY<span class="token punctuation">)</span> <span class="token operator">=</span> box<span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">"int"</span><span class="token punctuation">)</span><span class="token comment"># 在当前帧上绘制预测</span>label <span class="token operator">=</span> <span class="token string">"&#123;&#125;: &#123;:.2f&#125;%"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>CLASSES<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span>confidence <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span>cv2<span class="token punctuation">.</span>rectangle<span class="token punctuation">(</span>frame<span class="token punctuation">,</span> <span class="token punctuation">(</span>startX<span class="token punctuation">,</span> startY<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>endX<span class="token punctuation">,</span> endY<span class="token punctuation">)</span><span class="token punctuation">,</span>COLORS<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>y <span class="token operator">=</span> startY <span class="token operator">-</span> <span class="token number">15</span> <span class="token keyword">if</span> startY <span class="token operator">-</span> <span class="token number">15</span> <span class="token operator">></span> <span class="token number">15</span> <span class="token keyword">else</span> startY <span class="token operator">+</span> <span class="token number">15</span>cv2<span class="token punctuation">.</span>putText<span class="token punctuation">(</span>frame<span class="token punctuation">,</span> label<span class="token punctuation">,</span> <span class="token punctuation">(</span>startX<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span>cv2<span class="token punctuation">.</span>FONT_HERSHEY_SIMPLEX<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> COLORS<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token comment"># 显示输出帧</span>cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">"Frame"</span><span class="token punctuation">,</span> frame<span class="token punctuation">)</span>key <span class="token operator">=</span> cv2<span class="token punctuation">.</span>waitKey<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token number">0xFF</span><span class="token comment"># 如果按下 `q` 键，则跳出循环</span><span class="token keyword">if</span> key <span class="token operator">==</span> <span class="token builtin">ord</span><span class="token punctuation">(</span><span class="token string">"q"</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">break</span><span class="token comment"># 更新 FPS 计数器</span>fps<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 停止定时器并显示 FPS 信息</span>fps<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[INFO] elapsed time: &#123;:.2f&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>fps<span class="token punctuation">.</span>elapsed<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[INFO] approx. FPS: &#123;:.2f&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>fps<span class="token punctuation">.</span>fps<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 做一些清理</span>cv2<span class="token punctuation">.</span>destroyAllWindows<span class="token punctuation">(</span><span class="token punctuation">)</span>vs<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>调用方法:</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 用法</span>python real_time_object_detection<span class="token punctuation">.</span>py <span class="token operator">-</span><span class="token operator">-</span>prototxt MobileNetSSD_deploy<span class="token punctuation">.</span>prototxt<span class="token punctuation">.</span>txt <span class="token operator">-</span><span class="token operator">-</span>model MobileNetSSD_deploy<span class="token punctuation">.</span>caffemodel<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>测试效果：<br><img src="/blog/blog/2022/01/20/shi-yong-mobilenetssd-jin-xing-dui-xiang-jian-ce/%E4%BD%BF%E7%94%A8MobileNetSSD%E8%BF%9B%E8%A1%8C%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B/1571518-20210802144914755-1582237448.png"></li></ul><h3 id="3-配置文件"><a href="#3-配置文件" class="headerlink" title="3.配置文件"></a>3.配置文件</h3><p>设置卷积层及其模型相关配置</p><ul><li>MobileNetSSD_deploy.prototxt.txt</li></ul><pre class="line-numbers language-json" data-language="json"><code class="language-json">name<span class="token operator">:</span> <span class="token string">"MobileNet-SSD"</span>input<span class="token operator">:</span> <span class="token string">"data"</span>input_shape <span class="token punctuation">&#123;</span>  dim<span class="token operator">:</span> <span class="token number">1</span>  dim<span class="token operator">:</span> <span class="token number">3</span>  dim<span class="token operator">:</span> <span class="token number">300</span>  dim<span class="token operator">:</span> <span class="token number">300</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv0"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"data"</span>  top<span class="token operator">:</span> <span class="token string">"conv0"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">32</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    stride<span class="token operator">:</span> <span class="token number">2</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv0/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv0"</span>  top<span class="token operator">:</span> <span class="token string">"conv0"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv1/dw"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv0"</span>  top<span class="token operator">:</span> <span class="token string">"conv1/dw"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">32</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    group<span class="token operator">:</span> <span class="token number">32</span>    engine<span class="token operator">:</span> CAFFE    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv1/dw/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv1/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv1/dw"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv1"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv1/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv1"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">64</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv1/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv1"</span>  top<span class="token operator">:</span> <span class="token string">"conv1"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv2/dw"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv1"</span>  top<span class="token operator">:</span> <span class="token string">"conv2/dw"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">64</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    stride<span class="token operator">:</span> <span class="token number">2</span>    group<span class="token operator">:</span> <span class="token number">64</span>    engine<span class="token operator">:</span> CAFFE    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv2/dw/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv2/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv2/dw"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv2"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv2/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv2"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">128</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv2/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv2"</span>  top<span class="token operator">:</span> <span class="token string">"conv2"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv3/dw"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv2"</span>  top<span class="token operator">:</span> <span class="token string">"conv3/dw"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">128</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    group<span class="token operator">:</span> <span class="token number">128</span>    engine<span class="token operator">:</span> CAFFE    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv3/dw/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv3/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv3/dw"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv3"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv3/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv3"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">128</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv3/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv3"</span>  top<span class="token operator">:</span> <span class="token string">"conv3"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv4/dw"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv3"</span>  top<span class="token operator">:</span> <span class="token string">"conv4/dw"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">128</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    stride<span class="token operator">:</span> <span class="token number">2</span>    group<span class="token operator">:</span> <span class="token number">128</span>    engine<span class="token operator">:</span> CAFFE    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv4/dw/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv4/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv4/dw"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv4"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv4/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv4"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">256</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv4/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv4"</span>  top<span class="token operator">:</span> <span class="token string">"conv4"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv5/dw"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv4"</span>  top<span class="token operator">:</span> <span class="token string">"conv5/dw"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">256</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    group<span class="token operator">:</span> <span class="token number">256</span>    engine<span class="token operator">:</span> CAFFE    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv5/dw/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv5/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv5/dw"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv5"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv5/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv5"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">256</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv5/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv5"</span>  top<span class="token operator">:</span> <span class="token string">"conv5"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv6/dw"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv5"</span>  top<span class="token operator">:</span> <span class="token string">"conv6/dw"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">256</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    stride<span class="token operator">:</span> <span class="token number">2</span>    group<span class="token operator">:</span> <span class="token number">256</span>    engine<span class="token operator">:</span> CAFFE    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv6/dw/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv6/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv6/dw"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv6"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv6/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv6"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">512</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv6/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv6"</span>  top<span class="token operator">:</span> <span class="token string">"conv6"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv7/dw"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv6"</span>  top<span class="token operator">:</span> <span class="token string">"conv7/dw"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">512</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    group<span class="token operator">:</span> <span class="token number">512</span>    engine<span class="token operator">:</span> CAFFE    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv7/dw/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv7/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv7/dw"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv7"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv7/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv7"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">512</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv7/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv7"</span>  top<span class="token operator">:</span> <span class="token string">"conv7"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv8/dw"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv7"</span>  top<span class="token operator">:</span> <span class="token string">"conv8/dw"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">512</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    group<span class="token operator">:</span> <span class="token number">512</span>    engine<span class="token operator">:</span> CAFFE    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv8/dw/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv8/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv8/dw"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv8"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv8/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv8"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">512</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv8/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv8"</span>  top<span class="token operator">:</span> <span class="token string">"conv8"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv9/dw"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv8"</span>  top<span class="token operator">:</span> <span class="token string">"conv9/dw"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">512</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    group<span class="token operator">:</span> <span class="token number">512</span>    engine<span class="token operator">:</span> CAFFE    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv9/dw/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv9/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv9/dw"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv9"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv9/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv9"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">512</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv9/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv9"</span>  top<span class="token operator">:</span> <span class="token string">"conv9"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv10/dw"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv9"</span>  top<span class="token operator">:</span> <span class="token string">"conv10/dw"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">512</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    group<span class="token operator">:</span> <span class="token number">512</span>    engine<span class="token operator">:</span> CAFFE    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv10/dw/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv10/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv10/dw"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv10"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv10/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv10"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">512</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv10/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv10"</span>  top<span class="token operator">:</span> <span class="token string">"conv10"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv11/dw"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv10"</span>  top<span class="token operator">:</span> <span class="token string">"conv11/dw"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">512</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    group<span class="token operator">:</span> <span class="token number">512</span>    engine<span class="token operator">:</span> CAFFE    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv11/dw/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv11/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv11/dw"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv11"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv11/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv11"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">512</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv11/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv11"</span>  top<span class="token operator">:</span> <span class="token string">"conv11"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv12/dw"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv11"</span>  top<span class="token operator">:</span> <span class="token string">"conv12/dw"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">512</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    stride<span class="token operator">:</span> <span class="token number">2</span>    group<span class="token operator">:</span> <span class="token number">512</span>    engine<span class="token operator">:</span> CAFFE    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv12/dw/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv12/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv12/dw"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv12"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv12/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv12"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">1024</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv12/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv12"</span>  top<span class="token operator">:</span> <span class="token string">"conv12"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv13/dw"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv12"</span>  top<span class="token operator">:</span> <span class="token string">"conv13/dw"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">1024</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    group<span class="token operator">:</span> <span class="token number">1024</span>    engine<span class="token operator">:</span> CAFFE    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv13/dw/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv13/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv13/dw"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv13"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv13/dw"</span>  top<span class="token operator">:</span> <span class="token string">"conv13"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">1024</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv13/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv13"</span>  top<span class="token operator">:</span> <span class="token string">"conv13"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv14_1"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv13"</span>  top<span class="token operator">:</span> <span class="token string">"conv14_1"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">256</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv14_1/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv14_1"</span>  top<span class="token operator">:</span> <span class="token string">"conv14_1"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv14_2"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv14_1"</span>  top<span class="token operator">:</span> <span class="token string">"conv14_2"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">512</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    stride<span class="token operator">:</span> <span class="token number">2</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv14_2/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv14_2"</span>  top<span class="token operator">:</span> <span class="token string">"conv14_2"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv15_1"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv14_2"</span>  top<span class="token operator">:</span> <span class="token string">"conv15_1"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">128</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv15_1/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv15_1"</span>  top<span class="token operator">:</span> <span class="token string">"conv15_1"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv15_2"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv15_1"</span>  top<span class="token operator">:</span> <span class="token string">"conv15_2"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">256</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    stride<span class="token operator">:</span> <span class="token number">2</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv15_2/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv15_2"</span>  top<span class="token operator">:</span> <span class="token string">"conv15_2"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv16_1"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv15_2"</span>  top<span class="token operator">:</span> <span class="token string">"conv16_1"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">128</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv16_1/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv16_1"</span>  top<span class="token operator">:</span> <span class="token string">"conv16_1"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv16_2"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv16_1"</span>  top<span class="token operator">:</span> <span class="token string">"conv16_2"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">256</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    stride<span class="token operator">:</span> <span class="token number">2</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv16_2/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv16_2"</span>  top<span class="token operator">:</span> <span class="token string">"conv16_2"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv17_1"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv16_2"</span>  top<span class="token operator">:</span> <span class="token string">"conv17_1"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">64</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv17_1/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv17_1"</span>  top<span class="token operator">:</span> <span class="token string">"conv17_1"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv17_2"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv17_1"</span>  top<span class="token operator">:</span> <span class="token string">"conv17_2"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">128</span>    pad<span class="token operator">:</span> <span class="token number">1</span>    kernel_size<span class="token operator">:</span> <span class="token number">3</span>    stride<span class="token operator">:</span> <span class="token number">2</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv17_2/relu"</span>  type<span class="token operator">:</span> <span class="token string">"ReLU"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv17_2"</span>  top<span class="token operator">:</span> <span class="token string">"conv17_2"</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv11_mbox_loc"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv11"</span>  top<span class="token operator">:</span> <span class="token string">"conv11_mbox_loc"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">12</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv11_mbox_loc_perm"</span>  type<span class="token operator">:</span> <span class="token string">"Permute"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv11_mbox_loc"</span>  top<span class="token operator">:</span> <span class="token string">"conv11_mbox_loc_perm"</span>  permute_param <span class="token punctuation">&#123;</span>    order<span class="token operator">:</span> <span class="token number">0</span>    order<span class="token operator">:</span> <span class="token number">2</span>    order<span class="token operator">:</span> <span class="token number">3</span>    order<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv11_mbox_loc_flat"</span>  type<span class="token operator">:</span> <span class="token string">"Flatten"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv11_mbox_loc_perm"</span>  top<span class="token operator">:</span> <span class="token string">"conv11_mbox_loc_flat"</span>  flatten_param <span class="token punctuation">&#123;</span>    axis<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv11_mbox_conf"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv11"</span>  top<span class="token operator">:</span> <span class="token string">"conv11_mbox_conf"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">63</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv11_mbox_conf_perm"</span>  type<span class="token operator">:</span> <span class="token string">"Permute"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv11_mbox_conf"</span>  top<span class="token operator">:</span> <span class="token string">"conv11_mbox_conf_perm"</span>  permute_param <span class="token punctuation">&#123;</span>    order<span class="token operator">:</span> <span class="token number">0</span>    order<span class="token operator">:</span> <span class="token number">2</span>    order<span class="token operator">:</span> <span class="token number">3</span>    order<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv11_mbox_conf_flat"</span>  type<span class="token operator">:</span> <span class="token string">"Flatten"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv11_mbox_conf_perm"</span>  top<span class="token operator">:</span> <span class="token string">"conv11_mbox_conf_flat"</span>  flatten_param <span class="token punctuation">&#123;</span>    axis<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv11_mbox_priorbox"</span>  type<span class="token operator">:</span> <span class="token string">"PriorBox"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv11"</span>  bottom<span class="token operator">:</span> <span class="token string">"data"</span>  top<span class="token operator">:</span> <span class="token string">"conv11_mbox_priorbox"</span>  prior_box_param <span class="token punctuation">&#123;</span>    min_size<span class="token operator">:</span> <span class="token number">60.0</span>    aspect_ratio<span class="token operator">:</span> <span class="token number">2.0</span>    flip<span class="token operator">:</span> <span class="token boolean">true</span>    clip<span class="token operator">:</span> <span class="token boolean">false</span>    variance<span class="token operator">:</span> <span class="token number">0.1</span>    variance<span class="token operator">:</span> <span class="token number">0.1</span>    variance<span class="token operator">:</span> <span class="token number">0.2</span>    variance<span class="token operator">:</span> <span class="token number">0.2</span>    offset<span class="token operator">:</span> <span class="token number">0.5</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv13_mbox_loc"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv13"</span>  top<span class="token operator">:</span> <span class="token string">"conv13_mbox_loc"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">24</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv13_mbox_loc_perm"</span>  type<span class="token operator">:</span> <span class="token string">"Permute"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv13_mbox_loc"</span>  top<span class="token operator">:</span> <span class="token string">"conv13_mbox_loc_perm"</span>  permute_param <span class="token punctuation">&#123;</span>    order<span class="token operator">:</span> <span class="token number">0</span>    order<span class="token operator">:</span> <span class="token number">2</span>    order<span class="token operator">:</span> <span class="token number">3</span>    order<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv13_mbox_loc_flat"</span>  type<span class="token operator">:</span> <span class="token string">"Flatten"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv13_mbox_loc_perm"</span>  top<span class="token operator">:</span> <span class="token string">"conv13_mbox_loc_flat"</span>  flatten_param <span class="token punctuation">&#123;</span>    axis<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv13_mbox_conf"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv13"</span>  top<span class="token operator">:</span> <span class="token string">"conv13_mbox_conf"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">126</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv13_mbox_conf_perm"</span>  type<span class="token operator">:</span> <span class="token string">"Permute"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv13_mbox_conf"</span>  top<span class="token operator">:</span> <span class="token string">"conv13_mbox_conf_perm"</span>  permute_param <span class="token punctuation">&#123;</span>    order<span class="token operator">:</span> <span class="token number">0</span>    order<span class="token operator">:</span> <span class="token number">2</span>    order<span class="token operator">:</span> <span class="token number">3</span>    order<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv13_mbox_conf_flat"</span>  type<span class="token operator">:</span> <span class="token string">"Flatten"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv13_mbox_conf_perm"</span>  top<span class="token operator">:</span> <span class="token string">"conv13_mbox_conf_flat"</span>  flatten_param <span class="token punctuation">&#123;</span>    axis<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv13_mbox_priorbox"</span>  type<span class="token operator">:</span> <span class="token string">"PriorBox"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv13"</span>  bottom<span class="token operator">:</span> <span class="token string">"data"</span>  top<span class="token operator">:</span> <span class="token string">"conv13_mbox_priorbox"</span>  prior_box_param <span class="token punctuation">&#123;</span>    min_size<span class="token operator">:</span> <span class="token number">105.0</span>    max_size<span class="token operator">:</span> <span class="token number">150.0</span>    aspect_ratio<span class="token operator">:</span> <span class="token number">2.0</span>    aspect_ratio<span class="token operator">:</span> <span class="token number">3.0</span>    flip<span class="token operator">:</span> <span class="token boolean">true</span>    clip<span class="token operator">:</span> <span class="token boolean">false</span>    variance<span class="token operator">:</span> <span class="token number">0.1</span>    variance<span class="token operator">:</span> <span class="token number">0.1</span>    variance<span class="token operator">:</span> <span class="token number">0.2</span>    variance<span class="token operator">:</span> <span class="token number">0.2</span>    offset<span class="token operator">:</span> <span class="token number">0.5</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_loc"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv14_2"</span>  top<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_loc"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">24</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_loc_perm"</span>  type<span class="token operator">:</span> <span class="token string">"Permute"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_loc"</span>  top<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_loc_perm"</span>  permute_param <span class="token punctuation">&#123;</span>    order<span class="token operator">:</span> <span class="token number">0</span>    order<span class="token operator">:</span> <span class="token number">2</span>    order<span class="token operator">:</span> <span class="token number">3</span>    order<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_loc_flat"</span>  type<span class="token operator">:</span> <span class="token string">"Flatten"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_loc_perm"</span>  top<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_loc_flat"</span>  flatten_param <span class="token punctuation">&#123;</span>    axis<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_conf"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv14_2"</span>  top<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_conf"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">126</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_conf_perm"</span>  type<span class="token operator">:</span> <span class="token string">"Permute"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_conf"</span>  top<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_conf_perm"</span>  permute_param <span class="token punctuation">&#123;</span>    order<span class="token operator">:</span> <span class="token number">0</span>    order<span class="token operator">:</span> <span class="token number">2</span>    order<span class="token operator">:</span> <span class="token number">3</span>    order<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_conf_flat"</span>  type<span class="token operator">:</span> <span class="token string">"Flatten"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_conf_perm"</span>  top<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_conf_flat"</span>  flatten_param <span class="token punctuation">&#123;</span>    axis<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_priorbox"</span>  type<span class="token operator">:</span> <span class="token string">"PriorBox"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv14_2"</span>  bottom<span class="token operator">:</span> <span class="token string">"data"</span>  top<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_priorbox"</span>  prior_box_param <span class="token punctuation">&#123;</span>    min_size<span class="token operator">:</span> <span class="token number">150.0</span>    max_size<span class="token operator">:</span> <span class="token number">195.0</span>    aspect_ratio<span class="token operator">:</span> <span class="token number">2.0</span>    aspect_ratio<span class="token operator">:</span> <span class="token number">3.0</span>    flip<span class="token operator">:</span> <span class="token boolean">true</span>    clip<span class="token operator">:</span> <span class="token boolean">false</span>    variance<span class="token operator">:</span> <span class="token number">0.1</span>    variance<span class="token operator">:</span> <span class="token number">0.1</span>    variance<span class="token operator">:</span> <span class="token number">0.2</span>    variance<span class="token operator">:</span> <span class="token number">0.2</span>    offset<span class="token operator">:</span> <span class="token number">0.5</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_loc"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv15_2"</span>  top<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_loc"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">24</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_loc_perm"</span>  type<span class="token operator">:</span> <span class="token string">"Permute"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_loc"</span>  top<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_loc_perm"</span>  permute_param <span class="token punctuation">&#123;</span>    order<span class="token operator">:</span> <span class="token number">0</span>    order<span class="token operator">:</span> <span class="token number">2</span>    order<span class="token operator">:</span> <span class="token number">3</span>    order<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_loc_flat"</span>  type<span class="token operator">:</span> <span class="token string">"Flatten"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_loc_perm"</span>  top<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_loc_flat"</span>  flatten_param <span class="token punctuation">&#123;</span>    axis<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_conf"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv15_2"</span>  top<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_conf"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">126</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_conf_perm"</span>  type<span class="token operator">:</span> <span class="token string">"Permute"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_conf"</span>  top<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_conf_perm"</span>  permute_param <span class="token punctuation">&#123;</span>    order<span class="token operator">:</span> <span class="token number">0</span>    order<span class="token operator">:</span> <span class="token number">2</span>    order<span class="token operator">:</span> <span class="token number">3</span>    order<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_conf_flat"</span>  type<span class="token operator">:</span> <span class="token string">"Flatten"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_conf_perm"</span>  top<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_conf_flat"</span>  flatten_param <span class="token punctuation">&#123;</span>    axis<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_priorbox"</span>  type<span class="token operator">:</span> <span class="token string">"PriorBox"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv15_2"</span>  bottom<span class="token operator">:</span> <span class="token string">"data"</span>  top<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_priorbox"</span>  prior_box_param <span class="token punctuation">&#123;</span>    min_size<span class="token operator">:</span> <span class="token number">195.0</span>    max_size<span class="token operator">:</span> <span class="token number">240.0</span>    aspect_ratio<span class="token operator">:</span> <span class="token number">2.0</span>    aspect_ratio<span class="token operator">:</span> <span class="token number">3.0</span>    flip<span class="token operator">:</span> <span class="token boolean">true</span>    clip<span class="token operator">:</span> <span class="token boolean">false</span>    variance<span class="token operator">:</span> <span class="token number">0.1</span>    variance<span class="token operator">:</span> <span class="token number">0.1</span>    variance<span class="token operator">:</span> <span class="token number">0.2</span>    variance<span class="token operator">:</span> <span class="token number">0.2</span>    offset<span class="token operator">:</span> <span class="token number">0.5</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_loc"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv16_2"</span>  top<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_loc"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">24</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_loc_perm"</span>  type<span class="token operator">:</span> <span class="token string">"Permute"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_loc"</span>  top<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_loc_perm"</span>  permute_param <span class="token punctuation">&#123;</span>    order<span class="token operator">:</span> <span class="token number">0</span>    order<span class="token operator">:</span> <span class="token number">2</span>    order<span class="token operator">:</span> <span class="token number">3</span>    order<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_loc_flat"</span>  type<span class="token operator">:</span> <span class="token string">"Flatten"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_loc_perm"</span>  top<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_loc_flat"</span>  flatten_param <span class="token punctuation">&#123;</span>    axis<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_conf"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv16_2"</span>  top<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_conf"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">126</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_conf_perm"</span>  type<span class="token operator">:</span> <span class="token string">"Permute"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_conf"</span>  top<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_conf_perm"</span>  permute_param <span class="token punctuation">&#123;</span>    order<span class="token operator">:</span> <span class="token number">0</span>    order<span class="token operator">:</span> <span class="token number">2</span>    order<span class="token operator">:</span> <span class="token number">3</span>    order<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_conf_flat"</span>  type<span class="token operator">:</span> <span class="token string">"Flatten"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_conf_perm"</span>  top<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_conf_flat"</span>  flatten_param <span class="token punctuation">&#123;</span>    axis<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_priorbox"</span>  type<span class="token operator">:</span> <span class="token string">"PriorBox"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv16_2"</span>  bottom<span class="token operator">:</span> <span class="token string">"data"</span>  top<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_priorbox"</span>  prior_box_param <span class="token punctuation">&#123;</span>    min_size<span class="token operator">:</span> <span class="token number">240.0</span>    max_size<span class="token operator">:</span> <span class="token number">285.0</span>    aspect_ratio<span class="token operator">:</span> <span class="token number">2.0</span>    aspect_ratio<span class="token operator">:</span> <span class="token number">3.0</span>    flip<span class="token operator">:</span> <span class="token boolean">true</span>    clip<span class="token operator">:</span> <span class="token boolean">false</span>    variance<span class="token operator">:</span> <span class="token number">0.1</span>    variance<span class="token operator">:</span> <span class="token number">0.1</span>    variance<span class="token operator">:</span> <span class="token number">0.2</span>    variance<span class="token operator">:</span> <span class="token number">0.2</span>    offset<span class="token operator">:</span> <span class="token number">0.5</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_loc"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv17_2"</span>  top<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_loc"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">24</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_loc_perm"</span>  type<span class="token operator">:</span> <span class="token string">"Permute"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_loc"</span>  top<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_loc_perm"</span>  permute_param <span class="token punctuation">&#123;</span>    order<span class="token operator">:</span> <span class="token number">0</span>    order<span class="token operator">:</span> <span class="token number">2</span>    order<span class="token operator">:</span> <span class="token number">3</span>    order<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_loc_flat"</span>  type<span class="token operator">:</span> <span class="token string">"Flatten"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_loc_perm"</span>  top<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_loc_flat"</span>  flatten_param <span class="token punctuation">&#123;</span>    axis<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_conf"</span>  type<span class="token operator">:</span> <span class="token string">"Convolution"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv17_2"</span>  top<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_conf"</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">1.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">1.0</span>  <span class="token punctuation">&#125;</span>  param <span class="token punctuation">&#123;</span>    lr_mult<span class="token operator">:</span> <span class="token number">2.0</span>    decay_mult<span class="token operator">:</span> <span class="token number">0.0</span>  <span class="token punctuation">&#125;</span>  convolution_param <span class="token punctuation">&#123;</span>    num_output<span class="token operator">:</span> <span class="token number">126</span>    kernel_size<span class="token operator">:</span> <span class="token number">1</span>    weight_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"msra"</span>    <span class="token punctuation">&#125;</span>    bias_filler <span class="token punctuation">&#123;</span>      type<span class="token operator">:</span> <span class="token string">"constant"</span>      value<span class="token operator">:</span> <span class="token number">0.0</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_conf_perm"</span>  type<span class="token operator">:</span> <span class="token string">"Permute"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_conf"</span>  top<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_conf_perm"</span>  permute_param <span class="token punctuation">&#123;</span>    order<span class="token operator">:</span> <span class="token number">0</span>    order<span class="token operator">:</span> <span class="token number">2</span>    order<span class="token operator">:</span> <span class="token number">3</span>    order<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_conf_flat"</span>  type<span class="token operator">:</span> <span class="token string">"Flatten"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_conf_perm"</span>  top<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_conf_flat"</span>  flatten_param <span class="token punctuation">&#123;</span>    axis<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_priorbox"</span>  type<span class="token operator">:</span> <span class="token string">"PriorBox"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv17_2"</span>  bottom<span class="token operator">:</span> <span class="token string">"data"</span>  top<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_priorbox"</span>  prior_box_param <span class="token punctuation">&#123;</span>    min_size<span class="token operator">:</span> <span class="token number">285.0</span>    max_size<span class="token operator">:</span> <span class="token number">300.0</span>    aspect_ratio<span class="token operator">:</span> <span class="token number">2.0</span>    aspect_ratio<span class="token operator">:</span> <span class="token number">3.0</span>    flip<span class="token operator">:</span> <span class="token boolean">true</span>    clip<span class="token operator">:</span> <span class="token boolean">false</span>    variance<span class="token operator">:</span> <span class="token number">0.1</span>    variance<span class="token operator">:</span> <span class="token number">0.1</span>    variance<span class="token operator">:</span> <span class="token number">0.2</span>    variance<span class="token operator">:</span> <span class="token number">0.2</span>    offset<span class="token operator">:</span> <span class="token number">0.5</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"mbox_loc"</span>  type<span class="token operator">:</span> <span class="token string">"Concat"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv11_mbox_loc_flat"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv13_mbox_loc_flat"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_loc_flat"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_loc_flat"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_loc_flat"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_loc_flat"</span>  top<span class="token operator">:</span> <span class="token string">"mbox_loc"</span>  concat_param <span class="token punctuation">&#123;</span>    axis<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"mbox_conf"</span>  type<span class="token operator">:</span> <span class="token string">"Concat"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv11_mbox_conf_flat"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv13_mbox_conf_flat"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_conf_flat"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_conf_flat"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_conf_flat"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_conf_flat"</span>  top<span class="token operator">:</span> <span class="token string">"mbox_conf"</span>  concat_param <span class="token punctuation">&#123;</span>    axis<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"mbox_priorbox"</span>  type<span class="token operator">:</span> <span class="token string">"Concat"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv11_mbox_priorbox"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv13_mbox_priorbox"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv14_2_mbox_priorbox"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv15_2_mbox_priorbox"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv16_2_mbox_priorbox"</span>  bottom<span class="token operator">:</span> <span class="token string">"conv17_2_mbox_priorbox"</span>  top<span class="token operator">:</span> <span class="token string">"mbox_priorbox"</span>  concat_param <span class="token punctuation">&#123;</span>    axis<span class="token operator">:</span> <span class="token number">2</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"mbox_conf_reshape"</span>  type<span class="token operator">:</span> <span class="token string">"Reshape"</span>  bottom<span class="token operator">:</span> <span class="token string">"mbox_conf"</span>  top<span class="token operator">:</span> <span class="token string">"mbox_conf_reshape"</span>  reshape_param <span class="token punctuation">&#123;</span>    shape <span class="token punctuation">&#123;</span>      dim<span class="token operator">:</span> <span class="token number">0</span>      dim<span class="token operator">:</span> <span class="token number">-1</span>      dim<span class="token operator">:</span> <span class="token number">21</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"mbox_conf_softmax"</span>  type<span class="token operator">:</span> <span class="token string">"Softmax"</span>  bottom<span class="token operator">:</span> <span class="token string">"mbox_conf_reshape"</span>  top<span class="token operator">:</span> <span class="token string">"mbox_conf_softmax"</span>  softmax_param <span class="token punctuation">&#123;</span>    axis<span class="token operator">:</span> <span class="token number">2</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"mbox_conf_flatten"</span>  type<span class="token operator">:</span> <span class="token string">"Flatten"</span>  bottom<span class="token operator">:</span> <span class="token string">"mbox_conf_softmax"</span>  top<span class="token operator">:</span> <span class="token string">"mbox_conf_flatten"</span>  flatten_param <span class="token punctuation">&#123;</span>    axis<span class="token operator">:</span> <span class="token number">1</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>layer <span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">"detection_out"</span>  type<span class="token operator">:</span> <span class="token string">"DetectionOutput"</span>  bottom<span class="token operator">:</span> <span class="token string">"mbox_loc"</span>  bottom<span class="token operator">:</span> <span class="token string">"mbox_conf_flatten"</span>  bottom<span class="token operator">:</span> <span class="token string">"mbox_priorbox"</span>  top<span class="token operator">:</span> <span class="token string">"detection_out"</span>  include <span class="token punctuation">&#123;</span>    phase<span class="token operator">:</span> TEST  <span class="token punctuation">&#125;</span>  detection_output_param <span class="token punctuation">&#123;</span>    num_classes<span class="token operator">:</span> <span class="token number">21</span>    share_location<span class="token operator">:</span> <span class="token boolean">true</span>    background_label_id<span class="token operator">:</span> <span class="token number">0</span>    nms_param <span class="token punctuation">&#123;</span>      nms_threshold<span class="token operator">:</span> <span class="token number">0.45</span>      top_k<span class="token operator">:</span> <span class="token number">100</span>    <span class="token punctuation">&#125;</span>    code_type<span class="token operator">:</span> CENTER_SIZE    keep_top_k<span class="token operator">:</span> <span class="token number">100</span>    confidence_threshold<span class="token operator">:</span> <span class="token number">0.25</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-模型下载"><a href="#4-模型下载" class="headerlink" title="4.模型下载"></a>4.模型下载</h3><p>MobileNetSSD_deploy.caffemodel: <a href="https://share.weiyun.com/Wi04sqO7">https://share.weiyun.com/Wi04sqO7</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object_Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习术语表</title>
      <link href="/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/"/>
      <url>/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/</url>
      
        <content type="html"><![CDATA[<h2 id="机器学习术语表"><a href="#机器学习术语表" class="headerlink" title="机器学习术语表"></a>机器学习术语表</h2><p>本术语表整理谷歌收录词条，以Ａ-Z的首字母进行排序。<br>下面列出了一般的机器学习术语和 TensorFlow 专用术语的定义。</p><h3 id="A"><a href="#A" class="headerlink" title="A"></a>A</h3><h4 id="A-B-测试-A-B-testing"><a href="#A-B-测试-A-B-testing" class="headerlink" title="A/B 测试 (A/B testing)"></a>A/B 测试 (A/B testing)</h4><p>一种统计方法，用于将两种或多种技术进行比较，通常是将当前采用的技术与新技术进行比较。A/B 测试不仅旨在确定哪种技术的效果更好，而且还有助于了解相应差异是否具有显著的统计意义。A/B 测试通常是采用一种衡量方式对两种技术进行比较，但也适用于任意有限数量的技术和衡量方式。</p><h4 id="准确率-accuracy"><a href="#准确率-accuracy" class="headerlink" title="准确率 (accuracy)"></a>准确率 (accuracy)</h4><p>[<strong>分类模型</strong>]的正确预测所占的比例。在[<strong>多类别分类</strong>]中，准确率的定义如下：<br>$$<br>准确率 = \frac{正确的预测数}{样本总数}<br>$$<br>准确率正确的预测数样本总数准确率=正确的预测数样本总数</p><p>在<strong>二元分类</strong>中，准确率的定义如下：<br>$$<br>准确率 = \frac{正例数+负例数}{样本总数}<br>$$<br>准确率正例数负例数样本总数准确率=正例数+负例数样本总数</p><p>请参阅[<strong>正例</strong>]和[<strong>负例</strong>]。</p><h4 id="激活函数-activation-function"><a href="#激活函数-activation-function" class="headerlink" title="激活函数 (activation function)"></a>激活函数 (activation function)</h4><p>一种函数（例如 [<strong>ReLU</strong>]或 [<strong>S 型函数</strong>]），用于对上一层的所有输入求加权和，然后生成一个输出值（通常为非线性值），并将其传递给下一层。</p><h4 id="主动学习-Active-learning"><a href="#主动学习-Active-learning" class="headerlink" title="主动学习(Active learning)"></a>主动学习(Active learning)</h4><p>一种算法从学习的数据中选择一些数据的训练方法。当有标记的例子很少或很难获得时，主动学习特别有价值。主动学习算法不是盲目地寻找各种各样的有标记的例子，而是选择性地寻找它需要学习的特定范围的例子。</p><h4 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h4><p>一种先进的梯度下降法，用于重新调整每个参数的梯度，以便有效地为每个参数指定独立的<a href="https://developers.google.com/machine-learning/glossary/#learning_rate"><strong>学习速率</strong></a>。如需查看完整的解释，请参阅<a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">这篇论文</a>。</p><h4 id="层次聚类-agglomerative-clustering"><a href="#层次聚类-agglomerative-clustering" class="headerlink" title="层次聚类(agglomerative clustering)"></a>层次聚类(agglomerative clustering)</h4><p>请参阅[<strong>hierarchical clustering</strong>]。</p><h4 id="增强现实-AR"><a href="#增强现实-AR" class="headerlink" title="增强现实(AR)"></a>增强现实(AR)</h4><p>Augmented Reality的缩写，简称AR。<br>一种将计算机生成的图像叠加在用户对真实世界的视图上，从而提供合成视图的技术。</p><h4 id="PR曲线下的区域-Area-under-the-PR-curve"><a href="#PR曲线下的区域-Area-under-the-PR-curve" class="headerlink" title="PR曲线下的区域(Area under the PR curve)"></a>PR曲线下的区域(Area under the PR curve)</h4><p>通过对分类阈值的不同值绘制(召回率，精度)点，得到的插值精度-召回率曲线下的区域。根据计算方法的不同，PR AUC可能等价于模型的平均精度。</p><h4 id="ROC-曲线下面积-AUC-Area-under-the-ROC-Curve"><a href="#ROC-曲线下面积-AUC-Area-under-the-ROC-Curve" class="headerlink" title="ROC 曲线下面积 (AUC, Area under the ROC Curve)"></a>ROC 曲线下面积 (AUC, Area under the ROC Curve)</h4><p>一种会考虑所有可能[<strong>分类阈值</strong>]的评估指标。</p><p>[ROC 曲线]下面积是，对于随机选择的正类别样本确实为正类别，以及随机选择的负类别样本为正类别，分类器更确信前者的概率。换言之，分类器更有可能确信随机选择的正例实际上是正的概率，而不是随机选择的负例是正的概率。</p><h4 id="通用人工智能-artificial-general-intelligence"><a href="#通用人工智能-artificial-general-intelligence" class="headerlink" title="通用人工智能(artificial general intelligence)"></a>通用人工智能(artificial general intelligence)</h4><p>一种非人机制，表现出广泛的问题解决能力、创造力和适应性。例如，一个演示人工通用智能的程序可以翻译文本、谱写交响乐，并擅长于尚未发明的游戏。</p><h4 id="人工智能-artificial-intelligence"><a href="#人工智能-artificial-intelligence" class="headerlink" title="人工智能(artificial intelligence)"></a>人工智能(artificial intelligence)</h4><p>可以解决复杂任务的非人类程序或模型。例如，翻译文本的程序或模型或从放射线图像识别疾病的程序或模型都表现出人工智能。</p><p>正式地，[<strong>机器学习</strong>]是人工智能的一个子领域。但是，近年来，一些组织已开始互换使用<em>人工智能</em>和<em>机器学习</em>这两个术语。</p><h4 id="属性-attribute"><a href="#属性-attribute" class="headerlink" title="属性(attribute)"></a>属性(attribute)</h4><p>Feature特征的同义词。公平地说，属性通常指与个人有关的特征。</p><h4 id="自动化偏差-automation-bias"><a href="#自动化偏差-automation-bias" class="headerlink" title="自动化偏差(automation bias)"></a>自动化偏差(automation bias)</h4><p>当人类决策者偏向自动化决策系统提出的建议胜过非自动化决策时，即使自动化决策系统出错了。</p><h4 id="平均精度-average-precision"><a href="#平均精度-average-precision" class="headerlink" title="平均精度(average precision)"></a>平均精度(average precision)</h4><p>汇总排名结果序列的效果的指标。平均精度是通过取每个相关结果的<a href="https://developers.google.com/machine-learning/glossary#precision"><strong>精度</strong></a>值的平均值 （排名列表中每个结果的召回率相对于先前结果而言）来计算的。</p><p>另请参见[<strong>PR曲线下的面积</strong>]。</p><hr><h3 id="B"><a href="#B" class="headerlink" title="B"></a>B</h3><h4 id="反向传播算法-backpropagation"><a href="#反向传播算法-backpropagation" class="headerlink" title="反向传播算法 (backpropagation)"></a>反向传播算法 (backpropagation)</h4><p>在[<strong>神经网络</strong>]上执行[<strong>梯度下降法</strong>]的主要算法。该算法会先按前向传播方式计算（并缓存）每个节点的输出值，然后再按反向传播遍历图的方式计算损失函数值相对于每个参数的[偏导数]。</p><h4 id="词袋-Bag-of-words模型"><a href="#词袋-Bag-of-words模型" class="headerlink" title="词袋(Bag of words模型)"></a>词袋(Bag of words模型)</h4><p>短语或段落中单词的表示形式，与顺序无关。例如，词袋表示以下三个相同地短语：</p><ul><li>the dog jumps</li><li>jumps the dog</li><li>dog jumps the</li></ul><p>每个单词都映射到[<strong>稀疏向量</strong>]的索引，其中向量对词汇表中的每个单词都有一个索引。例如，将<em>dog dog跳跃</em>的短语映射到在对应于单词<em>the</em>，<em>dog</em>和 <em>jumps</em>的三个索引处具有非零值的特征向量。非零值可以是以下任意值：</p><ul><li>1表示单词的存在。</li><li>一个单词出现在书包中的次数计数。例如，如果短语<em>“栗色狗”是带有栗色毛皮的狗</em>，则 <em>栗色</em>和<em>dog</em>都将表示为2，而其他单词将表示为1。</li><li>其他一些值，例如单词出现在书包中的次数的对数。</li></ul><h4 id="基准-baseline"><a href="#基准-baseline" class="headerlink" title="基准 (baseline)"></a>基准 (baseline)</h4><p>一种简单的[<strong>模型</strong>]或启发法，用作比较模型效果时的参考点。<br>基准有助于模型开发者针对特定问题量化最低预期效果。</p><h4 id="批次-batch"><a href="#批次-batch" class="headerlink" title="批次 (batch)"></a>批次 (batch)</h4><p>[<strong>模型训练</strong>]的一次[<strong>迭代</strong>]（即一次[<strong>梯度</strong>]更新）中使用的样本集。</p><p>另请参阅[<strong>批次大小</strong>]。</p><h4 id="批次归一化"><a href="#批次归一化" class="headerlink" title="批次归一化"></a>批次归一化</h4><p>在 [<strong>隐藏层中</strong>]　[<strong>归一化</strong>]　[<strong>激活函数</strong>]的输入或输出 。批量规范化可以提供以下好处：</p><ul><li><p>通过防止[<strong>离群值</strong>]权重使[<strong>神经网络</strong>]更稳定。</p></li><li><p>提高[<strong>学习率</strong>]。</p></li><li><p>减少[<strong>过拟合</strong>]。</p></li></ul><h4 id="批次大小-batch-size"><a href="#批次大小-batch-size" class="headerlink" title="批次大小 (batch size)"></a>批次大小 (batch size)</h4><p>一个<a href="https://developers.google.com/machine-learning/glossary/#batch"><strong>批次</strong></a>中的样本数。例如，<a href="https://developers.google.com/machine-learning/glossary/#SGD"><strong>SGD</strong></a> 的批次大小为 1，而<a href="https://developers.google.com/machine-learning/glossary/#mini-batch"><strong>小批次</strong></a>的大小通常介于 10 到 1000 之间。批次大小在训练和推断期间通常是固定的；不过，TensorFlow 允许使用动态批次大小。</p><h4 id="贝叶斯神经网络"><a href="#贝叶斯神经网络" class="headerlink" title="贝叶斯神经网络"></a>贝叶斯神经网络</h4><p>一种考虑[<strong>权重</strong>]和输出不确定性的概率[<strong>神经网络</strong>]。一个标准的神经网络回归模型通常[<strong>预测</strong>]一个标量值;<br>例如，一个模型预测房价为853,000。相比之下，贝叶斯神经网络预测值的分布;例如，一个模型预测的房价是853,000，标准差是67200。贝叶斯神经网络依靠贝叶斯定理来计算权重和预测中的不确定性。贝叶斯神经网络在对不确定性进行量化时很有用，比如在与药品相关的模型中。贝叶斯神经网络也有助于防止[<strong>过拟合</strong>]。</p><h4 id="偏差-bias"><a href="#偏差-bias" class="headerlink" title="偏差 (bias)"></a>偏差 (bias)</h4><p>距离原点的截距或偏移。偏差（也称为<strong>偏差项</strong>）在机器学习模型中用 b 或 w0 表示。例如，在下面的公式中，偏差为 b：<br>$$<br>y’ = b + w_1x_1 + w_2x_2 + … + w_nx_n<br>$$<br>请勿与[<strong>预测偏差</strong>]混淆。</p><h4 id="二元分类-binary-classification"><a href="#二元分类-binary-classification" class="headerlink" title="二元分类 (binary classification)"></a>二元分类 (binary classification)</h4><p>一种分类任务，可输出两种互斥类别之一。例如，对电子邮件进行评估并输出“垃圾邮件”或“非垃圾邮件”的机器学习模型就是一个二元分类器。</p><h4 id="边界框-bounding-box"><a href="#边界框-bounding-box" class="headerlink" title="边界框(bounding box)"></a>边界框(bounding box)</h4><p>在图像中，一个感兴趣区域周围的矩形(x, y)坐标，例如下图中的狗。</p><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/1571518-20210519162650299-447450425.png" style="zoom: 50%;"><h4 id="分箱-binning"><a href="#分箱-binning" class="headerlink" title="分箱 (binning)"></a>分箱 (binning)</h4><p>请参阅<a href="https://developers.google.com/machine-learning/glossary/#bucketing"><strong>分桶</strong></a>。</p><h4 id="分桶-bucketing"><a href="#分桶-bucketing" class="headerlink" title="分桶 (bucketing)"></a>分桶 (bucketing)</h4><p>将一个特征（通常是[<strong>连续</strong>]特征）转换成多个二元特征（称为桶或箱），通常根据值区间进行转换。例如，您可以将温度区间分割为离散分箱，而不是将温度表示成单个连续的浮点特征。假设温度数据可精确到小数点后一位，则可以将介于 0.0 到 15.0 度之间的所有温度都归入一个分箱，将介于 15.1 到 30.0 度之间的所有温度归入第二个分箱，并将介于 30.1 到 50.0 度之间的所有温度归入第三个分箱。</p><hr><h3 id="C"><a href="#C" class="headerlink" title="C"></a>C</h3><h4 id="校准层-calibration-layer"><a href="#校准层-calibration-layer" class="headerlink" title="校准层 (calibration layer)"></a>校准层 (calibration layer)</h4><p>一种预测后调整，通常是为了降低[<strong>预测偏差</strong>]的影响。<br>调整后的预测和概率应与观察到的标签集的分布一致。</p><h4 id="候选集-candidate-generation"><a href="#候选集-candidate-generation" class="headerlink" title="候选集(candidate generation)"></a>候选集(candidate generation)</h4><p>推荐系统所选择的初始推荐集。例如，考虑一家提供10万种图书的书店。候选生成阶段为特定用户创建一个更小的适合的图书列表，比如500本。但即使是500本书也太多了，无法向用户推荐。随后的推荐系统阶段(如评分和重新排名)会将这500个推荐缩减为一个更小、更有用的推荐集。</p><h4 id="候选采样-candidate-sampling"><a href="#候选采样-candidate-sampling" class="headerlink" title="候选采样 (candidate sampling)"></a>候选采样 (candidate sampling)</h4><p>一种训练时进行的优化，会使用某种函数（例如 softmax）针对所有正类别标签计算概率，但对于负类别标签，则仅针对其随机样本计算概率。<br>例如，如果某个样本的标签为“小猎犬”和“狗”，则候选采样将针对“小猎犬”和“狗”类别输出以及其他类别（猫、棒棒糖、栅栏）的随机子集计算预测概率和相应的损失项。<br>这种采样基于的想法是，只要[<strong>正类别</strong>]始终得到适当的正增强，[<strong>负类别</strong>]就可以从频率较低的负增强中进行学习，这确实是在实际中观察到的情况。候选采样的目的是，通过不针对所有负类别计算预测结果来提高计算效率。</p><h4 id="分类数据-categorical-data"><a href="#分类数据-categorical-data" class="headerlink" title="分类数据 (categorical data)"></a>分类数据 (categorical data)</h4><p>具有一组离散的可能值的[<strong>特征</strong>]。<br>以某个名为 <code>house style</code> 的分类特征为例，该特征拥有一组离散的可能值（共三个），即 <code>Tudor, ranch, colonial</code>。通过将 <code>house style</code> 表示成分类数据，相应模型可以学习 <code>Tudor</code>、<code>ranch</code> 和 <code>colonial</code> 分别对房价的影响。</p><p>有时，离散集中的值是互斥的，只能将其中一个值应用于指定样本。<br>例如，<code>car maker</code> 分类特征可能只允许一个样本有一个值 (<code>Toyota</code>)。<br>在其他情况下，则可以应用多个值。一辆车可能会被喷涂多种不同的颜色，因此，<code>car color</code> 分类特征可能会允许单个样本具有多个值（例如 <code>red</code> 和 <code>white</code>）。</p><p>分类特征有时称为[<strong>离散特征</strong>]。<br>与[<strong>数值数据</strong>]相对。</p><h4 id="形心-centroid"><a href="#形心-centroid" class="headerlink" title="形心 (centroid)"></a>形心 (centroid)</h4><p>聚类的中心，由 [<strong>k-means</strong>]或 [<strong>k-median</strong>]算法决定。<br>例如，如果 k 为 3，则 k-means 或 k-median 算法会找出 3 个形心。</p><h4 id="基于质心的聚类-Centroid-based-clustering"><a href="#基于质心的聚类-Centroid-based-clustering" class="headerlink" title="基于质心的聚类(Centroid-based clustering)"></a>基于质心的聚类(Centroid-based clustering)</h4><p>一种聚类算法，将数据组织成非层次的聚类。K-means是应用最广泛的基于质心的聚类算法。</p><p>对比分层聚类算法。</p><h4 id="检查点-checkpoint"><a href="#检查点-checkpoint" class="headerlink" title="检查点 (checkpoint)"></a>检查点 (checkpoint)</h4><p>用于捕获模型变量在特定时间的状态的一种数据。<br>借助检查点，可以导出模型[<strong>权重</strong>]，跨多个会话执行训练，以及使训练在发生错误之后得以继续（例如作业抢占）。请注意，[<strong>图</strong>]本身不包含在检查点中。</p><h4 id="类别-class"><a href="#类别-class" class="headerlink" title="类别 (class)"></a>类别 (class)</h4><p>为标签枚举的一组目标值中的一个。例如，在检测垃圾邮件的[<strong>二元分类</strong>]模型中，两种类别分别是“垃圾邮件”和“非垃圾邮件”。在识别狗品种的[<strong>多类别分类</strong>]模型中，类别可以是“贵宾犬”、“小猎犬”、“哈巴犬”等等。</p><h4 id="分类不平衡的数据集-class-imbalanced-data-set"><a href="#分类不平衡的数据集-class-imbalanced-data-set" class="headerlink" title="分类不平衡的数据集 (class-imbalanced data set)"></a>分类不平衡的数据集 (class-imbalanced data set)</h4><p>一种[<strong>二元分类</strong>]问题，在此类问题中，两种类别的[<strong>标签</strong>]在出现频率方面具有很大的差距。<br>例如，在某个疾病数据集中，0.0001 的样本具有正类别标签，0.9999 的样本具有负类别标签，这就属于分类不平衡问题；但在某个足球比赛预测器中，0.51 的样本的标签为其中一个球队赢，0.49 的样本的标签为另一个球队赢，这就不属于分类不平衡问题。</p><h4 id="分类模型-classification-model"><a href="#分类模型-classification-model" class="headerlink" title="分类模型 (classification model)"></a>分类模型 (classification model)</h4><p>一种机器学习模型，用于区分两种或多种离散类别。例如，某个自然语言处理分类模型可以确定输入的句子是法语、西班牙语还是意大利语。请与[<strong>回归模型</strong>]进行比较。</p><h4 id="分类阈值-classification-threshold"><a href="#分类阈值-classification-threshold" class="headerlink" title="分类阈值 (classification threshold)"></a>分类阈值 (classification threshold)</h4><p>一种标量值条件，应用于模型预测的得分，旨在将[<strong>正类别</strong>]与[<strong>负类别</strong>]区分开。<br>将[<strong>逻辑回归</strong>]结果映射到[<strong>二元分类</strong>]时使用。以某个逻辑回归模型为例，该模型用于确定指定电子邮件是垃圾邮件的概率。如果分类阈值为 0.9，那么逻辑回归值高于 0.9 的电子邮件将被归类为“垃圾邮件”，低于 0.9 的则被归类为“非垃圾邮件”。</p><h4 id="剪裁-clipping"><a href="#剪裁-clipping" class="headerlink" title="剪裁(clipping)"></a>剪裁(clipping)</h4><p>一种处理异常值的技术。具体来说，就是将大于某一设定最大值的特性值减少到该最大值。同时，将小于某一特定最小值的特征值增加到该最小值。</p><p>例如，假设只有少数特性值位于40-60范围之外。在这种情况下，您可以执行以下操作:</p><ul><li><p>将所有超过60的值剪辑为60。</p></li><li><p>将40以下的所有值剪辑为40。</p></li></ul><p>除了将输入值带入指定范围外，裁剪还可以用于在训练期间强制在指定范围内设置渐变值。</p><h4 id="Cloud-TPU"><a href="#Cloud-TPU" class="headerlink" title="Cloud TPU"></a>Cloud TPU</h4><p>一个专门的硬件加速器，旨在加快谷歌云平台上的机器学习工作负载。</p><h4 id="聚类-clustering"><a href="#聚类-clustering" class="headerlink" title="聚类 (clustering)"></a>聚类 (clustering)</h4><p>将关联的[<strong>样本</strong>]分成一组，一般用于[<strong>非监督式学习</strong>]。在所有样本均分组完毕后，相关人员便可选择性地为每个聚类赋予含义。</p><p>聚类算法有很多。例如，[<strong>k-means</strong>]算法会基于样本与[<strong>形心</strong>]的接近程度聚类样本，如下图所示：</p><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/1571518-20210519071533560-1296281202.png" style="zoom: 50%;"><p>之后，研究人员便可查看这些聚类并进行其他操作，例如，将聚类 1 标记为“矮型树”，将聚类 2 标记为“全尺寸树”。<br>再举一个例子，例如基于样本与中心点距离的聚类算法，如下所示：</p><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/1571518-20210519071604091-1420676558.png" style="zoom: 50%;"><h4 id="协同适应-co-adaptation"><a href="#协同适应-co-adaptation" class="headerlink" title="协同适应(co-adaptation)"></a>协同适应(co-adaptation)</h4><p>神经元预测训练数据中的模式时，几乎完全依赖于其他特定神经元的输出，而不是依赖于整个网络的行为。<br>当导致共同适应的模式在验证数据中不存在时，协同适应就会导致过拟合。Dropout正则化减少了协同适应，因为Dropout确保神经元不能单独依赖于特定的其他神经元。</p><h4 id="协同过滤-collaborative-filtering"><a href="#协同过滤-collaborative-filtering" class="headerlink" title="协同过滤(collaborative filtering)"></a>协同过滤(collaborative filtering)</h4><p>根据许多其他用户的兴趣对一个用户的兴趣进行预测。协同过滤在推荐系统中经常使用。</p><h4 id="确认偏误-Confirmation-Bias"><a href="#确认偏误-Confirmation-Bias" class="headerlink" title="确认偏误(Confirmation Bias)"></a>确认偏误(<em>Confirmation</em> <em>Bias</em>)</h4><p>一种寻找、解释、偏好和回忆信息的倾向，以确认一个人先前存在的信念或假设。机器学习开发人员可能会无意中收集或标记数据，从而影响支持他们现有信念的结果。确认偏见是内隐偏见的一种形式。</p><p>实验者的偏见是一种确认偏见，即实验者继续训练模型，直到先前存在的假设被证实。</p><h4 id="混淆矩阵-confusion-matrix"><a href="#混淆矩阵-confusion-matrix" class="headerlink" title="混淆矩阵 (confusion matrix)"></a>混淆矩阵 (confusion matrix)</h4><p>一种 NxN 表格，用于总结[<strong>分类模型</strong>]的预测效果；即标签和模型预测的分类之间的关联。在混淆矩阵中，一个轴表示模型预测的标签，另一个轴表示实际标签。N 表示类别个数。在[<strong>二元分类</strong>]问题中，N=2。例如，下面显示了一个二元分类问题的混淆矩阵示例</p><table><thead><tr><th></th><th align="center">肿瘤（预测的标签）</th><th align="center">非肿瘤（预测的标签）</th></tr></thead><tbody><tr><td>肿瘤（实际标签）</td><td align="center">18</td><td align="center">1</td></tr><tr><td>非肿瘤（实际标签）</td><td align="center">6</td><td align="center">452</td></tr></tbody></table><p>上面的混淆矩阵显示，在 19 个实际有肿瘤的样本中，该模型正确地将 18 个归类为有肿瘤（18 个正例），错误地将 1 个归类为没有肿瘤（1 个假负例）。同样，在 458 个实际没有肿瘤的样本中，模型归类正确的有 452 个（452 个负例），归类错误的有 6 个（6 个假正例）。</p><p>多类别分类问题的混淆矩阵有助于确定出错模式。例如，某个混淆矩阵可以揭示，某个经过训练以识别手写数字的模型往往会将 4 错误地预测为 9，将 7 错误地预测为 1。</p><p>混淆矩阵包含计算各种效果指标（包括[<strong>精确率</strong>]和[<strong>召回率</strong>]）所需的充足信息。</p><h4 id="连续特征-continuous-feature"><a href="#连续特征-continuous-feature" class="headerlink" title="连续特征 (continuous feature)"></a>连续特征 (continuous feature)</h4><p>一种浮点特征，可能值的区间不受限制。与[<strong>离散特征</strong>]相对。</p><h4 id="随机抽样-Convenience-Sampling"><a href="#随机抽样-Convenience-Sampling" class="headerlink" title="随机抽样(Convenience Sampling)"></a>随机抽样(Convenience Sampling)</h4><p>使用未经科学收集的数据集来进行快速实验。之后，有必要切换到科学收集的数据集。</p><h4 id="收敛-convergence"><a href="#收敛-convergence" class="headerlink" title="收敛 (convergence)"></a>收敛 (convergence)</h4><p>通俗来说，收敛通常是指在训练期间达到的一种状态，即经过一定次数的迭代之后，训练[<strong>损失</strong>]和验证损失在每次迭代中的变化都非常小或根本没有变化。也就是说，如果采用当前数据进行额外的训练将无法改进模型，模型即达到收敛状态。在深度学习中，损失值有时会在最终下降之前的多次迭代中保持不变或几乎保持不变，暂时形成收敛的假象。</p><p>另请参阅[<strong>早停法</strong>。</p><p>另请参阅 Boyd 和 Vandenberghe 合著的 <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a>（《凸优化》）。</p><h4 id="凸函数-convex-function"><a href="#凸函数-convex-function" class="headerlink" title="凸函数 (convex function)"></a>凸函数 (convex function)</h4><p>一种函数，函数图像以上的区域为[<strong>凸集</strong>]。典型凸函数的形状类似于字母 <strong>U</strong>。例如，以下都是凸函数：</p><p><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9C%AF%E8%AF%AD%E8%A1%A8/1571518-20210519072107033-2112350803.png"></p><p>相反，以下函数则不是凸函数。请注意图像上方的区域如何不是凸集：</p><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/1571518-20210519072127464-1927828349.png" style="zoom:50%;"><p>严格凸函数只有一个局部最低点，该点也是全局最低点。经典的 U 形函数都是严格凸函数。不过，有些凸函数（例如直线）则不是这样。</p><p>很多常见的[<strong>损失函数</strong>]（包括下列函数）都是凸函数：</p><ul><li>[<strong>L2 损失函数</strong>]</li><li>[<strong>对数损失函数</strong>]</li><li>[<strong>L1 正则化</strong>]</li><li>[<strong>L2 正则化</strong>]</li></ul><p>[<strong>梯度下降法</strong>]的很多变体都一定能找到一个接近严格凸函数最小值的点。<br>同样，[<strong>随机梯度下降法</strong>]的很多变体都有很高的可能性能够找到接近严格凸函数最小值的点（但并非一定能找到）。</p><p>两个凸函数的和（例如 L2 损失函数 + L1 正则化）也是凸函数。</p><p>[<strong>深度模型</strong>]绝不会是凸函数。<br>值得注意的是，专门针对[<strong>凸优化</strong>]设计的算法往往总能在深度网络上找到非常好的解决方案，虽然这些解决方案并不一定对应于全局最小值。</p><h4 id="凸优化-convex-optimization"><a href="#凸优化-convex-optimization" class="headerlink" title="凸优化 (convex optimization)"></a>凸优化 (convex optimization)</h4><p>使用数学方法（例如[<strong>梯度下降法</strong>]寻找[<strong>凸函数</strong>]最小值的过程。<br>机器学习方面的大量研究都是专注于如何通过公式将各种问题表示成凸优化问题，以及如何更高效地解决这些问题。</p><p>如需完整的详细信息，请参阅 Boyd 和 Vandenberghe 合著的 <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a>（《凸优化》）。</p><h4 id="凸集-convex-set"><a href="#凸集-convex-set" class="headerlink" title="凸集 (convex set)"></a>凸集 (convex set)</h4><p>欧几里得空间的一个子集，其中任意两点之间的连线仍完全落在该子集内。例如，下面的两个图形都是凸集：<br><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/1571518-20210519072513653-561955074.png" style="zoom:80%;"><br>相反，下面的两个图形都不是凸集：<br><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/1571518-20210519072536904-1929909153.png" style="zoom:80%;"></p><h4 id="卷积-convolution"><a href="#卷积-convolution" class="headerlink" title="卷积 (convolution)"></a>卷积 (convolution)</h4><p>简单来说，卷积在数学中指两个函数的组合。在机器学习中，卷积结合使用卷积过滤器和输入矩阵来训练权重。</p><p>机器学习中的“卷积”一词通常是[<strong>卷积运算</strong>]或[<strong>卷积层</strong>]的简称。</p><p>如果没有卷积，机器学习算法就需要学习大张量中每个单元格各自的权重。<br>例如，用 2K x 2K 图像训练的机器学习算法将被迫找出 400 万个单独的权重。<br>而使用卷积，机器学习算法只需在[<strong>卷积过滤器</strong>]中找出每个单元格的权重，大大减少了训练模型所需的内存。<br>在应用卷积过滤器后，它只需跨单元格进行复制，每个单元格都会与过滤器相乘。</p><h4 id="卷积过滤器-convolutional-filter"><a href="#卷积过滤器-convolutional-filter" class="headerlink" title="卷积过滤器 (convolutional filter)"></a>卷积过滤器 (convolutional filter)</h4><p>[<strong>卷积运算</strong>]中的两个参与方之一。（另一个参与方是输入矩阵切片。）<br>卷积过滤器是一种矩阵，其[<strong>等级</strong>]与输入矩阵相同，但形状小一些。以 28×28 的输入矩阵为例，过滤器可以是小于 28×28 的任何二维矩阵。</p><p>在图形操作中，卷积过滤器中的所有单元格通常按照固定模式设置为 1 和 0。在机器学习中，卷积过滤器通常先选择随机数字，然后由网络训练出理想值。</p><h4 id="卷积层-convolutional-layer"><a href="#卷积层-convolutional-layer" class="headerlink" title="卷积层 (convolutional layer)"></a>卷积层 (convolutional layer)</h4><p>深度神经网络的一个层，[<strong>卷积过滤器</strong>]会在其中传递输入矩阵。<br>以下面的 3x3 [<strong>卷积过滤器</strong>]为例：</p><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/1571518-20210519072849968-1039794844.png" style="zoom: 67%;"><p>下面的动画显示了一个由 9 个卷积运算（涉及 5x5 输入矩阵）组成的卷积层。请注意，每个卷积运算都涉及一个不同的 3x3 输入矩阵切片。由此产生的 3×3 矩阵（右侧）就包含 9 个卷积运算的结果：</p><p><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9C%AF%E8%AF%AD%E8%A1%A8/1571518-20210519073016112-527565287.gif"></p><h4 id="卷积神经网络-convolutional-neural-network"><a href="#卷积神经网络-convolutional-neural-network" class="headerlink" title="卷积神经网络 (convolutional neural network)"></a>卷积神经网络 (convolutional neural network)</h4><p>一种神经网络，其中至少有一层为[<strong>卷积层</strong>]。典型的卷积神经网络包含以下几层的组合：</p><ul><li>卷积层</li><li>池化层</li><li>密集层</li></ul><p>卷积神经网络在解决某些类型的问题（如图像识别）上取得了巨大成功。</p><h4 id="卷积运算-convolutional-operation"><a href="#卷积运算-convolutional-operation" class="headerlink" title="卷积运算 (convolutional operation)"></a>卷积运算 (convolutional operation)</h4><p>如下所示的两步数学运算：</p><ol><li>对[<strong>卷积过滤器</strong>]和输入矩阵切片执行元素级乘法。（输入矩阵切片与卷积过滤器具有相同的等级和大小。）</li><li>对生成的积矩阵中的所有值求和。</li></ol><p>以下面的 5x5 输入矩阵为例</p><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/1571518-20210519074746692-201592030.png" style="zoom: 67%;">现在，以下面这个 2x2 卷积过滤器为例：<img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/1571518-20210519074814242-928398171.png" style="zoom: 67%;"><p>每个卷积运算都涉及一个 2x2 输入矩阵切片。例如，假设我们使用输入矩阵左上角的 2x2 切片。这样一来，对此切片进行卷积运算将如下所示：</p><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/1571518-20210519080944331-2116578709.png" style="zoom: 80%;"><p>[<strong>卷积层</strong>]由一系列卷积运算组成，每个卷积运算都针对不同的输入矩阵切片。</p><h4 id="成本-cost"><a href="#成本-cost" class="headerlink" title="成本 (cost)"></a>成本 (cost)</h4><p>与[<strong>损失</strong>]的含义相同。</p><h4 id="交叉熵-cross-entropy"><a href="#交叉熵-cross-entropy" class="headerlink" title="交叉熵 (cross-entropy)"></a>交叉熵 (cross-entropy)</h4><p>[<strong>对数损失函数</strong>]向[<strong>多类别分类问题</strong>]的一种泛化。</p><p>交叉熵可以量化两种概率分布之间的差异。</p><p>另请参阅[<strong>困惑度</strong>]。</p><h4 id="交叉验证-cross-validation"><a href="#交叉验证-cross-validation" class="headerlink" title="交叉验证(cross-validation)"></a>交叉验证(cross-validation)</h4><p>通过对保留在训练集中的一个或多个非重叠数据子集测试模型，来估计模型如何很好地推广到新数据的一种机制。</p><h4 id="自定义-Estimator-custom-Estimator"><a href="#自定义-Estimator-custom-Estimator" class="headerlink" title="自定义 Estimator (custom Estimator)"></a>自定义 Estimator (custom Estimator)</h4><p>您按照<a href="https://www.tensorflow.org/extend/estimators">这些说明</a>自行编写的 [<strong>Estimator</strong>]。</p><p>与[<strong>预创建的 Estimator</strong>]相对。</p><h3 id="D"><a href="#D" class="headerlink" title="D"></a>D</h3><h4 id="数据分析-data-analysis"><a href="#数据分析-data-analysis" class="headerlink" title="数据分析 (data analysis)"></a>数据分析 (data analysis)</h4><p>根据样本、测量结果和可视化内容来理解数据。数据分析在首次收到数据集、构建第一个模型之前特别有用。此外，数据分析在理解实验和调试系统问题方面也至关重要。</p><h4 id="数据增强-data-augmentation"><a href="#数据增强-data-augmentation" class="headerlink" title="数据增强(data augmentation)"></a>数据增强(data augmentation)</h4><p>通过将现有示例转换为创建额外示例，人为地提高训练示例的范围和数量。例如，假设图像是您的特性之一，但您的数据集没有包含足够的图像示例，因此模型无法学习有用的关联。理想情况下，您应该向您的数据集添加足够的标记图像，以使您的模型能够正确地训练。如果这是不可能的，数据增强可以旋转、拉伸和反射每一张图像来产生原始图像的许多变体，可能产生足够的标记数据来进行出色的训练。</p><h4 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h4><p>一种热门的数据类型，用于表示 Pandas 中的数据集。DataFrame 类似于表格。DataFrame 的每一列都有一个名称（标题），每一行都由一个数字标识。</p><h4 id="数据集-data-set"><a href="#数据集-data-set" class="headerlink" title="数据集 (data set)"></a>数据集 (data set)</h4><p>一组[<strong>样本</strong>]的集合。</p><h4 id="Dataset-API-tf-data"><a href="#Dataset-API-tf-data" class="headerlink" title="Dataset API (tf.data)"></a>Dataset API (tf.data)</h4><p>一种高级别的 TensorFlow API，用于读取数据并将其转换为机器学习算法所需的格式。<br><code>tf.data.Dataset</code> 对象表示一系列元素，其中每个元素都包含一个或多个[<strong>张量</strong>]。<br><code>tf.data.Iterator</code> 对象可获取 <code>Dataset</code> 中的元素。</p><p>如需详细了解 Dataset API，请参阅《TensorFlow 编程人员指南》中的[导入数据]。</p><h4 id="决策边界-decision-boundary"><a href="#决策边界-decision-boundary" class="headerlink" title="决策边界 (decision boundary)"></a>决策边界 (decision boundary)</h4><p>在[<strong>二元分类</strong>]或[<strong>多类别分类问题</strong>]中，模型学到的类别之间的分界线。例如，在以下表示某个二元分类问题的图片中，决策边界是橙色类别和蓝色类别之间的分界线：</p><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/1571518-20210519110308213-810938903.png" style="zoom: 67%;"><h4 id="决策阈值-decision-threshold"><a href="#决策阈值-decision-threshold" class="headerlink" title="决策阈值(decision threshold)"></a>决策阈值(decision threshold)</h4><p>同义词为[<strong>分类阈值</strong>]。</p><h4 id="决策树-decision-tree"><a href="#决策树-decision-tree" class="headerlink" title="决策树(decision tree)"></a>决策树(decision tree)</h4><p>用分支语句序列表示的模型。例如，下面这个过于简化的决策树通过几次分叉来预测房子的价格(以几千美元为单位)。根据这棵决策树，一栋面积超过160平方米、有三个以上卧室、建造时间不到10年的房子，预计价格为51万美元。<br><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9C%AF%E8%AF%AD%E8%A1%A8/1571518-20210519165648046-628953210.png"></p><p>机器学习可以生成深度决策树。</p><h4 id="深度模型-deep-model"><a href="#深度模型-deep-model" class="headerlink" title="深度模型(deep model)"></a>深度模型(deep model)</h4><p>一种包含多个隐含层的神经网络。</p><p>与宽模式形成对比。</p><h4 id="深度-depth"><a href="#深度-depth" class="headerlink" title="深度(depth)"></a>深度(depth)</h4><p>神经网络中学习权重的的层数(包括任何嵌入层)。例如，有5个隐含层和1个输出层的神经网络的深度为6。</p><h4 id="维度-dimensions"><a href="#维度-dimensions" class="headerlink" title="维度(dimensions)"></a>维度(dimensions)</h4><p>具有以下任何定义的重载术语：</p><ul><li><p>[<strong>张量</strong>]的坐标级数。例如：</p><ul><li>标量的尺寸为零；标量的尺寸为0。例如，<code>[&quot;Hello&quot;]</code>。</li><li>向量具有一维。例如，<code>[3, 5, 7, 11]</code>。</li><li>矩阵有两个维度。例如，<code>[[2, 4, 18], [5, 7, 14]]</code>。</li></ul><p>您可以在具有一个坐标的一维向量中唯一地指定特定单元格；您需要两个坐标才能在二维矩阵中唯一指定特定单元格。</p></li><li><p>[<strong>特征向量</strong>]的条目数。</p></li><li><p>[<strong>嵌入</strong>]层中的元素数。</p></li></ul><h4 id="降采样-downsampling"><a href="#降采样-downsampling" class="headerlink" title="降采样(downsampling)"></a>降采样(downsampling)</h4><p>重载术语可能表示以下任一情况：</p><ul><li>减少要素中的信息量，以便更有效地训练模型。例如，在训练图像识别模型之前，先将高分辨率图像降采样为较低分辨率格式。</li><li>培训比例过高的课堂实例比例过低，以改进代表性不足的课程的模型培训。例如，在一个[<strong>类别不平衡的数据集中</strong>]，模型倾向于了解很多关于 [<strong>多数类别的知识</strong>]，而对于[<strong>少数类别的知识则</strong>]不够 。下采样有助于平衡多数和少数类的培训量。</li></ul><h4 id="密集特征-dense-feature"><a href="#密集特征-dense-feature" class="headerlink" title="密集特征 (dense feature)"></a>密集特征 (dense feature)</h4><p>一种大部分值是非零值的[<strong>特征</strong>]，通常是浮点值[<strong>张量</strong>]。与[<strong>稀疏特征</strong>]相对。｜</p><h4 id="离散特征-discrete-feature"><a href="#离散特征-discrete-feature" class="headerlink" title="离散特征 (discrete feature)"></a>离散特征 (discrete feature)</h4><p>一种[<strong>特征</strong>]，包含有限个可能值。例如，某个值只能是“动物”、“蔬菜”或“矿物”的特征便是一个离散特征（或分类特征）。与[<strong>连续特征</strong>]相对。</p><h4 id="设备-device"><a href="#设备-device" class="headerlink" title="设备 (device)"></a>设备 (device)</h4><p>一类可运行 TensorFlow 会话的硬件，包括 CPU、GPU 和 TPU。</p><h4 id="丢弃正则化-dropout-regularization"><a href="#丢弃正则化-dropout-regularization" class="headerlink" title="丢弃正则化 (dropout regularization)"></a>丢弃正则化 (dropout regularization)</h4><p>[<strong>正则化</strong>]的一种形式，在训练[<strong>神经网络</strong>]方面非常有用。丢弃正则化的运作机制是，在一个梯度步长中移除从神经网络层中随机选择的固定数量的单元。丢弃的单元越多，正则化效果就越强。这类似于训练神经网络以模拟较小网络的指数级规模集成学习。如需完整的详细信息，请参阅 <a href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>（《丢弃：一种防止神经网络过拟合的简单方法》）。</p><h4 id="动态模型-dynamic-model"><a href="#动态模型-dynamic-model" class="headerlink" title="动态模型 (dynamic model)"></a>动态模型 (dynamic model)</h4><p>一种[<strong>模型</strong>]，以持续更新的方式在线接受训练。也就是说，数据会源源不断地进入这种模型。</p><h3 id="E"><a href="#E" class="headerlink" title="E"></a>E</h3><h4 id="早停法-early-stopping"><a href="#早停法-early-stopping" class="headerlink" title="早停法 (early stopping)"></a>早停法 (early stopping)</h4><p>一种[<strong>正则化</strong>]方法，是指在训练损失仍可以继续降低之前结束模型训练。<br>使用早停法时，您会在[<strong>验证数据集</strong>]的损失开始增大（也就是[<strong>泛化</strong>]效果变差）时结束模型训练。</p><h4 id="嵌套-embeddings"><a href="#嵌套-embeddings" class="headerlink" title="嵌套 (embeddings)"></a>嵌套 (embeddings)</h4><p>一种分类特征，以连续值特征表示。通常，嵌套是指将高维度向量映射到低维度的空间。例如，您可以采用以下两种方式之一来表示英文句子中的单词：</p><ul><li>表示成包含百万个元素（高维度）的[<strong>稀疏向量</strong>]，其中所有元素都是整数。向量中的每个单元格都表示一个单独的英文单词，单元格中的值表示相应单词在句子中出现的次数。由于单个英文句子包含的单词不太可能超过 50 个，因此向量中几乎每个单元格都包含 0。少数非 0 的单元格中将包含一个非常小的整数（通常为 1），该整数表示相应单词在句子中出现的次数。</li><li>表示成包含数百个元素（低维度）的[<strong>密集向量</strong>]，其中每个元素都存储一个介于 0 到 1 之间的浮点值。这就是一种嵌套。</li></ul><p>在 TensorFlow 中，会按[反向传播] [损失]训练嵌套，和训练[<strong>神经网络</strong>]中的任何其他参数一样。</p><h4 id="经验风险最小化-ERM-empirical-risk-minimization"><a href="#经验风险最小化-ERM-empirical-risk-minimization" class="headerlink" title="经验风险最小化 (ERM, empirical risk minimization)"></a>经验风险最小化 (ERM, empirical risk minimization)</h4><p>用于选择可以将基于训练集的损失降至最低的函数。与[<strong>结构风险最小化</strong>]相对。</p><h4 id="集成学习-ensemble"><a href="#集成学习-ensemble" class="headerlink" title="集成学习 (ensemble)"></a>集成学习 (ensemble)</h4><p>多个[<strong>模型</strong>]的预测结果的并集。您可以通过以下一项或多项来创建集成学习：</p><ul><li>不同的初始化</li><li>不同的[<strong>超参数</strong>]</li><li>不同的整体结构</li></ul><p>[深度模型和宽度模型]属于一种集成学习。</p><h4 id="周期-epoch"><a href="#周期-epoch" class="headerlink" title="周期 (epoch)"></a>周期 (epoch)</h4><p>在训练时，整个数据集的一次完整遍历，以便不漏掉任何一个样本。<br>因此，一个周期表示（<code>N</code>/[<strong>批次大小</strong>]）次训练[<strong>迭代</strong>]，其中 <code>N</code> 是样本总数。</p><h4 id="Estimator"><a href="#Estimator" class="headerlink" title="Estimator"></a>Estimator</h4><p><code>tf.Estimator</code> 类的一个实例，用于封装负责构建 TensorFlow 图并运行 TensorFlow 会话的逻辑。<br>您可以创建[<strong>自定义 Estimator</strong>]（如需相关介绍，请<a href="https://www.tensorflow.org/extend/estimators">点击此处</a>），也可以实例化其他人<a href="https://developers.google.com/machine-learning/glossary/#pre-made_Estimator"><strong>预创建的 Estimator</strong></a>。</p><h4 id="样本-example"><a href="#样本-example" class="headerlink" title="样本 (example)"></a>样本 (example)</h4><p>数据集的一行。一个样本包含一个或多个[<strong>特征</strong>]，此外还可能包含一个<a href="https://developers.google.com/machine-learning/glossary/#label"><strong>标签</strong></a>。</p><p>另请参阅[<strong>有标签样本</strong>]和[<strong>无标签样本</strong>]。</p><h3 id="F"><a href="#F" class="headerlink" title="F"></a>F</h3><h4 id="假负例-FN-false-negative"><a href="#假负例-FN-false-negative" class="headerlink" title="假负例 (FN, false negative)"></a>假负例 (FN, false negative)</h4><p>被模型错误地预测为[<strong>负类别</strong>]的样本。<br>例如，模型推断出某封电子邮件不是垃圾邮件（负类别），但该电子邮件其实是垃圾邮件。</p><h4 id="假正例-FP-false-positive"><a href="#假正例-FP-false-positive" class="headerlink" title="假正例 (FP, false positive)"></a>假正例 (FP, false positive)</h4><p>被模型错误地预测为<a href="https://developers.google.com/machine-learning/glossary/#positive_class"><strong>正类别</strong></a>的样本。例如，模型推断出某封电子邮件是垃圾邮件（正类别），但该电子邮件其实不是垃圾邮件。</p><h4 id="假正例率（false-positive-rate-简称-FP-率）"><a href="#假正例率（false-positive-rate-简称-FP-率）" class="headerlink" title="假正例率（false positive rate, 简称 FP 率）"></a>假正例率（false positive rate, 简称 FP 率）</h4><p>[<strong>ROC 曲线</strong>]中的 x 轴。FP 率的定义如下：<br>$$<br>假正例率 = \frac{假正例数}{假正例数+负例数} \<br>FP率 = \frac{FP}{FP + FN}<br>$$</p><h4 id="特征-feature"><a href="#特征-feature" class="headerlink" title="特征 (feature)"></a>特征 (feature)</h4><p>在进行[<strong>预测</strong>]时使用的输入变量。</p><h4 id="特征列-tf-feature-column"><a href="#特征列-tf-feature-column" class="headerlink" title="特征列 (tf.feature_column)"></a>特征列 (tf.feature_column)</h4><p>指定模型应该如何解读特定特征的一种函数。此类函数的输出结果是所有 <a href="https://developers.google.com/machine-learning/glossary/#Estimators"><strong>Estimators</strong></a> 构造函数的必需参数。</p><p>借助 <code>tf.feature_column</code> 函数，模型可对输入特征的不同表示法轻松进行实验。<br>有关详情，请参阅《TensorFlow 编程人员指南》中的[特征列]一章。</p><p>“特征列”是 Google 专用的术语。特征列在 Yahoo/Microsoft 使用的 <a href="https://en.wikipedia.org/wiki/Vowpal_Wabbit">VW</a> 系统中称为“命名空间”，也称为<a href="https://www.csie.ntu.edu.tw/~cjlin/libffm/">场</a>。</p><h4 id="特征组合-feature-cross"><a href="#特征组合-feature-cross" class="headerlink" title="特征组合 (feature cross)"></a>特征组合 (feature cross)</h4><p>通过将单独的特征进行组合（求笛卡尔积）而形成的[<strong>合成特征</strong>]。<br>特征组合有助于表达非线性关系。</p><h4 id="特征工程-feature-engineering"><a href="#特征工程-feature-engineering" class="headerlink" title="特征工程 (feature engineering)"></a>特征工程 (feature engineering)</h4><p>指以下过程：确定哪些[<strong>特征</strong>]可能在训练模型方面非常有用，然后将日志文件及其他来源的原始数据转换为所需的特征。在 TensorFlow 中，特征工程通常是指将原始日志文件条目转换为 [<strong>tf.Example</strong>]协议缓冲区。另请参阅 <a href="https://github.com/tensorflow/transform">tf.Transform</a>。</p><p>特征工程有时称为<strong>特征提取</strong>。</p><h4 id="特征集-feature-set"><a href="#特征集-feature-set" class="headerlink" title="特征集 (feature set)"></a>特征集 (feature set)</h4><p>训练机器学习模型时采用的一组[<strong>特征</strong>]。例如，对于某个用于预测房价的模型，邮政编码、房屋面积以及房屋状况可以组成一个简单的特征集。</p><h4 id="特征规范-feature-spec"><a href="#特征规范-feature-spec" class="headerlink" title="特征规范 (feature spec)"></a>特征规范 (feature spec)</h4><p>用于描述如何从 <a href="https://developers.google.com/machine-learning/glossary/#tf.Example"><strong>tf.Example</strong></a> 协议缓冲区提取[<strong>特征</strong>]数据。由于 tf.Example 协议缓冲区只是一个数据容器，因此您必须指定以下内容：</p><ul><li>要提取的数据（即特征的键）</li><li>数据类型（例如 float 或 int）</li><li>长度（固定或可变）</li></ul><p>[<strong>Estimator API</strong>]提供了一些可用来根据给定 [<strong>FeatureColumns</strong>]列表生成特征规范的工具。</p><h4 id="少量样本学习-few-shot-learning"><a href="#少量样本学习-few-shot-learning" class="headerlink" title="少量样本学习 (few-shot learning)"></a>少量样本学习 (few-shot learning)</h4><p>一种机器学习方法（通常用于对象分类），旨在仅通过少量训练样本学习有效的分类器。</p><p>另请参阅[<strong>单样本学习</strong>]。</p><h4 id="完整-softmax-full-softmax"><a href="#完整-softmax-full-softmax" class="headerlink" title="完整 softmax (full softmax)"></a>完整 softmax (full softmax)</h4><p>请参阅 [<strong>softmax</strong>]。与[<strong>候选采样</strong>]相对。</p><h4 id="全连接层-fully-connected-layer"><a href="#全连接层-fully-connected-layer" class="headerlink" title="全连接层 (fully connected layer)"></a>全连接层 (fully connected layer)</h4><p>一种[<strong>隐藏层</strong>]，其中的每个[<strong>节点</strong>]均与下一个隐藏层中的每个节点相连。</p><p>全连接层又称为[<strong>密集层</strong>]。</p><h3 id="G"><a href="#G" class="headerlink" title="G"></a>G</h3><h4 id="泛化-generalization"><a href="#泛化-generalization" class="headerlink" title="泛化 (generalization)"></a>泛化 (generalization)</h4><p>指的是模型依据训练时采用的数据，针对以前未见过的新数据做出正确预测的能力。</p><h4 id="广义线性模型-generalized-linear-model"><a href="#广义线性模型-generalized-linear-model" class="headerlink" title="广义线性模型 (generalized linear model)"></a>广义线性模型 (generalized linear model)</h4><p>[<strong>最小二乘回归</strong>]模型（基于[高斯噪声]）向其他类型的模型（基于其他类型的噪声，例如<a href="https://en.wikipedia.org/wiki/Shot_noise">泊松噪声</a>或分类噪声）进行的一种泛化。广义线性模型的示例包括：</p><ul><li>[<strong>逻辑回归</strong>]</li><li>多类别回归</li><li>最小二乘回归</li></ul><p>可以通过[凸优化]找到广义线性模型的参数。</p><p>广义线性模型具有以下特性：</p><ul><li>最优的最小二乘回归模型的平均预测结果等于训练数据的平均标签。</li><li>最优的逻辑回归模型预测的平均概率等于训练数据的平均标签。</li></ul><p>广义线性模型的功能受其特征的限制。与深度模型不同，广义线性模型无法“学习新特征”。</p><h4 id="梯度-gradient"><a href="#梯度-gradient" class="headerlink" title="梯度 (gradient)"></a>梯度 (gradient)</h4><p>[<strong>偏导数</strong>]相对于所有自变量的向量。在机器学习中，梯度是模型函数偏导数的向量。梯度指向最高速上升的方向。</p><h4 id="梯度裁剪-gradient-clipping"><a href="#梯度裁剪-gradient-clipping" class="headerlink" title="梯度裁剪 (gradient clipping)"></a>梯度裁剪 (gradient clipping)</h4><p>在应用[<strong>梯度</strong>]值之前先设置其上限。<br>梯度裁剪有助于确保数值稳定性以及防止[梯度爆炸]。</p><h4 id="梯度下降法-gradient-descent"><a href="#梯度下降法-gradient-descent" class="headerlink" title="梯度下降法 (gradient descent)"></a>梯度下降法 (gradient descent)</h4><p>一种通过计算并且减小梯度将[<strong>损失</strong>]降至最低的技术，它以训练数据为条件，来计算损失相对于模型参数的梯度。通俗来说，梯度下降法以迭代方式调整参数，逐渐找到[<strong>权重</strong>]和偏差的最佳组合，从而将损失降至最低。</p><h4 id="图-graph"><a href="#图-graph" class="headerlink" title="图 (graph)"></a>图 (graph)</h4><p>TensorFlow 中的一种计算规范。图中的节点表示操作。<br>边缘具有方向，表示将某项操作的结果（一个[张量]）作为一个操作数传递给另一项操作。<br>可以使用 <a href><strong>TensorBoard</strong></a> 直观呈现图。</p><h3 id="H"><a href="#H" class="headerlink" title="H"></a>H</h3><h4 id="启发法-heuristic"><a href="#启发法-heuristic" class="headerlink" title="启发法 (heuristic)"></a>启发法 (heuristic)</h4><p>一种非最优但实用的问题解决方案，足以用于进行改进或从中学习。</p><h4 id="隐藏层-hidden-layer"><a href="#隐藏层-hidden-layer" class="headerlink" title="隐藏层 (hidden layer)"></a>隐藏层 (hidden layer)</h4><p>[<strong>神经网络</strong>]中的合成层，介于[<strong>输入层</strong>]（即特征）和[<strong>输出层</strong>]（即预测）之间。<br>神经网络包含一个或多个隐藏层。</p><h4 id="合页损失函数-hinge-loss"><a href="#合页损失函数-hinge-loss" class="headerlink" title="合页损失函数 (hinge loss)"></a>合页损失函数 (hinge loss)</h4><p>一系列用于[<strong>分类</strong>]的[<strong>损失</strong>]函数，旨在找到距离每个训练样本都尽可能远的[<strong>决策边界</strong>]，从而使样本和边界之间的裕度最大化。 [<strong>KSVM</strong>]使用合页损失函数（或相关函数，例如平方合页损失函数）。对于二元分类，合页损失函数的定义如下：<br>$$<br>loss = max(0, 1- (y’*y))<br>$$<br>其中”y”表示分类器模型的原始输出：<br>$$<br>y’ = b + w_1x_1 + w_2x_2 + … + w_nx_n<br>$$<br>“y”表示真标签，值为 -1 或 +1。</p><p>因此，合页损失与 (y * y’) 的关系图如下所示：</p><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/1571518-20210519133921272-2065342172.png" style="zoom: 50%;"><h4 id="维持数据-holdout-data"><a href="#维持数据-holdout-data" class="headerlink" title="维持数据 (holdout data)"></a>维持数据 (holdout data)</h4><p>训练期间故意不使用（“维持”）的[<strong>样本</strong>]。<br>[<strong>验证数据集</strong>]和[<strong>测试数据集</strong>]都属于维持数据。<br>维持数据有助于评估模型向训练时所用数据之外的数据进行泛化的能力。<br>与基于训练数据集的损失相比，基于维持数据集的损失有助于更好地估算基于未见过的数据集的损失。</p><h4 id="超参数-hyperparameter"><a href="#超参数-hyperparameter" class="headerlink" title="超参数 (hyperparameter)"></a>超参数 (hyperparameter)</h4><p>在模型训练的连续过程中，您调节的“旋钮”。<br>例如，[<strong>学习速率</strong>]就是一种超参数。与[<strong>参数</strong>]相对。</p><h4 id="超平面-hyperplane"><a href="#超平面-hyperplane" class="headerlink" title="超平面 (hyperplane)"></a>超平面 (hyperplane)</h4><p>将一个空间划分为两个子空间的边界。<br>例如，在二维空间中，直线就是一个超平面，在三维空间中，平面则是一个超平面。<br>在机器学习中更典型的是：超平面是分隔高维度空间的边界。<br>[<strong>核支持向量机</strong>]利用超平面将正类别和负类别区分开来（通常是在极高维度空间中）。</p><h3 id="I"><a href="#I" class="headerlink" title="I"></a>I</h3><h4 id="独立同等分布-i-i-d-independently-and-identically-distributed"><a href="#独立同等分布-i-i-d-independently-and-identically-distributed" class="headerlink" title="独立同等分布 (i.i.d, independently and identically distributed)"></a>独立同等分布 (i.i.d, independently and identically distributed)</h4><p>从不会改变的分布中提取的数据，其中提取的每个值都不依赖于之前提取的值。<br>i.i.d. 是机器学习的<a href="https://en.wikipedia.org/wiki/Ideal_gas">理想气体</a> - 一种实用的数学结构，但在现实世界中几乎从未发现过。<br>例如，某个网页的访问者在短时间内的分布可能为 i.i.d.，即分布在该短时间内没有变化，且一位用户的访问行为通常与另一位用户的访问行为无关。不过，如果将时间窗口扩大，网页访问者的分布可能呈现出季节性变化。</p><h4 id="推断-inference"><a href="#推断-inference" class="headerlink" title="推断 (inference)"></a>推断 (inference)</h4><p>在机器学习中，推断通常指以下过程：通过将训练过的模型应用于[<strong>无标签样本</strong>]来做出预测。<br>在统计学中，推断是指在某些观测数据条件下拟合分布参数的过程。<br>（请参阅<a href="https://en.wikipedia.org/wiki/Statistical_inference">维基百科中有关统计学推断的文章</a>。）</p><h4 id="输入函数-input-function"><a href="#输入函数-input-function" class="headerlink" title="输入函数 (input function)"></a>输入函数 (input function)</h4><p>在 TensorFlow 中，用于将输入数据返回到 [<strong>Estimator</strong>](的训练、评估或预测方法的函数。<br>例如，训练输入函数会返回[<strong>训练集</strong>]中的[<strong>一批</strong>]特征和标签。</p><h4 id="输入层-input-layer"><a href="#输入层-input-layer" class="headerlink" title="输入层 (input layer)"></a>输入层 (input layer)</h4><p>[<strong>神经网络</strong>]中的第一层（接收输入数据的层）。</p><h4 id="实例-instance"><a href="#实例-instance" class="headerlink" title="实例 (instance)"></a>实例 (instance)</h4><p>与[<strong>样本</strong>]的含义相同。</p><h4 id="可解释性-interpretability"><a href="#可解释性-interpretability" class="headerlink" title="可解释性 (interpretability)"></a>可解释性 (interpretability)</h4><p>模型的预测可解释的难易程度。深度模型通常不可解释，也就是说，很难对深度模型的不同层进行解释。<br>相比之下，线性回归模型和[<strong>宽度模型</strong>]的可解释性通常要好得多。</p><h4 id="评分者间一致性信度-inter-rater-agreement"><a href="#评分者间一致性信度-inter-rater-agreement" class="headerlink" title="评分者间一致性信度 (inter-rater agreement)"></a>评分者间一致性信度 (inter-rater agreement)</h4><p>一种衡量指标，用于衡量在执行某项任务时评分者达成一致的频率。<br>如果评分者未达成一致，则可能需要改进任务说明。有时也称为<strong>注释者间一致性信度</strong>或<strong>评分者间可靠性信度</strong>。<br>另请参阅 <a href="https://en.wikipedia.org/wiki/Cohen's_kappa">Cohen’s kappa</a>（最热门的评分者间一致性信度衡量指标之一）。</p><h4 id="迭代-iteration"><a href="#迭代-iteration" class="headerlink" title="迭代 (iteration)"></a>迭代 (iteration)</h4><p>模型的权重在训练期间的一次更新。<br>迭代包含计算参数在单[<strong>批次</strong>]数据上的梯度损失。</p><h3 id="K"><a href="#K" class="headerlink" title="K"></a>K</h3><h4 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h4><p>一种热门的[<strong>聚类</strong>]算法，用于对非监督式学习中的样本进行分组。k-means 算法基本上会执行以下操作：</p><ul><li>以迭代方式确定最佳的 k 中心点（称为[<strong>形心</strong>]）。</li><li>将每个样本分配到最近的形心。与同一个形心距离最近的样本属于同一个组。</li></ul><p>k-means 算法会挑选形心位置，以最大限度地减小每个样本与其最接近形心之间的距离的累积平方。</p><p>以下面的小狗高度与小狗宽度的关系图为例：</p><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/1571518-20210519135331320-1862124757.png" style="zoom:50%;"><p>如果 k=3，则 k-means 算法会确定三个形心。每个样本都被分配到与其最接近的形心，最终产生三个组：</p><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/1571518-20210519135408861-92266783.png" style="zoom:50%;"><p>假设制造商想要确定小、中和大号狗毛衣的理想尺寸。在该聚类中，三个形心用于标识每只狗的平均高度和平均宽度。因此，制造商可能应该根据这三个形心确定毛衣尺寸。请注意，聚类的形心通常不是聚类中的样本。</p><p>上图显示了 k-means 应用于仅具有两个特征（高度和宽度）的样本。请注意，k-means 可以跨多个特征为样本分组。</p><h4 id="k-median"><a href="#k-median" class="headerlink" title="k-median"></a>k-median</h4><p>与 [<strong>k-means</strong>]紧密相关的聚类算法。两者的实际区别如下：</p><ul><li>对于 k-means，确定形心的方法是，最大限度地减小候选形心与它的每个样本之间的距离平方和。</li><li>对于 k-median，确定形心的方法是，最大限度地减小候选形心与它的每个样本之间的距离总和。</li></ul><p>请注意，距离的定义也有所不同：</p><ul><li>k-means 采用从形心到样本的[欧几里得距离]。（在二维空间中，欧几里得距离即使用勾股定理来计算斜边。）例如，(2,2) 与 (5,-2) 之间的 k-means 距离为：</li></ul><p>$$<br>欧几里德距离=\sqrt{(2-5)^2 + (2 - (-2))^2} = 5<br>$$</p><ul><li>k-median 采用从形心到样本的[曼哈顿距离]。这个距离是每个维度中绝对差异值的总和。例如，(2,2) 与 (5,-2) 之间的 k-median 距离为：<br>$$<br>曼哈顿距离 = |2-5|+|2 - (-2)| = 7<br>$$</li></ul><h4 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a>Keras</h4><p>一种热门的 Python 机器学习 API。<br><a href="https://keras.io/">Keras</a> 能够在多种深度学习框架上运行，其中包括 TensorFlow（在该框架上，Keras 作为 <a href="https://www.tensorflow.org/api_docs/python/tf/keras"><strong>tf.keras</strong></a> 提供）。</p><h4 id="核支持向量机-KSVM-Kernel-Support-Vector-Machines"><a href="#核支持向量机-KSVM-Kernel-Support-Vector-Machines" class="headerlink" title="核支持向量机 (KSVM, Kernel Support Vector Machines)"></a>核支持向量机 (KSVM, Kernel Support Vector Machines)</h4><p>一种分类算法，旨在通过将输入数据向量映射到更高维度的空间，来最大化[<strong>正类别</strong>]和[<strong>负类别</strong>]之间的裕度。<br>以某个输入数据集包含一百个特征的分类问题为例。<br>为了最大化正类别和负类别之间的裕度，KSVM 可以在内部将这些特征映射到百万维度的空间。<br>KSVM 使用[合页损失函数]。</p><h3 id="L"><a href="#L" class="headerlink" title="L"></a>L</h3><h4 id="L1-损失函数-L₁-loss"><a href="#L1-损失函数-L₁-loss" class="headerlink" title="L1 损失函数 (L₁ loss)"></a>L1 损失函数 (L₁ loss)</h4><p>一种[<strong>损失</strong>]函数，基于模型预测的值与[<strong>标签</strong>]的实际值之差的绝对值。<br>与 [<strong>L2 损失函数</strong>]相比，L1 损失函数对离群值的敏感性弱一些。</p><h4 id="L1-正则化-L₁-regularization"><a href="#L1-正则化-L₁-regularization" class="headerlink" title="L1 正则化 (L₁ regularization)"></a>L1 正则化 (L₁ regularization)</h4><p>一种[<strong>正则化</strong>]，根据权重的绝对值的总和来惩罚权重。<br>在依赖[<strong>稀疏特征</strong>]的模型中，L1 正则化有助于使不相关或几乎不相关的特征的权重正好为 0，从而将这些特征从模型中移除。与 [<strong>L2 正则化</strong>]相对。</p><h4 id="L2-损失函数-L₂-loss"><a href="#L2-损失函数-L₂-loss" class="headerlink" title="L2 损失函数 (L₂ loss)"></a>L2 损失函数 (L₂ loss)</h4><p>请参阅[<strong>平方损失函数</strong>]。</p><h4 id="L2-正则化-L₂-regularization"><a href="#L2-正则化-L₂-regularization" class="headerlink" title="L2 正则化 (L₂ regularization)"></a>L2 正则化 (L₂ regularization)</h4><p>一种[<strong>正则化</strong>]，根据权重的平方和来惩罚权重。<br>L2 正则化有助于使离群值（具有较大正值或较小负值）权重接近于 0，但又不正好为 0。（与 [<strong>L1 正则化</strong>]相对。）在线性模型中，L2 正则化始终可以改进泛化。</p><h4 id="标签-label"><a href="#标签-label" class="headerlink" title="标签 (label)"></a>标签 (label)</h4><p>在监督式学习中，标签指[<strong>样本</strong>]的“答案”或“结果”部分。有标签数据集中的每个样本都包含一个或多个特征以及一个标签。例如，在房屋数据集中，特征可能包括卧室数、卫生间数以及房龄，而标签则可能是房价。在垃圾邮件检测数据集中，特征可能包括主题行、发件人以及电子邮件本身，而标签则可能是“垃圾邮件”或“非垃圾邮件”。</p><h4 id="有标签样本-labeled-example"><a href="#有标签样本-labeled-example" class="headerlink" title="有标签样本 (labeled example)"></a>有标签样本 (labeled example)</h4><p>包含[<strong>特征</strong>]和[<strong>标签</strong>]的样本。在监督式训练中，模型从有标签样本中学习规律。</p><h4 id="lambda"><a href="#lambda" class="headerlink" title="lambda"></a>lambda</h4><p>与[<strong>正则化率</strong>]的含义相同。</p><p>（多含义术语，我们在此关注的是该术语在[<strong>正则化</strong>]中的定义。）</p><h4 id="层-layer"><a href="#层-layer" class="headerlink" title="层 (layer)"></a>层 (layer)</h4><p>[<strong>神经网络</strong>]中的一组[<strong>神经元</strong>]，负责处理一组输入特征，或一组神经元的输出。</p><p>此外还指 TensorFlow 中的抽象层。层是 Python 函数，以[<strong>张量</strong>]和配置选项作为输入，然后生成其他张量作为输出。当必要的张量组合起来后，用户便可以通过[<strong>模型函数</strong>]将结果转换为 [<strong>Estimator</strong>]。</p><h4 id="Layers-API-tf-layers"><a href="#Layers-API-tf-layers" class="headerlink" title="Layers API (tf.layers)"></a>Layers API (tf.layers)</h4><p>一种 TensorFlow API，用于以层组合的方式构建[<strong>深度</strong>]神经网络。通过 Layers API，您可以构建不同类型的[<strong>层</strong>]，例如：</p><ul><li>通过 <code>tf.layers.Dense</code> 构建[<strong>全连接层</strong>]。</li><li>通过 <code>tf.layers.Conv2D</code> 构建卷积层。</li></ul><p>在编写[<strong>自定义 Estimator</strong>](时，您可以编写“层”对象来定义所有[<strong>隐藏层</strong>]的特征。</p><p>Layers API 遵循 [<strong>Keras</strong>] layers API 规范。也就是说，除了前缀不同以外，Layers API 中的所有函数均与 Keras layers API 中的对应函数具有相同的名称和签名。</p><h4 id="学习速率-learning-rate"><a href="#学习速率-learning-rate" class="headerlink" title="学习速率 (learning rate)"></a>学习速率 (learning rate)</h4><p>在训练模型时用于梯度下降的一个标量。在每次迭代期间，[<strong>梯度下降法</strong>]都会将学习速率与梯度相乘。得出的乘积称为<strong>梯度步长</strong>。</p><p>学习速率是一个重要的[<strong>超参数</strong>]。</p><h4 id="最小二乘回归-least-squares-regression"><a href="#最小二乘回归-least-squares-regression" class="headerlink" title="最小二乘回归 (least squares regression)"></a>最小二乘回归 (least squares regression)</h4><p>一种通过最小化 [<strong>L2 损失</strong>]训练出的线性回归模型。</p><h4 id="线性回归-linear-regression"><a href="#线性回归-linear-regression" class="headerlink" title="线性回归 (linear regression)"></a>线性回归 (linear regression)</h4><p>一种[<strong>回归模型</strong>]，通过将输入特征进行线性组合输出连续值。</p><h4 id="逻辑回归-logistic-regression"><a href="#逻辑回归-logistic-regression" class="headerlink" title="逻辑回归 (logistic regression)"></a>逻辑回归 (logistic regression)</h4><p>一种模型，通过将 [<strong>S 型函数</strong>]应用于线性预测，生成分类问题中每个可能的离散标签值的概率。虽然逻辑回归经常用于[<strong>二元分类</strong>]问题，但也可用于[<strong>多类别</strong>]分类问题（其叫法变为<strong>多类别逻辑回归</strong>或<strong>多项回归</strong>）。</p><h4 id="对数-logits"><a href="#对数-logits" class="headerlink" title="对数 (logits)"></a>对数 (logits)</h4><p>分类模型生成的原始（非标准化）预测向量，通常会传递给标准化函数。如果模型要解决多类别分类问题，则对数通常变成 [softmax 函数]的输入。之后，softmax 函数会生成一个（标准化）概率向量，对应于每个可能的类别。</p><p>此外，对数有时也称为 [<strong>S 型函数</strong>]的元素级反函数。<br>如需了解详细信息，请参阅 <a href="https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits">tf.nn.sigmoid_cross_entropy_with_logits</a>。</p><h4 id="对数损失函数-Log-Loss"><a href="#对数损失函数-Log-Loss" class="headerlink" title="对数损失函数 (Log Loss)"></a>对数损失函数 (Log Loss)</h4><p>二元[<strong>逻辑回归</strong>](中使用的[<strong>损失</strong>]函数。</p><h4 id="对数几率-log-odds"><a href="#对数几率-log-odds" class="headerlink" title="对数几率 (log-odds)"></a>对数几率 (log-odds)</h4><p>某个事件几率的对数。</p><p>如果事件涉及二元概率，则<strong>几率</strong>指的是成功概率 (p) 与失败概率 (1-p) 之比。例如，假设某个给定事件的成功概率为 90％，失败概率为 10％。在这种情况下，几率的计算公式如下：<br>$$<br>几率 = \frac{p}{(1-p)} = \frac{.9}{.1} = 9<br>$$<br>简单来说，对数几率即几率的对数。按照惯例，“对数”指自然对数，但对数的基数其实可以是任何大于 1 的数。若遵循惯例，上述示例的对数几率应为：<br>$$<br>对数几率 = ln(9) = 2.2<br>$$<br>对数几率是 [<strong>S 型函数</strong>]的反函数。</p><h4 id="损失-Loss"><a href="#损失-Loss" class="headerlink" title="损失 (Loss)"></a>损失 (Loss)</h4><p>一种衡量指标，用于衡量模型的[<strong>预测</strong>](偏离其[<strong>标签</strong>]的程度。或者更悲观地说是衡量模型有多差。要确定此值，模型必须定义损失函数。例如，线性回归模型通常将[<strong>均方误差</strong>]用作损失函数，而逻辑回归模型则使用[<strong>对数损失函数</strong>]。</p><h3 id="M"><a href="#M" class="headerlink" title="M"></a>M</h3><h4 id="机器学习-machine-learning"><a href="#机器学习-machine-learning" class="headerlink" title="机器学习 (machine learning)"></a>机器学习 (machine learning)</h4><p>一种程序或系统，用于根据输入数据构建（训练）预测模型。这种系统会利用学到的模型根据从分布（训练该模型时使用的同一分布）中提取的新数据（以前从未见过的数据）进行实用的预测。机器学习还指与这些程序或系统相关的研究领域。</p><h4 id="均方误差-MSE-Mean-Squared-Error"><a href="#均方误差-MSE-Mean-Squared-Error" class="headerlink" title="均方误差 (MSE, Mean Squared Error)"></a>均方误差 (MSE, Mean Squared Error)</h4><p>每个样本的平均平方损失。<br>MSE 的计算方法是[<strong>平方损失</strong>]除以[<strong>样本</strong>]数。<br>[<strong>TensorFlow Playground</strong>]显示的“训练损失”值和“测试损失”值都是 MSE。</p><h4 id="指标-metric"><a href="#指标-metric" class="headerlink" title="指标 (metric)"></a>指标 (metric)</h4><p>您关心的一个数值。可能可以也可能不可以直接在机器学习系统中得到优化。您的系统尝试优化的指标称为[<strong>目标</strong>]。</p><h4 id="Metrics-API-tf-metrics"><a href="#Metrics-API-tf-metrics" class="headerlink" title="Metrics API (tf.metrics)"></a>Metrics API (tf.metrics)</h4><p>一种用于评估模型的 TensorFlow API。<br>例如，<code>tf.metrics.accuracy</code> 用于确定模型的预测与标签匹配的频率。<br>在编写[<strong>自定义 Estimator</strong>]时，您可以调用 Metrics API 函数来指定应如何评估您的模型。</p><h4 id="小批次-mini-batch"><a href="#小批次-mini-batch" class="headerlink" title="小批次 (mini-batch)"></a>小批次 (mini-batch)</h4><p>从整批[<strong>样本</strong>]内随机选择并在训练或推断过程的一次迭代中一起运行的一小部分样本。<br>小批次的[<strong>批次大小</strong>]通常介于 10 到 1000 之间。与基于完整的训练数据计算损失相比，基于小批次数据计算损失要高效得多。</p><h4 id="小批次随机梯度下降法-SGD-mini-batch-stochastic-gradient-descent"><a href="#小批次随机梯度下降法-SGD-mini-batch-stochastic-gradient-descent" class="headerlink" title="小批次随机梯度下降法 (SGD, mini-batch stochastic gradient descent)"></a>小批次随机梯度下降法 (SGD, mini-batch stochastic gradient descent)</h4><p>一种采用[<strong>小批次</strong>]样本的[<strong>梯度下降法</strong>]。也就是说，小批次 SGD 会根据一小部分训练数据来估算梯度。[<strong>Vanilla SGD</strong>] 使用的小批次的大小为 1。</p><h4 id="ML"><a href="#ML" class="headerlink" title="ML"></a>ML</h4><p>[<strong>机器学习</strong>]的缩写。</p><h4 id="模型-model"><a href="#模型-model" class="headerlink" title="模型 (model)"></a>模型 (model)</h4><p>机器学习系统从训练数据学到的内容的表示形式。多含义术语，可以理解为下列两种相关含义之一：</p><ul><li>一种 [<strong>TensorFlow</strong>]图，用于表示预测的计算结构。</li><li>该 TensorFlow 图的特定权重和偏差，通过[<strong>训练</strong>]决定。</li></ul><h4 id="模型函数-model-function"><a href="#模型函数-model-function" class="headerlink" title="模型函数 (model function)"></a>模型函数 (model function)</h4><p>[<strong>Estimator</strong>]中的函数，用于实现机器学习训练、评估和推断。例如，模型函数的训练部分可以处理以下任务：定义深度神经网络的拓扑并确定其[<strong>优化器</strong>]函数。如果使用[<strong>预创建的 Estimator</strong>]，则有人已为您编写了模型函数。如果使用[<strong>自定义 Estimator</strong>]，则必须自行编写模型函数。</p><p>有关编写模型函数的详细信息，请参阅<a href="https://www.tensorflow.org/get_started/custom_estimators">创建自定义 Estimator</a>。</p><h4 id="模型训练-model-training"><a href="#模型训练-model-training" class="headerlink" title="模型训练 (model training)"></a>模型训练 (model training)</h4><p>确定最佳[<strong>模型</strong>]的过程。</p><h4 id="动量-Momentum"><a href="#动量-Momentum" class="headerlink" title="动量 (Momentum)"></a>动量 (Momentum)</h4><p>一种先进的梯度下降法，其中学习步长不仅取决于当前步长的导数，还取决于之前一步或多步的步长的导数。<br>动量涉及计算梯度随时间而变化的指数级加权移动平均值，与物理学中的动量类似。<br>动量有时可以防止学习过程被卡在局部最小的情况。</p><h4 id="多类别分类-multi-class-classification"><a href="#多类别分类-multi-class-classification" class="headerlink" title="多类别分类 (multi-class classification)"></a>多类别分类 (multi-class classification)</h4><p>区分两种以上类别的分类问题。<br>例如，枫树大约有 128 种，因此，确定枫树种类的模型就属于多类别模型。<br>反之，仅将电子邮件分为两类（“垃圾邮件”和“非垃圾邮件”）的模型属于[<strong>二元分类模型</strong>]。</p><h2 id="多项分类-multinomial-classification"><a href="#多项分类-multinomial-classification" class="headerlink" title="多项分类 (multinomial classification)"></a>多项分类 (multinomial classification)</h2><p>与[<strong>多类别分类</strong>]的含义相同。</p><h4 id="N"><a href="#N" class="headerlink" title="N"></a>N</h4><h4 id="NaN-陷阱-NaN-trap"><a href="#NaN-陷阱-NaN-trap" class="headerlink" title="NaN 陷阱 (NaN trap)"></a>NaN 陷阱 (NaN trap)</h4><p>模型中的一个数字在训练期间变成 [NaN]，这会导致模型中的很多或所有其他数字最终也会变成 NaN。<br>NaN 是“非数字”的缩写。</p><h4 id="负类别-negative-class"><a href="#负类别-negative-class" class="headerlink" title="负类别 (negative class)"></a>负类别 (negative class)</h4><p>在[<strong>二元分类</strong>]中，一种类别称为正类别，另一种类别称为负类别。正类别是我们要寻找的类别，负类别则是另一种可能性。例如，在医学检查中，负类别可以是“非肿瘤”。在电子邮件分类器中，负类别可以是“非垃圾邮件”。另请参阅[<strong>正类别</strong>]。</p><h4 id="神经网络-neural-network"><a href="#神经网络-neural-network" class="headerlink" title="神经网络 (neural network)"></a>神经网络 (neural network)</h4><p>一种模型，灵感来源于脑部结构，由多个层构成（至少有一个是[<strong>隐藏层</strong>]），每个层都包含简单相连的单元或[<strong>神经元</strong>]（具有非线性关系）。</p><h4 id="神经元-neuron"><a href="#神经元-neuron" class="headerlink" title="神经元 (neuron)"></a>神经元 (neuron)</h4><p>[<strong>神经网络</strong>]中的节点，通常会接收多个输入值并生成一个输出值。<br>神经元通过将[<strong>激活函数</strong>]（非线性转换）应用于输入值的加权和来计算输出值。</p><h4 id="节点-node"><a href="#节点-node" class="headerlink" title="节点 (node)"></a>节点 (node)</h4><p>多含义术语，可以理解为下列两种含义之一：</p><ul><li>[<strong>隐藏层</strong>])中的神经元。</li><li>TensorFlow [<strong>图</strong>]中的操作。</li></ul><h4 id="标准化-normalization"><a href="#标准化-normalization" class="headerlink" title="标准化 (normalization)"></a>标准化 (normalization)</h4><p>将实际的值区间转换为标准的值区间（通常为 -1 到 +1 或 0 到 1）的过程。<br>例如，假设某个特征的自然区间是 800 到 6000。通过减法和除法运算，您可以将这些值标准化为位于 -1 到 +1 区间内。</p><p>另请参阅[<strong>缩放</strong>]。</p><h4 id="数值数据-numerical-data"><a href="#数值数据-numerical-data" class="headerlink" title="数值数据 (numerical data)"></a>数值数据 (numerical data)</h4><p>用整数或实数表示的[<strong>特征</strong>]。<br>例如，在房地产模型中，您可能会用数值数据表示房子大小（以平方英尺或平方米为单位）。如果用数值数据表示特征，则可以表明特征的值相互之间具有数学关系，并且与标签可能也有数学关系。例如，如果用数值数据表示房子大小，则可以表明面积为 200 平方米的房子是面积为 100 平方米的房子的两倍。此外，房子面积的平方米数可能与房价存在一定的数学关系。</p><p>并非所有整数数据都应表示成数值数据。例如，世界上某些地区的邮政编码是整数，但在模型中，不应将整数邮政编码表示成数值数据。这是因为邮政编码 <code>20000</code> 在效力上并不是邮政编码 10000 的两倍（或一半）。此外，虽然不同的邮政编码确实与不同的房地产价值有关，但我们也不能假设邮政编码为 20000 的房地产在价值上是邮政编码为 10000 的房地产的两倍。邮政编码应表示成[<strong>分类数据</strong>]。</p><p>数值特征有时称为[<strong>连续特征</strong>]。</p><h4 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h4><p>一个<a href="http://www.numpy.org/">开放源代码数学库</a>，在 Python 中提供高效的数组操作。<br>[<strong>Pandas</strong>]建立在 Numpy 之上。</p><h3 id="O"><a href="#O" class="headerlink" title="O"></a>O</h3><h4 id="目标-objective"><a href="#目标-objective" class="headerlink" title="目标 (objective)"></a>目标 (objective)</h4><p>算法尝试优化的指标。</p><h4 id="离线推断-offline-inference"><a href="#离线推断-offline-inference" class="headerlink" title="离线推断 (offline inference)"></a>离线推断 (offline inference)</h4><p>生成一组[<strong>预测</strong>]，存储这些预测，然后根据需求检索这些预测。与[<strong>在线推断</strong>]相对。</p><h4 id="独热编码-one-hot-encoding"><a href="#独热编码-one-hot-encoding" class="headerlink" title="独热编码 (one-hot encoding)"></a>独热编码 (one-hot encoding)</h4><p>一种稀疏向量，其中：</p><ul><li>一个元素设为 1。</li><li>所有其他元素均设为 0。</li></ul><p>独热编码常用于表示拥有有限个可能值的字符串或标识符。例如，假设某个指定的植物学数据集记录了 15000 个不同的物种，其中每个物种都用独一无二的字符串标识符来表示。在特征工程过程中，您可能需要将这些字符串标识符编码为独热向量，向量的大小为 15000。</p><h4 id="单样本学习（one-shot-learning，通常用于对象分类）"><a href="#单样本学习（one-shot-learning，通常用于对象分类）" class="headerlink" title="单样本学习（one-shot learning，通常用于对象分类）"></a>单样本学习（one-shot learning，通常用于对象分类）</h4><p>一种机器学习方法，通常用于对象分类，旨在通过单个训练样本学习有效的分类器。</p><p>另请参阅[<strong>少量样本学习</strong>]。</p><h4 id="一对多-one-vs-all"><a href="#一对多-one-vs-all" class="headerlink" title="一对多 (one-vs.-all)"></a>一对多 (one-vs.-all)</h4><p>假设某个分类问题有 N 种可能的解决方案，一对多解决方案将包含 N 个单独的[<strong>二元分类器</strong>]- 一个二元分类器对应一种可能的结果。例如，假设某个模型用于区分样本属于动物、蔬菜还是矿物，一对多解决方案将提供下列三个单独的二元分类器：</p><ul><li>动物和非动物</li><li>蔬菜和非蔬菜</li><li>矿物和非矿物</li></ul><h4 id="在线推断-online-inference"><a href="#在线推断-online-inference" class="headerlink" title="在线推断 (online inference)"></a>在线推断 (online inference)</h4><p>根据需求生成[<strong>预测</strong>]。与[<strong>离线推断</strong>]相对。</p><h4 id="操作-op-Operation"><a href="#操作-op-Operation" class="headerlink" title="操作 (op, Operation)"></a>操作 (op, Operation)</h4><p>TensorFlow 图中的节点。在 TensorFlow 中，任何创建、操纵或销毁[<strong>张量</strong>]的过程都属于操作。例如，矩阵相乘就是一种操作，该操作以两个张量作为输入，并生成一个张量作为输出。</p><h4 id="优化器-optimizer"><a href="#优化器-optimizer" class="headerlink" title="优化器 (optimizer)"></a>优化器 (optimizer)</h4><p>[<strong>梯度下降法</strong>]的一种具体实现。TensorFlow 的优化器基类是 [tf.train.Optimizer]。不同的优化器可能会利用以下一个或多个概念来增强梯度下降法在指定[<strong>训练集</strong>]中的效果：</p><ul><li>[动量] (Momentum)</li><li>更新频率（<a href="https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer">AdaGrad</a> = ADAptive GRADient descent；<a href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer">Adam</a> = ADAptive with Momentum；RMSProp）</li><li>稀疏性/正则化 (<a href="https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer">Ftrl</a>)</li><li>更复杂的数学方法（<a href="https://www.tensorflow.org/api_docs/python/tf/train/ProximalGradientDescentOptimizer">Proximal</a>，等等）</li></ul><p>甚至还包括 <a href="https://arxiv.org/abs/1606.04474">NN 驱动的优化器</a>。</p><h4 id="离群值-outlier"><a href="#离群值-outlier" class="headerlink" title="离群值 (outlier)"></a>离群值 (outlier)</h4><p>与大多数其他值差别很大的值。在机器学习中，下列所有值都是离群值。</p><ul><li>绝对值很高的[<strong>权重</strong>]。</li><li>与实际值相差很大的预测值。</li><li>值比平均值高大约 3 个标准偏差的输入数据。</li></ul><p>离群值常常会导致模型训练出现问题。</p><h4 id="输出层-output-layer"><a href="#输出层-output-layer" class="headerlink" title="输出层 (output layer)"></a>输出层 (output layer)</h4><p>神经网络的“最后”一层，也是包含答案的层。</p><h4 id="过拟合-overfitting"><a href="#过拟合-overfitting" class="headerlink" title="过拟合 (overfitting)"></a>过拟合 (overfitting)</h4><p>创建的模型与[<strong>训练数据</strong>]过于匹配，以致于模型无法根据新数据做出正确的预测。</p><h4 id="P"><a href="#P" class="headerlink" title="P"></a>P</h4><h4 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h4><p>面向列的数据分析 API。很多机器学习框架（包括 TensorFlow）都支持将 Pandas 数据结构作为输入。<br>请参阅 <a href="http://pandas.pydata.org/">Pandas 文档</a>。</p><h4 id="参数-parameter"><a href="#参数-parameter" class="headerlink" title="参数 (parameter)"></a>参数 (parameter)</h4><p>机器学习系统自行训练的模型的变量。<br>例如，[<strong>权重</strong>]就是一种参数，它们的值是机器学习系统通过连续的训练迭代逐渐学习到的。与[<strong>超参数</strong>]相对。</p><h4 id="参数服务器-PS-Parameter-Server"><a href="#参数服务器-PS-Parameter-Server" class="headerlink" title="参数服务器 (PS, Parameter Server)"></a>参数服务器 (PS, Parameter Server)</h4><p>一种作业，负责在分布式设置中跟踪模型[<strong>参数</strong>]。</p><h4 id="参数更新-parameter-update"><a href="#参数更新-parameter-update" class="headerlink" title="参数更新 (parameter update)"></a>参数更新 (parameter update)</h4><p>在训练期间（通常是在[<strong>梯度下降法</strong>]的单次迭代中）调整模型[<strong>参数</strong>]的操作。</p><h4 id="偏导数-partial-derivative"><a href="#偏导数-partial-derivative" class="headerlink" title="偏导数 (partial derivative)"></a>偏导数 (partial derivative)</h4><p>一种导数，除一个变量之外的所有变量都被视为常量。例如，f(x, y) 对 x 的偏导数就是 f(x) 的导数（即，使 y 保持恒定）。f 对 x 的偏导数仅关注 x 如何变化，而忽略公式中的所有其他变量。</p><h4 id="划分策略-partitioning-strategy"><a href="#划分策略-partitioning-strategy" class="headerlink" title="划分策略 (partitioning strategy)"></a>划分策略 (partitioning strategy)</h4><p>在[<strong>参数服务器</strong>]间分割变量的算法。</p><h4 id="性能-performance"><a href="#性能-performance" class="headerlink" title="性能 (performance)"></a>性能 (performance)</h4><p>多含义术语，具有以下含义：</p><ul><li>在软件工程中的传统含义。即：相应软件的运行速度有多快（或有多高效）？</li><li>在机器学习中的含义。在机器学习领域，性能旨在回答以下问题：相应[<strong>模型</strong>]的准确度有多高？即模型在预测方面的表现有多好？</li></ul><h4 id="困惑度-perplexity"><a href="#困惑度-perplexity" class="headerlink" title="困惑度 (perplexity)"></a>困惑度 (perplexity)</h4><p>一种衡量指标，用于衡量[<strong>模型</strong>]能够多好地完成任务。例如，假设任务是读取用户使用智能手机键盘输入字词时输入的前几个字母，然后列出一组可能的完整字词。此任务的困惑度 (P) 是：为了使列出的字词中包含用户尝试输入的实际字词，您需要提供的猜测项的个数。</p><p>困惑度与[<strong>交叉熵</strong>]的关系如下：<br>$$<br>P = 2^{-cross \quad entropy}<br>$$</p><h4 id="流水线-pipeline"><a href="#流水线-pipeline" class="headerlink" title="流水线 (pipeline)"></a>流水线 (pipeline)</h4><p>机器学习算法的基础架构。流水线包括收集数据、将数据放入训练数据文件、训练一个或多个模型，以及将模型导出到生产环境。</p><h4 id="池化-pooling"><a href="#池化-pooling" class="headerlink" title="池化 (pooling)"></a>池化 (pooling)</h4><p>将一个或多个由前趋的[<strong>卷积层</strong>]创建的矩阵压缩为较小的矩阵。<br>池化通常是取整个池化区域的最大值或平均值。以下面的 3x3 矩阵为例：</p><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/1571518-20210519144058838-1869342839.png" style="zoom:50%;"><p>池化运算与卷积运算类似：将矩阵分割为多个切片，然后按[<strong>步长</strong>]逐个运行卷积运算。例如，假设池化运算按 1x1 步长将卷积矩阵分割为 2x2 个切片。如下图所示，进行了四个池化运算。假设每个池化运算都选择该切片中四个值的最大值：</p><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/1571518-20210519144154895-1220687892.png" style="zoom:50%;"><p>池化有助于在输入矩阵中实现[<strong>平移不变性</strong>]。</p><p>对于视觉应用来说，池化的更正式名称为<strong>空间池化</strong>。时间序列应用通常将池化称为<strong>时序池化</strong>。按照不太正式的说法，池化通常称为<strong>下采样</strong>或<strong>降采样</strong>。</p><h4 id="正类别-positive-class"><a href="#正类别-positive-class" class="headerlink" title="正类别 (positive class)"></a>正类别 (positive class)</h4><p>在[<strong>二元分类</strong>]中，两种可能的类别分别被标记为正类别和负类别。正类别结果是我们要测试的对象。（不可否认的是，我们会同时测试这两种结果，但只关注正类别结果。）例如，在医学检查中，正类别可以是“肿瘤”。在电子邮件分类器中，正类别可以是“垃圾邮件”。</p><p>与[<strong>负类别</strong>]相对。</p><h4 id="精确率-precision"><a href="#精确率-precision" class="headerlink" title="精确率 (precision)"></a>精确率 (precision)</h4><p>一种[<strong>分类模型</strong>]指标。</p><p>精确率指模型正确预测[<strong>正类别</strong>]的频率，即：<br>$$<br>精确率 = \frac{正例数}{正例数 + 假正例数}<br>$$</p><h4 id="预测-prediction"><a href="#预测-prediction" class="headerlink" title="预测 (prediction)"></a>预测 (prediction)</h4><p>模型在收到输入[<strong>样本</strong>]后的输出。</p><h4 id="预测偏差-prediction-bias"><a href="#预测偏差-prediction-bias" class="headerlink" title="预测偏差 (prediction bias)"></a>预测偏差 (prediction bias)</h4><p>一种值，用于表明[<strong>预测</strong>]平均值与数据集中[<strong>标签</strong>]的平均值相差有多大。</p><h4 id="预创建的-Estimator-pre-made-Estimator"><a href="#预创建的-Estimator-pre-made-Estimator" class="headerlink" title="预创建的 Estimator (pre-made Estimator)"></a>预创建的 Estimator (pre-made Estimator)</h4><p>其他人已建好的 [<strong>Estimator</strong>]。<br>TensorFlow 提供了一些预创建的 Estimator，包括 <code>DNNClassifier</code>、<code>DNNRegressor</code> 和 <code>LinearClassifier</code>。您可以按照<a href="https://www.tensorflow.org/extend/estimators">这些说明</a>构建自己预创建的 Estimator。</p><h4 id="预训练模型-pre-trained-model"><a href="#预训练模型-pre-trained-model" class="headerlink" title="预训练模型 (pre-trained model)"></a>预训练模型 (pre-trained model)</h4><p>已经过训练的模型或模型组件（例如[<strong>嵌套</strong>]）。<br>有时，您需要将预训练的嵌套馈送到[<strong>神经网络</strong>]。<br>在其他时候，您的模型将自行训练嵌套，而不依赖于预训练的嵌套。</p><h4 id="先验信念-prior-belief"><a href="#先验信念-prior-belief" class="headerlink" title="先验信念 (prior belief)"></a>先验信念 (prior belief)</h4><p>在开始采用相应数据进行训练之前，您对这些数据抱有的信念。<br>例如，[<strong>L2 正则化</strong>]依赖的先验信念是[<strong>权重</strong>]应该很小且应以 0 为中心呈正态分布。</p><h3 id="Q"><a href="#Q" class="headerlink" title="Q"></a>Q</h3><h4 id="队列-queue"><a href="#队列-queue" class="headerlink" title="队列 (queue)"></a>队列 (queue)</h4><p>一种 TensorFlow [<strong>操作</strong>]，用于实现队列数据结构。通常用于 I/O 中。</p><h3 id="R"><a href="#R" class="headerlink" title="R"></a>R</h3><h4 id="等级-rank"><a href="#等级-rank" class="headerlink" title="等级 (rank)"></a>等级 (rank)</h4><p>机器学习中的一个多含义术语，可以理解为下列含义之一：</p><ul><li>[<strong>张量</strong>]中的维数。例如，标量等级为 0，向量等级为 1，矩阵等级为 2。</li><li>在将类别从最高到最低进行排序的机器学习问题中，类别的顺序位置。例如，行为排序系统可以将狗狗的奖励从最高（牛排）到最低（枯萎的羽衣甘蓝）进行排序。</li></ul><h4 id="评分者-rater"><a href="#评分者-rater" class="headerlink" title="评分者 (rater)"></a>评分者 (rater)</h4><p>为[<strong>样本</strong>]提供[<strong>标签</strong>]的人。有时称为“注释者”。</p><h4 id="召回率-recall"><a href="#召回率-recall" class="headerlink" title="召回率 (recall)"></a>召回率 (recall)</h4><p>一种[<strong>分类模型</strong>]指标，用于回答以下问题：在所有可能的正类别标签中，模型正确地识别出了多少个？即：<br>$$<br>召回率＝ \frac{正例数}{正例数 + 假正例数}<br>$$</p><h4 id="修正线性单元-ReLU-Rectified-Linear-Unit"><a href="#修正线性单元-ReLU-Rectified-Linear-Unit" class="headerlink" title="修正线性单元 (ReLU, Rectified Linear Unit)"></a>修正线性单元 (ReLU, Rectified Linear Unit)</h4><p>一种[<strong>激活函数</strong>]，其规则如下：</p><ul><li>如果输入为负数或 0，则输出 0。</li><li>如果输入为正数，则输出等于输入。</li></ul><h4 id="回归模型-regression-model"><a href="#回归模型-regression-model" class="headerlink" title="回归模型 (regression model)"></a>回归模型 (regression model)</h4><p>一种模型，能够输出连续的值（通常为浮点值）。<br>请与[<strong>分类模型</strong>]进行比较，分类模型会输出离散值，例如“黄花菜”或“虎皮百合”。</p><h4 id="正则化-regularization"><a href="#正则化-regularization" class="headerlink" title="正则化 (regularization)"></a>正则化 (regularization)</h4><p>对模型复杂度的惩罚。<br>正则化有助于防止出现[<strong>过拟合</strong>]，包含以下类型：</p><ul><li>[<strong>L1 正则化</strong>]</li><li>[<strong>L2 正则化</strong>]</li><li>[<strong>丢弃正则化</strong>]</li><li>[<strong>早停法</strong>]（这不是正式的正则化方法，但可以有效限制过拟合）</li></ul><h4 id="正则化率-regularization-rate"><a href="#正则化率-regularization-rate" class="headerlink" title="正则化率 (regularization rate)"></a>正则化率 (regularization rate)</h4><p>一种标量值，以 lambda 表示，用于指定正则化函数的相对重要性。<br>从下面简化的[<strong>损失</strong>]公式中可以看出正则化率的影响：<br>$$<br>最小化(损失方程 + \lambda(正则化方程))<br>$$<br>提高正则化率可以减少[<strong>过拟合</strong>]，但可能会使模型的[<strong>准确率</strong>]降低。</p><h4 id="表示法-representation"><a href="#表示法-representation" class="headerlink" title="表示法 (representation)"></a>表示法 (representation)</h4><p>将数据映射到实用[<strong>特征</strong>]的过程。</p><h4 id="受试者工作特征曲线（receiver-operating-characteristic，简称-ROC-曲线）"><a href="#受试者工作特征曲线（receiver-operating-characteristic，简称-ROC-曲线）" class="headerlink" title="受试者工作特征曲线（receiver operating characteristic，简称 ROC 曲线）"></a>受试者工作特征曲线（receiver operating characteristic，简称 ROC 曲线）</h4><p>不同[<strong>分类阈值</strong>]下的[<strong>正例率</strong>]和[<strong>假正例率</strong>]构成的曲线。<br>另请参阅[<strong>曲线下面积</strong>]。</p><h4 id="根目录-root-directory"><a href="#根目录-root-directory" class="headerlink" title="根目录 (root directory)"></a>根目录 (root directory)</h4><p>您指定的目录，用于托管多个模型的 TensorFlow 检查点和事件文件的子目录。</p><h4 id="均方根误差-RMSE-Root-Mean-Squared-Error"><a href="#均方根误差-RMSE-Root-Mean-Squared-Error" class="headerlink" title="均方根误差 (RMSE, Root Mean Squared Error)"></a>均方根误差 (RMSE, Root Mean Squared Error)</h4><p>[<strong>均方误差</strong>]的平方根。</p><h4 id="旋转不变性-rotational-invariance"><a href="#旋转不变性-rotational-invariance" class="headerlink" title="旋转不变性 (rotational invariance)"></a>旋转不变性 (rotational invariance)</h4><p>在图像分类问题中，即使图像的方向发生变化，算法也能成功地对图像进行分类。例如，无论网球拍朝上、侧向还是朝下放置，该算法仍然可以识别它。请注意，并非总是希望旋转不变；例如，倒置的“9”不应分类为“9”。</p><p>另请参阅[<strong>平移不变性</strong>]和[<strong>大小不变性</strong>]。</p><h3 id="S"><a href="#S" class="headerlink" title="S"></a>S</h3><h4 id="SavedModel"><a href="#SavedModel" class="headerlink" title="SavedModel"></a>SavedModel</h4><p>保存和恢复 TensorFlow 模型时建议使用的格式。<br>SavedModel 是一种独立于语言且可恢复的序列化格式，使较高级别的系统和工具可以创建、使用和转换 TensorFlow 模型。</p><p>如需完整的详细信息，请参阅《TensorFlow 编程人员指南》中的<a href="https://www.tensorflow.org/programmers_guide/saved_model">保存和恢复</a>。</p><h4 id="Saver"><a href="#Saver" class="headerlink" title="Saver"></a>Saver</h4><p>一种 [TensorFlow 对象]，负责保存模型检查点。</p><h4 id="缩放-scaling"><a href="#缩放-scaling" class="headerlink" title="缩放 (scaling)"></a>缩放 (scaling)</h4><p>[<strong>特征工程</strong>]中的一种常用做法，是指对某个特征的值区间进行调整，使之与数据集中其他特征的值区间一致。例如，假设您希望数据集中所有浮点特征的值都位于 0 到 1 区间内，如果某个特征的值位于 0 到 500 区间内，您就可以通过将每个值除以 500 来缩放该特征。</p><p>另请参阅[<strong>标准化</strong>]。</p><h4 id="scikit-learn"><a href="#scikit-learn" class="headerlink" title="scikit-learn"></a>scikit-learn</h4><p>一个热门的开放源代码机器学习平台。</p><p>请访问 <a href="http://www.scikit-learn.org/">www.scikit-learn.org</a>。</p><h4 id="半监督式学习-semi-supervised-learning"><a href="#半监督式学习-semi-supervised-learning" class="headerlink" title="半监督式学习 (semi-supervised learning)"></a>半监督式学习 (semi-supervised learning)</h4><p>训练模型时采用的数据中，某些训练样本有标签，而其他样本则没有标签。半监督式学习采用的一种技术是推断无标签样本的标签，然后使用推断出的标签进行训练，以创建新模型。如果获得有标签样本需要高昂的成本，而无标签样本则有很多，那么半监督式学习将非常有用。</p><h4 id="序列模型-sequence-model"><a href="#序列模型-sequence-model" class="headerlink" title="序列模型 (sequence model)"></a>序列模型 (sequence model)</h4><p>一种模型，其输入具有序列依赖性。</p><p>例如，根据之前观看过的一系列视频对观看的下一个视频进行预测。</p><h4 id="会话-tf-session"><a href="#会话-tf-session" class="headerlink" title="会话 (tf.session)"></a>会话 (tf.session)</h4><p>封装了 TensorFlow 运行时状态的对象，用于运行全部或部分[<strong>图</strong>]。在使用底层 TensorFlow API 时，您可以直接创建并管理一个或多个 <code>tf.session</code> 对象。在使用 Estimator API 时，Estimator 会为您创建会话对象。</p><h4 id="S-型函数-sigmoid-function"><a href="#S-型函数-sigmoid-function" class="headerlink" title="S 型函数 (sigmoid function)"></a>S 型函数 (sigmoid function)</h4><p>一种函数，可将逻辑回归输出或多项回归输出（对数几率）映射到概率，以返回介于 0 到 1 之间的值。S 型函数的公式如下：<br>$$<br>y = \frac{1}{1 + e^{-\sigma}}<br>$$<br>在[<strong>逻辑回归</strong>]问题中，σ 非常简单：<br>$$<br>\sigma = b + w_1x_1 + w_2x_2 + … + w_nx_n<br>$$<br>换句话说，S 型函数可将$\sigma$转换为介于 0 到 1 之间的概率。<br>在某些[<strong>神经网络</strong>]中，S 型函数可作为[<strong>激活函数</strong>]使用。</p><h4 id="大小不变性-size-invariance"><a href="#大小不变性-size-invariance" class="headerlink" title="大小不变性 (size invariance)"></a>大小不变性 (size invariance)</h4><p>在图像分类问题中，即使图像的大小发生变化，算法也能成功地对图像进行分类。例如，无论一只猫以 200 万像素还是 20 万像素呈现，该算法仍然可以识别它。请注意，即使是最好的图像分类算法，在大小不变性方面仍然会存在切实的限制。例如，对于仅以 20 像素呈现的猫图像，算法（或人）不可能正确对其进行分类。</p><p>另请参阅[<strong>平移不变性</strong>]和[<strong>旋转不变性</strong>]。</p><h4 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h4><p>一种函数，可提供[<strong>多类别分类模型</strong>]中每个可能类别的概率。这些概率的总和正好为 1.0。例如，softmax 可能会得出某个图像是狗、猫和马的概率分别是 0.9、0.08 和 0.02。（也称为<strong>完整 softmax</strong>。）</p><p>与[<strong>候选采样</strong>]相对。</p><h4 id="稀疏特征-sparse-feature"><a href="#稀疏特征-sparse-feature" class="headerlink" title="稀疏特征 (sparse feature)"></a>稀疏特征 (sparse feature)</h4><p>一种[<strong>特征</strong>]向量，其中的大多数值都为 0 或为空。例如，某个向量包含一个为 1 的值和一百万个为 0 的值，则该向量就属于稀疏向量。再举一个例子，搜索查询中的单词也可能属于稀疏特征 - 在某种指定语言中有很多可能的单词，但在某个指定的查询中仅包含其中几个。</p><p>与[<strong>密集特征</strong>]相对。</p><h4 id="稀疏表示法-sparse-representation"><a href="#稀疏表示法-sparse-representation" class="headerlink" title="稀疏表示法 (sparse representation)"></a>稀疏表示法 (sparse representation)</h4><p>一种张量[<strong>表示法</strong>]，仅存储非零元素。</p><p>例如，英语中包含约一百万个单词。表示一个英语句子中所用单词的数量，考虑以下两种方式：</p><ul><li>要采用<strong>密集表示法</strong>来表示此句子，则必须为所有一百万个单元格设置一个整数，然后在大部分单元格中放入 0，在少数单元格中放入一个非常小的整数。</li><li>要采用稀疏表示法来表示此句子，则仅存储象征句子中实际存在的单词的单元格。因此，如果句子只包含 20 个独一无二的单词，那么该句子的稀疏表示法将仅在 20 个单元格中存储一个整数。</li></ul><p>例如，假设以两种方式来表示句子“Dogs wag tails.”。如下表所示，密集表示法将使用约一百万个单元格；稀疏表示法则只使用 3 个单元格：</p><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/1571518-20210519150029838-965833002.png" style="zoom: 67%;"><h4 id="稀疏性-sparsity"><a href="#稀疏性-sparsity" class="headerlink" title="稀疏性 (sparsity)"></a>稀疏性 (sparsity)</h4><p>向量或矩阵中设置为 0（或空）的元素数除以该向量或矩阵中的条目总数。<br>以一个 10x10 矩阵（其中 98 个单元格都包含 0）为例。<br>稀疏性的计算方法如下：<br>$$<br>稀疏性 = \frac{98}{100} = 0.98<br>$$<br><strong>特征稀疏性</strong>是指特征向量的稀疏性；<strong>模型稀疏性</strong>是指模型权重的稀疏性。</p><h4 id="空间池化-spatial-pooling"><a href="#空间池化-spatial-pooling" class="headerlink" title="空间池化 (spatial pooling)"></a>空间池化 (spatial pooling)</h4><p>请参阅[<strong>池化</strong>]。</p><h4 id="平方合页损失函数-squared-hinge-loss"><a href="#平方合页损失函数-squared-hinge-loss" class="headerlink" title="平方合页损失函数 (squared hinge loss)"></a>平方合页损失函数 (squared hinge loss)</h4><p>[<strong>合页损失函数</strong>]的平方。与常规合页损失函数相比，平方合页损失函数对离群值的惩罚更严厉。</p><h4 id="平方损失函数-squared-loss"><a href="#平方损失函数-squared-loss" class="headerlink" title="平方损失函数 (squared loss)"></a>平方损失函数 (squared loss)</h4><p>在[<strong>线性回归</strong>]中使用的[<strong>损失</strong>]函数（也称为 <strong>L2 损失函数</strong>）。<br>该函数可计算模型为有标签[<strong>样本</strong>]预测的值和[<strong>标签</strong>]的实际值之差的平方。由于取平方值，因此该损失函数会放大不佳预测的影响。也就是说，与 [<strong>L1 损失函数</strong>]相比，平方损失函数对离群值的反应更强烈。</p><h4 id="静态模型-static-model"><a href="#静态模型-static-model" class="headerlink" title="静态模型 (static model)"></a>静态模型 (static model)</h4><p>离线训练的一种模型。</p><h4 id="平稳性-stationarity"><a href="#平稳性-stationarity" class="headerlink" title="平稳性 (stationarity)"></a>平稳性 (stationarity)</h4><p>数据集中数据的一种属性，表示数据分布在一个或多个维度保持不变。这种维度最常见的是时间，即表明平稳性的数据不随时间而变化。例如，从 9 月到 12 月，表明平稳性的数据没有发生变化。</p><h4 id="步-step"><a href="#步-step" class="headerlink" title="步 (step)"></a>步 (step)</h4><p>对一个[<strong>批次</strong>]的向前和向后评估。</p><h4 id="步长-step-size"><a href="#步长-step-size" class="headerlink" title="步长 (step size)"></a>步长 (step size)</h4><p>与[<strong>学习速率</strong>]的含义相同。</p><h4 id="随机梯度下降法-SGD-stochastic-gradient-descent"><a href="#随机梯度下降法-SGD-stochastic-gradient-descent" class="headerlink" title="随机梯度下降法 (SGD, stochastic gradient descent)"></a>随机梯度下降法 (SGD, stochastic gradient descent)</h4><p>批次大小为 1 的一种[<strong>梯度下降法</strong>]。<br>换句话说，SGD 依赖于从数据集中随机均匀选择的单个样本来计算每步的梯度估算值。</p><h4 id="结构风险最小化-SRM-structural-risk-minimization"><a href="#结构风险最小化-SRM-structural-risk-minimization" class="headerlink" title="结构风险最小化 (SRM, structural risk minimization)"></a>结构风险最小化 (SRM, structural risk minimization)</h4><p>一种算法，用于平衡以下两个目标：</p><ul><li>期望构建最具预测性的模型（例如损失最低）。</li><li>期望使模型尽可能简单（例如强大的正则化）。</li></ul><p>例如，旨在将基于训练集的损失和正则化降至最低的函数就是一种结构风险最小化算法。</p><p>如需更多信息，请参阅 <a href="http://www.svms.org/srm/%E3%80%82">http://www.svms.org/srm/。</a></p><p>与[<strong>经验风险最小化</strong>]相对。</p><h4 id="步长-stride"><a href="#步长-stride" class="headerlink" title="步长 (stride)"></a>步长 (stride)</h4><p>在卷积运算或池化中，下一个系列的输入切片的每个维度中的增量。<br>例如，下面的动画演示了卷积运算过程中的一个 (1,1) 步长。<br>因此，下一个输入切片是从上一个输入切片向右移动一个步长的位置开始。<br>当运算到达右侧边缘时，下一个切片将回到最左边，但是下移一个位置。</p><p><img src="/blog/blog/2022/01/20/ji-qi-xue-xi-zhu-yu-biao/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9C%AF%E8%AF%AD%E8%A1%A8/1571518-20210519150736162-113936158.gif"></p><p>前面的示例演示了一个二维步长。如果输入矩阵为三维，那么步长也将是三维。</p><h4 id="下采样-subsampling"><a href="#下采样-subsampling" class="headerlink" title="下采样 (subsampling)"></a>下采样 (subsampling)</h4><p>请参阅[<strong>池化</strong>]。</p><h4 id="总结-summary"><a href="#总结-summary" class="headerlink" title="总结 (summary)"></a>总结 (summary)</h4><p>在 TensorFlow 中的某一[<strong>步</strong>]计算出的一个值或一组值，通常用于在训练期间跟踪模型指标。</p><h4 id="监督式机器学习-supervised-machine-learning"><a href="#监督式机器学习-supervised-machine-learning" class="headerlink" title="监督式机器学习 (supervised machine learning)"></a>监督式机器学习 (supervised machine learning)</h4><p>根据输入数据及其对应的[<strong>标签</strong>]来训练[<strong>模型</strong>]。监督式机器学习类似于学生通过研究一系列问题及其对应的答案来学习某个主题。在掌握了问题和答案之间的对应关系后，学生便可以回答关于同一主题的新问题（以前从未见过的问题）。<br>请与[<strong>非监督式机器学习</strong>]进行比较。</p><h4 id="合成特征-synthetic-feature"><a href="#合成特征-synthetic-feature" class="headerlink" title="合成特征 (synthetic feature)"></a>合成特征 (synthetic feature)</h4><p>一种[<strong>特征</strong>]，不在输入特征之列，而是从一个或多个输入特征衍生而来。合成特征包括以下类型：</p><ul><li>对连续特征进行[<strong>分桶</strong>]，以分为多个区间分箱。</li><li>将一个特征值与其他特征值或其本身相乘（或相除）。</li><li>创建一个[<strong>特征组合</strong>]。</li></ul><p>仅通过[<strong>标准化</strong>]或[<strong>缩放</strong>]创建的特征不属于合成特征。</p><h3 id="T"><a href="#T" class="headerlink" title="T"></a>T</h3><h4 id="目标-target"><a href="#目标-target" class="headerlink" title="目标 (target)"></a>目标 (target)</h4><p>与[<strong>标签</strong>]的含义相同。</p><h4 id="时态数据-temporal-data"><a href="#时态数据-temporal-data" class="headerlink" title="时态数据 (temporal data)"></a>时态数据 (temporal data)</h4><p>在不同时间点记录的数据。例如，记录的一年中每一天的冬外套销量就属于时态数据。</p><h4 id="张量-Tensor"><a href="#张量-Tensor" class="headerlink" title="张量 (Tensor)"></a>张量 (Tensor)</h4><p>TensorFlow 程序中的主要数据结构。<br>张量是 N 维（其中 N 可能非常大）数据结构，最常见的是标量、向量或矩阵。<br>张量的元素可以包含整数值、浮点值或字符串值。</p><h4 id="张量处理单元-TPU-Tensor-Processing-Unit"><a href="#张量处理单元-TPU-Tensor-Processing-Unit" class="headerlink" title="张量处理单元 (TPU, Tensor Processing Unit)"></a>张量处理单元 (TPU, Tensor Processing Unit)</h4><p>一种 ASIC（应用专用集成电路），用于优化 TensorFlow 程序的性能。</p><h4 id="张量等级-Tensor-rank"><a href="#张量等级-Tensor-rank" class="headerlink" title="张量等级 (Tensor rank)"></a>张量等级 (Tensor rank)</h4><p>请参阅[<strong>等级</strong>]。</p><h4 id="张量形状-Tensor-shape"><a href="#张量形状-Tensor-shape" class="headerlink" title="张量形状 (Tensor shape)"></a>张量形状 (Tensor shape)</h4><p>[<strong>张量</strong>]在各种维度中包含的元素数。<br>例如，张量 [5, 10] 在一个维度中的形状为 5，在另一个维度中的形状为 10。</p><h4 id="张量大小-Tensor-size"><a href="#张量大小-Tensor-size" class="headerlink" title="张量大小 (Tensor size)"></a>张量大小 (Tensor size)</h4><p>[<strong>张量</strong>]包含的标量总数。<br>例如，张量 [5, 10] 的大小为 50。</p><h4 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h4><p>一个信息中心，用于显示在执行一个或多个 TensorFlow 程序期间保存的摘要信息。</p><h4 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h4><p>一个大型的分布式机器学习平台。该术语还指 TensorFlow 堆栈中的基本 API 层，该层支持对数据流图进行一般计算。</p><p>虽然 TensorFlow 主要应用于机器学习领域，但也可用于需要使用数据流图进行数值计算的非机器学习任务。</p><h4 id="TensorFlow-Playground"><a href="#TensorFlow-Playground" class="headerlink" title="TensorFlow Playground"></a>TensorFlow Playground</h4><p>一款用于直观呈现不同的[<strong>超参数</strong>]对模型（主要是神经网络）训练的影响的程序。</p><p>要试用 TensorFlow Playground，请前往 <a href="http://playground.tensorflow.org/">http://playground.tensorflow.org</a>。</p><h4 id="TensorFlow-Serving"><a href="#TensorFlow-Serving" class="headerlink" title="TensorFlow Serving"></a>TensorFlow Serving</h4><p>一个平台，用于将训练过的模型部署到生产环境。</p><h4 id="测试集-test-set"><a href="#测试集-test-set" class="headerlink" title="测试集 (test set)"></a>测试集 (test set)</h4><p>数据集的子集，用于在[<strong>模型</strong>]经由验证集的初步验证之后测试模型。</p><p>与[<strong>训练集</strong>]和[<strong>验证集</strong>]相对。</p><h4 id="tf-Example"><a href="#tf-Example" class="headerlink" title="tf.Example"></a>tf.Example</h4><p>一种标准[协议缓冲区]，旨在描述用于机器学习模型训练或推断的输入数据。</p><h4 id="时间序列分析-time-series-analysis"><a href="#时间序列分析-time-series-analysis" class="headerlink" title="时间序列分析 (time series analysis)"></a>时间序列分析 (time series analysis)</h4><p>机器学习和统计学的一个子领域，旨在分析[<strong>时态数据</strong>]。<br>很多类型的机器学习问题都需要时间序列分析，其中包括分类、聚类、预测和异常检测。例如，您可以利用时间序列分析根据历史销量数据预测未来每月的冬外套销量。</p><h4 id="训练-training"><a href="#训练-training" class="headerlink" title="训练 (training)"></a>训练 (training)</h4><p>确定构成模型的理想[<strong>参数</strong>]的过程。</p><h4 id="训练集-training-set"><a href="#训练集-training-set" class="headerlink" title="训练集 (training set)"></a>训练集 (training set)</h4><p>数据集的子集，用于训练模型。</p><p>与[<strong>验证集</strong>]和[<strong>测试集</strong>]相对。</p><h4 id="迁移学习-transfer-learning"><a href="#迁移学习-transfer-learning" class="headerlink" title="迁移学习 (transfer learning)"></a>迁移学习 (transfer learning)</h4><p>将信息从一个机器学习任务迁移到另一个机器学习任务。例如，在多任务学习中，一个模型可以完成多项任务，例如针对不同任务具有不同输出节点的[<strong>深度模型</strong>]。迁移学习可能涉及将知识从较简单任务的解决方案迁移到较复杂的任务，或者将知识从数据较多的任务迁移到数据较少的任务。</p><p>大多数机器学习系统都只能完成一项任务。迁移学习是迈向人工智能的一小步；在人工智能中，单个程序可以完成多项任务。</p><h4 id="平移不变性-translational-invariance"><a href="#平移不变性-translational-invariance" class="headerlink" title="平移不变性 (translational invariance)"></a>平移不变性 (translational invariance)</h4><p>在图像分类问题中，即使图像中对象的位置发生变化，算法也能成功对图像进行分类。例如，无论一只狗位于画面正中央还是画面左侧，该算法仍然可以识别它。</p><p>另请参阅[<strong>大小不变性</strong>]和[<strong>旋转不变性</strong>]。</p><h4 id="负例-TN-true-negative"><a href="#负例-TN-true-negative" class="headerlink" title="负例 (TN, true negative)"></a>负例 (TN, true negative)</h4><p>被模型正确地预测为[<strong>负类别</strong>]的样本。例如，模型推断出某封电子邮件不是垃圾邮件，而该电子邮件确实不是垃圾邮件。</p><h4 id="正例-TP-true-positive"><a href="#正例-TP-true-positive" class="headerlink" title="正例 (TP, true positive)"></a>正例 (TP, true positive)</h4><p>被模型正确地预测为[<strong>正类别</strong>]的样本。例如，模型推断出某封电子邮件是垃圾邮件，而该电子邮件确实是垃圾邮件。</p><h4 id="正例率（true-positive-rate-简称-TP-率）"><a href="#正例率（true-positive-rate-简称-TP-率）" class="headerlink" title="正例率（true positive rate, 简称 TP 率）"></a>正例率（true positive rate, 简称 TP 率）</h4><p>与[<strong>召回率</strong>]的含义相同，即：<br>$$<br>正例率　= \frac{正例数}{正例数+假负例数}<br>$$<br>正例率是 [<strong>ROC 曲线</strong>]的 y 轴。</p><h3 id="U"><a href="#U" class="headerlink" title="U"></a>U</h3><h4 id="无标签样本-unlabeled-example"><a href="#无标签样本-unlabeled-example" class="headerlink" title="无标签样本 (unlabeled example)"></a>无标签样本 (unlabeled example)</h4><p>包含[<strong>特征</strong>]但没有[<strong>标签</strong>]的样本。<br>无标签样本是用于进行[<strong>推断</strong>]的输入内容。<br>在[<strong>半监督式</strong>]和[<strong>非监督式</strong>]学习中，在训练期间会使用无标签样本。</p><h4 id="非监督式机器学习-unsupervised-machine-learning"><a href="#非监督式机器学习-unsupervised-machine-learning" class="headerlink" title="非监督式机器学习 (unsupervised machine learning)"></a>非监督式机器学习 (unsupervised machine learning)</h4><p>训练[<strong>模型</strong>]，以找出数据集（通常是无标签数据集）中的规律。</p><p>非监督式机器学习最常见的用途是将数据分为不同的聚类，使相似的样本位于同一组中。<br>例如，非监督式机器学习算法可以根据音乐的各种属性将歌曲分为不同的聚类。<br>所得聚类可以作为其他机器学习算法（例如音乐推荐服务）的输入。<br>在很难获取真标签的领域，聚类可能会非常有用。<br>例如，在反滥用和反欺诈等领域，聚类有助于人们更好地了解相关数据。</p><p>非监督式机器学习的另一个例子是[<strong>主成分分析 (PCA)</strong>]。<br>例如，通过对包含数百万购物车中物品的数据集进行主成分分析，可能会发现有柠檬的购物车中往往也有抗酸药。</p><p>请与[<strong>监督式机器学习</strong>]进行比较。</p><h3 id="V"><a href="#V" class="headerlink" title="V"></a>V</h3><h4 id="验证集-validation-set"><a href="#验证集-validation-set" class="headerlink" title="验证集 (validation set)"></a>验证集 (validation set)</h4><p>数据集的一个子集，从训练集分离而来，用于调整[<strong>超参数</strong>]。</p><p>与[<strong>训练集</strong>]和[<strong>测试集</strong>]相对。</p><h3 id="W"><a href="#W" class="headerlink" title="W"></a>W</h3><h4 id="权重-weight"><a href="#权重-weight" class="headerlink" title="权重 (weight)"></a>权重 (weight)</h4><p>线性模型中[<strong>特征</strong>]的系数，或深度网络中的边。训练线性模型的目标是确定每个特征的理想权重。如果权重为 0，则相应的特征对模型来说没有任何贡献。</p><h4 id="宽度模型-wide-model"><a href="#宽度模型-wide-model" class="headerlink" title="宽度模型 (wide model)"></a>宽度模型 (wide model)</h4><p>一种线性模型，通常有很多[<strong>稀疏输入特征</strong>]。<br>我们之所以称之为“宽度模型”，是因为这是一种特殊类型的[<strong>神经网络</strong>]，其大量输入均直接与输出节点相连。与深度模型相比，宽度模型通常更易于调试和检查。虽然宽度模型无法通过[<strong>隐藏层</strong>]来表示非线性关系，但可以利用[<strong>特征组合</strong>]、[<strong>分桶</strong>]等转换以不同的方式为非线性关系建模。</p><p>与[<strong>深度模型</strong>]相对。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine_Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于特征匹配的实时平面目标检测算法</title>
      <link href="/blog/2022/01/20/ji-yu-te-zheng-pi-pei-de-shi-shi-ping-mian-mu-biao-jian-ce-suan-fa/"/>
      <url>/blog/2022/01/20/ji-yu-te-zheng-pi-pei-de-shi-shi-ping-mian-mu-biao-jian-ce-suan-fa/</url>
      
        <content type="html"><![CDATA[<p>一直想基于传统图像匹配方式做一个融合Demo，也算是对上个阶段学习的一个总结。<br>由此，便采购了一个摄像头，在此基础上做了实时检测平面目标的特征匹配算法。<br>代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># coding: utf-8</span><span class="token triple-quoted-string string">'''@author: linxu@contact: 17746071609@163.com@time: 2021-07-26 上午11:54@desc: 基于特征匹配的实时平面目标检测算法@Ref: https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.htm'''</span><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> cv2<span class="token keyword">class</span> <span class="token class-name">ObjectDetector</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">""" 基于特征匹配的实时平面目标检测算法 """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 特征点检测-选择不同的特征描述子</span>        self<span class="token punctuation">.</span>feature_detector <span class="token operator">=</span> cv2<span class="token punctuation">.</span>AKAZE_create<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># Use AKAZE</span>        <span class="token comment"># self.feature_detector = cv2.ORB_create() # Use ORB</span>        <span class="token comment"># self.feature_detector = cv2.KAZE_create()# Use KAZE</span>        <span class="token comment"># self.feature_detector = cv2.SIFT_create()# Use SIFT</span>        <span class="token comment"># self.feature_detector = cv2.BRISK_create()# Use BRISK</span>        <span class="token comment"># 摄像头相机参数设置VideoCapture</span>        self<span class="token punctuation">.</span>vidcap <span class="token operator">=</span> cv2<span class="token punctuation">.</span>VideoCapture<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>vidcap<span class="token punctuation">.</span><span class="token builtin">set</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">640</span><span class="token punctuation">)</span> <span class="token comment"># 宽度</span>        self<span class="token punctuation">.</span>vidcap<span class="token punctuation">.</span><span class="token builtin">set</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">480</span><span class="token punctuation">)</span> <span class="token comment"># 高度</span>        self<span class="token punctuation">.</span>vidcap<span class="token punctuation">.</span><span class="token builtin">set</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">)</span>  <span class="token comment"># 帧率</span>        <span class="token comment"># 通过ROI（感兴趣区域）来注册目标对象</span>        self<span class="token punctuation">.</span>sub_topleft <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">220</span><span class="token punctuation">]</span> <span class="token comment"># [0, 0] # [y,x]100 220</span>        self<span class="token punctuation">.</span>sub_width <span class="token operator">=</span> <span class="token number">200</span> <span class="token comment">#640 200</span>        self<span class="token punctuation">.</span>sub_height <span class="token operator">=</span> <span class="token number">200</span> <span class="token comment">#480 200</span>        self<span class="token punctuation">.</span>sub_bottomright <span class="token operator">=</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>sub_topleft<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>sub_height <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span>\        self<span class="token punctuation">.</span>sub_topleft<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>sub_width <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span>        <span class="token comment"># rect矩形框体</span>        self<span class="token punctuation">.</span>rect_color <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment"># green</span>        self<span class="token punctuation">.</span>rect_thickness <span class="token operator">=</span> <span class="token number">3</span>        self<span class="token punctuation">.</span>rect_tl_outer_xy <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>sub_topleft<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>rect_thickness<span class="token punctuation">,</span> self<span class="token punctuation">.</span>sub_topleft<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>rect_thickness<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>rect_br_outer_xy <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>sub_bottomright<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>rect_thickness<span class="token punctuation">,</span> self<span class="token punctuation">.</span>sub_bottomright<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>rect_thickness<span class="token punctuation">)</span>        <span class="token comment"># 特征（描述符）向量距离的阈值</span>        self<span class="token punctuation">.</span>ratio <span class="token operator">=</span> <span class="token number">0.75</span>        self<span class="token punctuation">.</span>registered <span class="token operator">=</span> <span class="token boolean">False</span>        self<span class="token punctuation">.</span>min_match_count <span class="token operator">=</span> <span class="token number">5</span>        self<span class="token punctuation">.</span>show_rectangle <span class="token operator">=</span> <span class="token boolean">True</span>    <span class="token keyword">def</span> <span class="token function">register</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""注册目标对象"""</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\n将目标物体靠近相机."</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"确保对象完全覆盖矩形内部（背景不可见）."</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"然后，按“r”注册对象.\n"</span><span class="token punctuation">)</span>        <span class="token keyword">while</span> self<span class="token punctuation">.</span>vidcap<span class="token punctuation">.</span>isOpened<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            ret<span class="token punctuation">,</span> frame <span class="token operator">=</span> self<span class="token punctuation">.</span>vidcap<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>            cv2<span class="token punctuation">.</span>rectangle<span class="token punctuation">(</span>frame<span class="token punctuation">,</span> self<span class="token punctuation">.</span>rect_tl_outer_xy<span class="token punctuation">,</span> self<span class="token punctuation">.</span>rect_br_outer_xy<span class="token punctuation">,</span>\                          self<span class="token punctuation">.</span>rect_color<span class="token punctuation">,</span> self<span class="token punctuation">.</span>rect_thickness<span class="token punctuation">)</span>            cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">"Registration (press 'r' to register)"</span><span class="token punctuation">,</span> frame<span class="token punctuation">)</span>            <span class="token keyword">if</span> cv2<span class="token punctuation">.</span>waitKey<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token number">0xFF</span> <span class="token operator">==</span> <span class="token builtin">ord</span><span class="token punctuation">(</span><span class="token string">'r'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token comment"># 图像切片</span>                subimg <span class="token operator">=</span> frame<span class="token punctuation">[</span>self<span class="token punctuation">.</span>sub_topleft<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>sub_topleft<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>sub_height<span class="token punctuation">)</span><span class="token punctuation">,</span>                               self<span class="token punctuation">.</span>sub_topleft<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>sub_topleft<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>sub_width<span class="token punctuation">)</span><span class="token punctuation">]</span>                self<span class="token punctuation">.</span>kp0<span class="token punctuation">,</span> self<span class="token punctuation">.</span>des0 <span class="token operator">=</span> self<span class="token punctuation">.</span>feature_detector<span class="token punctuation">.</span>detectAndCompute<span class="token punctuation">(</span>subimg<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>queryimg <span class="token operator">=</span> subimg                self<span class="token punctuation">.</span>registered <span class="token operator">=</span> <span class="token boolean">True</span>                <span class="token keyword">break</span>    <span class="token keyword">def</span> <span class="token function">detect</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">""" 使用特征点查找对象 """</span>        <span class="token keyword">global</span> mask        <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>registered<span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Call 'register()' first."</span><span class="token punctuation">)</span>            <span class="token keyword">return</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Start detection..."</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"按“q”退出."</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"按“h”隐藏绿色矩形.\n"</span><span class="token punctuation">)</span>        <span class="token comment"># 声明一个暴力匹配器Blute-Force (BF) matcher</span>        bf <span class="token operator">=</span> cv2<span class="token punctuation">.</span>BFMatcher<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">while</span> self<span class="token punctuation">.</span>vidcap<span class="token punctuation">.</span>isOpened<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            ret<span class="token punctuation">,</span> frame <span class="token operator">=</span> self<span class="token punctuation">.</span>vidcap<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token comment"># 关键点（kp）检测和计算描述符（des）</span>            kp<span class="token punctuation">,</span> des <span class="token operator">=</span> self<span class="token punctuation">.</span>feature_detector<span class="token punctuation">.</span>detectAndCompute<span class="token punctuation">(</span>frame<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>            <span class="token comment"># 在关键点之间应用knn匹配</span>            matches <span class="token operator">=</span> bf<span class="token punctuation">.</span>knnMatch<span class="token punctuation">(</span>self<span class="token punctuation">.</span>des0<span class="token punctuation">,</span> des<span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>            <span class="token comment"># 根据阈值筛选关键特征点</span>            <span class="token comment"># good = [[m] for m, n in matches if m.distance &lt; self.ratio * n.distance]</span>            good <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>            <span class="token keyword">for</span> m<span class="token punctuation">,</span> n <span class="token keyword">in</span> matches<span class="token punctuation">:</span>                <span class="token keyword">if</span> m<span class="token punctuation">.</span>distance <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>ratio <span class="token operator">*</span> n<span class="token punctuation">.</span>distance<span class="token punctuation">:</span>                    good<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span>m<span class="token punctuation">]</span><span class="token punctuation">)</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'len'</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>good<span class="token punctuation">)</span><span class="token punctuation">)</span>            contours <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>            <span class="token comment"># 查找单应性矩阵</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>good<span class="token punctuation">)</span> <span class="token operator">></span> self<span class="token punctuation">.</span>min_match_count<span class="token punctuation">)</span> <span class="token keyword">and</span> self<span class="token punctuation">.</span>show_rectangle<span class="token punctuation">:</span>                <span class="token comment"># 建立坐标矩阵</span>                src_pts <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>kp0<span class="token punctuation">[</span>m<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>queryIdx<span class="token punctuation">]</span><span class="token punctuation">.</span>pt <span class="token keyword">for</span> m <span class="token keyword">in</span> good<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>                dst_pts <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span>kp<span class="token punctuation">[</span>m<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>trainIdx<span class="token punctuation">]</span><span class="token punctuation">.</span>pt <span class="token keyword">for</span> m <span class="token keyword">in</span> good<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>                <span class="token comment"># 计算多个二维点对之间的最优单映射变换矩阵 H</span>                M<span class="token punctuation">,</span> mask <span class="token operator">=</span> cv2<span class="token punctuation">.</span>findHomography<span class="token punctuation">(</span>src_pts<span class="token punctuation">,</span> dst_pts<span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>RANSAC<span class="token punctuation">,</span> <span class="token number">5.0</span><span class="token punctuation">)</span>                <span class="token comment"># Assume color camera</span>                <span class="token comment"># cv2.imshow('queryimg',self.queryimg)</span>                h<span class="token punctuation">,</span> w<span class="token punctuation">,</span> c <span class="token operator">=</span> self<span class="token punctuation">.</span>queryimg<span class="token punctuation">.</span>shape                pts <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> h<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>w<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> h<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>w<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>                dst <span class="token operator">=</span> cv2<span class="token punctuation">.</span>perspectiveTransform<span class="token punctuation">(</span>pts<span class="token punctuation">,</span> M<span class="token punctuation">)</span>                <span class="token comment"># cv2.circle(frame,tuple(dst_pts[0][0]),5, (255,0,0))</span>                <span class="token comment"># cv2.circle(frame, tuple(dst_pts[1][0]), 5, (0, 255, 0))</span>                <span class="token comment"># cv2.circle(frame, tuple(dst_pts[2][0]), 5, (0, 0, 255))</span>                <span class="token comment"># cv2.circle(frame, tuple(dst_pts[3][0]), 5, (255, 255, 0))</span>                <span class="token comment"># cv2.imshow('circle', frame)</span>                frame <span class="token operator">=</span> cv2<span class="token punctuation">.</span>polylines<span class="token punctuation">(</span>frame<span class="token punctuation">,</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">(</span>dst<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>LINE_AA<span class="token punctuation">)</span>            <span class="token comment"># 可视化匹配过程</span>            <span class="token comment"># 绘画参数</span>            <span class="token comment"># draw_params = dict(flags=2)</span>            draw_params <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>matchColor<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> singlePointColor<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">)</span><span class="token punctuation">,</span>flags<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>            img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>drawMatchesKnn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>queryimg<span class="token punctuation">,</span> self<span class="token punctuation">.</span>kp0<span class="token punctuation">,</span> frame<span class="token punctuation">,</span> kp<span class="token punctuation">,</span> good<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token operator">**</span>draw_params<span class="token punctuation">)</span>            cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">"Detection (press 'q' to quit)"</span><span class="token punctuation">,</span> img<span class="token punctuation">)</span>            key_pressed <span class="token operator">=</span> cv2<span class="token punctuation">.</span>waitKey<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> key_pressed <span class="token operator">&amp;</span> <span class="token number">0xFF</span> <span class="token operator">==</span> <span class="token builtin">ord</span><span class="token punctuation">(</span><span class="token string">'q'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">break</span>            <span class="token keyword">if</span> key_pressed <span class="token operator">&amp;</span> <span class="token number">0xFF</span> <span class="token operator">==</span> <span class="token builtin">ord</span><span class="token punctuation">(</span><span class="token string">'h'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                self<span class="token punctuation">.</span>show_rectangle <span class="token operator">=</span> <span class="token boolean">False</span>    <span class="token keyword">def</span> <span class="token function">close</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">""" 释放VideoCapture并销毁windows """</span>        self<span class="token punctuation">.</span>vidcap<span class="token punctuation">.</span>release<span class="token punctuation">(</span><span class="token punctuation">)</span>        cv2<span class="token punctuation">.</span>destroyAllWindows<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    obj_detector <span class="token operator">=</span> ObjectDetector<span class="token punctuation">(</span><span class="token punctuation">)</span>    obj_detector<span class="token punctuation">.</span>register<span class="token punctuation">(</span><span class="token punctuation">)</span>    obj_detector<span class="token punctuation">.</span>detect<span class="token punctuation">(</span><span class="token punctuation">)</span>    obj_detector<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>测试效果，如下：</p><img src="/blog/blog/2022/01/20/ji-yu-te-zheng-pi-pei-de-shi-shi-ping-mian-mu-biao-jian-ce-suan-fa/1571518-20210731094043726-1934831855.png" style="zoom: 50%;"><img src="/blog/blog/2022/01/20/ji-yu-te-zheng-pi-pei-de-shi-shi-ping-mian-mu-biao-jian-ce-suan-fa/1571518-20210731094121015-639529574.png" style="zoom: 50%;">]]></content>
      
      
      <categories>
          
          <category> 特征匹配 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object_Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>yolov5_efficient</title>
      <link href="/blog/2022/01/20/yolov5-efficient/"/>
      <url>/blog/2022/01/20/yolov5-efficient/</url>
      
        <content type="html"><![CDATA[<h1 id="Yolov5-Efficient"><a href="#Yolov5-Efficient" class="headerlink" title="Yolov5_Efficient"></a>Yolov5_Efficient</h1><p><img src="/blog/blog/2022/01/20/yolov5-efficient/yolov5-efficient/1571518-20211105103947692-337341194.png"></p><h2 id="Use-yolov5-efficiently-高效地使用Yolo-v5"><a href="#Use-yolov5-efficiently-高效地使用Yolo-v5" class="headerlink" title="Use yolov5 efficiently(高效地使用Yolo v5)"></a>Use yolov5 efficiently(高效地使用Yolo v5)</h2><h2 id="1-Introduction-介绍"><a href="#1-Introduction-介绍" class="headerlink" title="1.Introduction-介绍"></a>1.Introduction-介绍</h2><p>The repository is reconstructed and annotated based on UltralyTICS / YOLOV5, and other functions are added thereto, such as automatic annotation with the Grab Cut, and the pointing center point.</p><p>该存储库基于Ultralytics/yolov5进行重构与注释，并且在此基础上加入其他功能，例如自动标注与漫水填充GrabCut，以及绘制中心点进行连线。</p><hr><h2 id="2-Performance-效果表现"><a href="#2-Performance-效果表现" class="headerlink" title="2.Performance-效果表现"></a>2.Performance-效果表现</h2><h3 id="2-1-Yolo-detect-GrabCut"><a href="#2-1-Yolo-detect-GrabCut" class="headerlink" title="2.1 Yolo detect+GrabCut"></a>2.1 Yolo detect+GrabCut</h3><table><thead><tr><th>detect预测结果</th><th>GrabCut分割结果</th></tr></thead><tbody><tr><td><img src="/blog/blog/2022/01/20/yolov5-efficient/yolov5-efficient/1571518-20211105110409690-1455179243.png"></td><td><img src="/blog/blog/2022/01/20/yolov5-efficient/yolov5-efficient/1571518-20211105110418858-940040936.png"></td></tr><tr><td><img src="/blog/blog/2022/01/20/yolov5-efficient/yolov5-efficient/1571518-20211105110426763-812179903.png"></td><td><img src="/blog/blog/2022/01/20/yolov5-efficient/yolov5-efficient/1571518-20211105110435881-328445319.png"></td></tr></tbody></table><p><a href="https://www.bilibili.com/video/BV1ET4y1o7ZR/"><strong>demo video</strong></a></p><h3 id="2-2-Yolo-center-point-line-中心点连线"><a href="#2-2-Yolo-center-point-line-中心点连线" class="headerlink" title="2.2 Yolo center point line 中心点连线"></a>2.2 Yolo center point line 中心点连线</h3><table><thead><tr><th>demo1</th><th>demo2</th></tr></thead><tbody><tr><td><img src="/blog/blog/2022/01/20/yolov5-efficient/yolov5-efficient/1571518-20211105110651730-834327083.png"></td><td><img src="/blog/blog/2022/01/20/yolov5-efficient/yolov5-efficient/1571518-20211105110705075-1370353606.png"></td></tr></tbody></table><h3 id="2-3-autolabel半自动标注"><a href="#2-3-autolabel半自动标注" class="headerlink" title="2.3 autolabel半自动标注"></a>2.3 autolabel半自动标注</h3><p><a href="https://www.bilibili.com/video/BV1ET4y1o7ZR/">demo video</a></p><h2 id="3-How-to-Use-用法"><a href="#3-How-to-Use-用法" class="headerlink" title="3.How to Use-用法"></a>3.How to Use-用法</h2><h3 id="3-1-Train-训练自定义数据集"><a href="#3-1-Train-训练自定义数据集" class="headerlink" title="3.1 Train 训练自定义数据集"></a>3.1 Train 训练自定义数据集</h3><p>可参考Ultralytics/yolov5的训练用法：</p><h4 id="3-1-1命令行方式："><a href="#3-1-1命令行方式：" class="headerlink" title="3.1.1命令行方式："></a>3.1.1命令行方式：</h4><p>单独使用：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python yolov5_master&#x2F;train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>集成使用：</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python yolov5_master&#x2F;main.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/blog/blog/2022/01/20/yolov5-efficient/yolov5-efficient/1571518-20211105111817968-708151746.png"></p><h4 id="3-1-2-IDE方式："><a href="#3-1-2-IDE方式：" class="headerlink" title="3.1.2 IDE方式："></a>3.1.2 IDE方式：</h4><p><img src="/blog/blog/2022/01/20/yolov5-efficient/yolov5-efficient/1571518-20211105112013475-1447251119.png"></p><h3 id="3-2-detect-推理预测"><a href="#3-2-detect-推理预测" class="headerlink" title="3.2 detect 推理预测"></a>3.2 detect 推理预测</h3><h4 id="3-2-1命令行方式："><a href="#3-2-1命令行方式：" class="headerlink" title="3.2.1命令行方式："></a>3.2.1命令行方式：</h4><p>单独使用-测试图片</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python yolov5_master&#x2F;detect.py --source .&#x2F;testfiles&#x2F;img1.jpg --weights runs&#x2F;train&#x2F;bmyolov5s&#x2F;weights&#x2F;best.pt <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>单独使用-测试视频</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python yolov5_master&#x2F;detect.py --source .&#x2F;testfiles&#x2F;video.mp4 --weights runs&#x2F;train&#x2F;bmyolov5s&#x2F;weights&#x2F;best.pt <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>集成使用</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python yolov5_master&#x2F;main.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>集成方式中包含参数设置与训练、预测的配置选择，可自行更换。</p><h3 id="3-3-Yolo-detect-GrabCut使用"><a href="#3-3-Yolo-detect-GrabCut使用" class="headerlink" title="3.3 Yolo detect+GrabCut使用"></a>3.3 Yolo detect+GrabCut使用</h3><p>直接在IDE中修改配置即可。</p><h3 id="3-4-Yolo-center-point-line使用"><a href="#3-4-Yolo-center-point-line使用" class="headerlink" title="3.4 Yolo center point line使用"></a>3.4 Yolo center point line使用</h3><p>直接在IDE中修改配置即可。</p><h3 id="3-5-autolabel自动标注"><a href="#3-5-autolabel自动标注" class="headerlink" title="3.5 autolabel自动标注"></a>3.5 autolabel自动标注</h3><p>细节在注释中介绍，直接在IDE中修改配置即可。</p>]]></content>
      
      
      <categories>
          
          <category> Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object_Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YOLOv3_Detect_Web</title>
      <link href="/blog/2022/01/20/yolov3/"/>
      <url>/blog/2022/01/20/yolov3/</url>
      
        <content type="html"><![CDATA[<h1 id="YOLOv3-Detect-Web"><a href="#YOLOv3-Detect-Web" class="headerlink" title="YOLOv3_Detect_Web"></a>YOLOv3_Detect_Web</h1><p>Use Yolov3 detect on Web</p><p><img src="/blog/blog/2022/01/20/yolov3/yolov3/1571518-20211229095207661-1020206979.png"></p><p>使用 YOLOv3（PyTorch 和 Django 实现）的对象检测应用程序。<br>网页和 REST API由Django Web框架实现。</p><h1 id="1-Introduction-介绍"><a href="#1-Introduction-介绍" class="headerlink" title="1. Introduction 介绍"></a>1. Introduction 介绍</h1><hr><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><p>这是一个使用 YOLOv3 提供对象检测并生成 REST API 的 Web 应用程序。<br>它是使用 Django 框架和 PyTorch（用于 YOLO 模型）实现的。<br>这里开发了接受图像作为请求的 Django API，API 的响应是 JSON 对象。<br>输入图像被转换为 float32 类型的 NumPy 数组并传递给 YOLOv3 对象检测模型。<br>该模型对图像执行对象检测，并生成一个 JSON 对象，其中包含所有对象的名称及其在图像中各自的置信度。</p><h2 id="1-2-视频Demo"><a href="#1-2-视频Demo" class="headerlink" title="1.2 视频Demo"></a>1.2 视频Demo</h2><iframe src="//player.bilibili.com/player.html?aid=378475774&bvid=BV1Bf4y1c797&cid=419899362&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><h1 id="2-Required-Libraries-依赖库对应版本及环境配置"><a href="#2-Required-Libraries-依赖库对应版本及环境配置" class="headerlink" title="2. Required Libraries 依赖库对应版本及环境配置"></a>2. Required Libraries 依赖库对应版本及环境配置</h1><hr><h2 id="2-1-所需依赖库"><a href="#2-1-所需依赖库" class="headerlink" title="2.1  所需依赖库"></a>2.1  所需依赖库</h2><p>下面提到了所需的库及其版本：</p><ul><li>Python  (3.7)</li><li>Django  (3.0.3)</li><li>PyTorch (1.3.1)</li><li>Pillow  (7.1.2)</li><li>OpenCV  (4.2.0)</li><li>NumPy   (1.18.5)</li></ul><p>可见requirements.txt。</p><h2 id="2-2-配置测试环境"><a href="#2-2-配置测试环境" class="headerlink" title="2.2 配置测试环境"></a>2.2 配置测试环境</h2><ul><li>利用Anaconda创建名为web的虚拟环境</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda create -n web python&#x3D;3.7<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>进入虚拟环境</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda activate web<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>根据requirements文件在清华源下进行依赖库安装(推荐使用)</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">pip install -r requirements.txt -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="3-Required-files-for-Detection-检测必需的文件"><a href="#3-Required-files-for-Detection-检测必需的文件" class="headerlink" title="3. Required files for Detection 检测必需的文件"></a>3. Required files for Detection 检测必需的文件</h1><hr><p>要使用预训练模型的对象检测，我们需要三个重要文件，分别为以下：</p><ul><li><strong>yolov3.cfg - cfg：</strong><br>该文件用来逐块描述网络的布局。官方 cfg 文件可在Darknet github 存储库中找到。<br>但是，为了获得更好的性能，我对配置文件做了一些更改。</li><li><strong>yolov3.weights:</strong><br>我们使用来自 darknet53 模型的权重。</li><li><strong>coco.names:</strong><br>文件包含我们的模型经过训练可以识别的不同对象的名称。</li></ul><h1 id="4-Steps-to-Follow-Working"><a href="#4-Steps-to-Follow-Working" class="headerlink" title="4.Steps to Follow (Working)"></a>4.Steps to Follow (Working)</h1><hr><p>这个存储库可以做两件事：</p><ol><li>基于YOLOv3和Django的网页程序应用实现</li><li>REST API的生成（API测试使用POSTMAN完成）</li></ol><h2 id="4-1-网页应用实现"><a href="#4-1-网页应用实现" class="headerlink" title="4.1 网页应用实现"></a>4.1 网页应用实现</h2><ul><li>**step-1.**克隆 GitHub 存储库</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git clone https:&#x2F;&#x2F;github.com&#x2F;isLinXu&#x2F;YOLOv3_Detect_Web.git<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>**step-2.**将目录更改为克隆的 Repository 文件夹。</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cd YOLOv3_Detect_Web<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><strong>step-3</strong>.由于.cfg 和 coco.names 文件已在此存储库中默认设置好， 可根据需要进行自行修改。<br>现在，我们需要做的就是下载权重文件。<br>在命令提示符中使用以下命令下载 yolov3.weights：</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">wget https:&#x2F;&#x2F;pjreddie.com&#x2F;media&#x2F;files&#x2F;yolov3.weights<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><strong>step-4</strong>.安装所有必需的库。</li><li>**step-5.**执行下面的代码：（这条命令只需要执行一次，用来初始化创建）</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python manage.py collectstatic<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>此命令启动 Django 并收集所有静态文件。</p><ul><li><strong>step-6</strong>.然后，开始服务：</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python manage.py runserver<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>此命令启动 Django 服务器。</p><p>现在我们都准备好运行应用程序了。</p><ul><li><strong>step-7</strong>..执行上述代码后，您将看到如下内容：</li></ul><p><img src="/blog/blog/2022/01/20/yolov3/yolov3/1571518-20211229102043283-1298501310.png"></p><ul><li><p>**step-8.**点击链接。这会将您定向到 Web 浏览器。</p><p><img src="/blog/blog/2022/01/20/yolov3/yolov3/1571518-20211229102544422-65710418.png"></p></li><li><p><strong>step-9</strong>.通过拖放或浏览模式选择图像。</p><p><img src="/blog/blog/2022/01/20/yolov3/yolov3/1571518-20211229102538414-994383395.png"></p></li><li><p><strong>setp-10</strong>:上传图片</p></li></ul><p><img src="/blog/blog/2022/01/20/yolov3/yolov3/1571518-20211229111240890-1808341010.png"><br><img src="/blog/blog/2022/01/20/yolov3/yolov3/1571518-20211229112155005-1239105137.png"></p><ul><li><p><strong>step-11</strong>: 点击DEDECT-OBJECT，进行检测图片，这时会将结果解析为json并显示出来。<br>Django Web-app 的输入是一个图像。此输入图像被转换为 float32 类型的 NumPy 数组并传递给 YOLOv3 模型。<br>该模型对图像执行对象检测，并生成一个 JSON 对象，其中包含所有对象的名称及其在图像中各自的频率。</p><p><img src="/blog/blog/2022/01/20/yolov3/yolov3/1571518-20211229111348163-1486954962.png"><br>表单响应是 JSON 对象。此 JSON 对象如上所示显示。</p></li><li><p><strong>step-12:</strong> 单击“Show Predictions”显示检测结果,查看带有边界框的图像。</p></li></ul><p><img src="/blog/blog/2022/01/20/yolov3/yolov3/1571518-20211229111605379-1714549773.png"></p><ul><li><strong>step-13:</strong> 要尝试其他图像，请单击”Choose a New File”</li></ul><h2 id="4-2-REST-API-实现——POSTMAN"><a href="#4-2-REST-API-实现——POSTMAN" class="headerlink" title="4.2 REST API 实现——POSTMAN"></a>4.2 REST API 实现——POSTMAN</h2><p>Postman 是一个可扩展的 API 测试工具。要遵循的步骤是：</p><ol><li><p>按照上面提到的前 6 个步骤进行操作。</p></li><li><p>确保服务器正常运行</p><p><img src="/blog/blog/2022/01/20/yolov3/yolov3/1571518-20211229112728941-539819044.png"></p></li><li><p>打开 POSTMAN 并选择 POST 选项。输入上面显示的服务器链接并将 /object_detection/api_request/ 附加到它。</p></li><li><p>点击body,输入key value作为”image”，选择图片文件点击“Send”进行发送</p></li><li><p>输入图像被转换为 float32 类型的 NumPy 数组并传递给 YOLOv3 模型。该模型对图像执行对象检测，并生成一个 JSON 对象，其中包含所有对象的名称及其在图像中各自的频率。</p><p><img src="/blog/blog/2022/01/20/yolov3/yolov3/1571518-20211229115830026-325581645.png"></p></li><li><p>HttpResponse 是 JSON 对象。其中此 JSON 对象如上所示显示。</p></li></ol><p>例如：127.0.0.1:8000/object_detection/api_request/</p>]]></content>
      
      
      <categories>
          
          <category> 算法模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object_Detection </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
