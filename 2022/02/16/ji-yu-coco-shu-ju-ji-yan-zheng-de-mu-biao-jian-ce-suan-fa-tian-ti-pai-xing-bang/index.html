<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="基于COCO数据集验证的目标检测算法天梯排行榜, LinXu">
    <meta name="description" content="基于COCO数据集验证的目标检测算法天梯排行榜
AP50



Rank
Model
box AP
AP50
Paper
Code
Result
Year
Tags



1
SwinV2-G (HTC++)
63.1

Swin Tran">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>基于COCO数据集验证的目标检测算法天梯排行榜 | LinXu</title>
    <link rel="icon" type="image/png" href="/blog/favicon.png">

    <link rel="stylesheet" type="text/css" href="/blog/libs/awesome/css/all.min.css">
    <link rel="stylesheet" type="text/css" href="/blog/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/blog/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/blog/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/blog/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/blog/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/blog/css/my.css">

    <script src="/blog/libs/jquery/jquery-3.6.0.min.js"></script>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/blog/" class="waves-effect waves-light">
                    
                    <img src="/blog/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">LinXu</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/blog/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/blog/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/blog/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/blog/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="" class="waves-effect waves-light">

      
      <i class="fas fa-list" style="zoom: 0.6;"></i>
      
      <span>关于</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/blog/about">
          
          <i class="fas fa-users" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>About</span>
        </a>
      </li>
      
      <li>
        <a href="/blog/cv">
          
          <i class="fas fa-files" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>cv</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="" class="waves-effect waves-light">

      
      <i class="fas fa-list" style="zoom: 0.6;"></i>
      
      <span>Medias</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/blog/musics">
          
          <i class="fas fa-music" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>Musics</span>
        </a>
      </li>
      
      <li>
        <a href="/blog/movies">
          
          <i class="fas fa-film" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>Movies</span>
        </a>
      </li>
      
      <li>
        <a href="/blog/books">
          
          <i class="fas fa-book" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>Books</span>
        </a>
      </li>
      
      <li>
        <a href="/blog/galleries">
          
          <i class="fas fa-image" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>Galleries</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/blog/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">LinXu</div>
        <div class="logo-desc">
            
            广积粮 高筑墙 广称王
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/blog/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/blog/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/blog/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/blog/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-list"></i>
			
			关于
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/blog/about " style="margin-left:75px">
				  
				   <i class="fa fas fa-users" style="position: absolute;left:50px" ></i>
			      
		          <span>About</span>
                  </a>
                </li>
              
                <li>

                  <a href="/blog/cv " style="margin-left:75px">
				  
				   <i class="fa fas fa-files" style="position: absolute;left:50px" ></i>
			      
		          <span>cv</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-list"></i>
			
			Medias
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/blog/musics " style="margin-left:75px">
				  
				   <i class="fa fas fa-music" style="position: absolute;left:50px" ></i>
			      
		          <span>Musics</span>
                  </a>
                </li>
              
                <li>

                  <a href="/blog/movies " style="margin-left:75px">
				  
				   <i class="fa fas fa-film" style="position: absolute;left:50px" ></i>
			      
		          <span>Movies</span>
                  </a>
                </li>
              
                <li>

                  <a href="/blog/books " style="margin-left:75px">
				  
				   <i class="fa fas fa-book" style="position: absolute;left:50px" ></i>
			      
		          <span>Books</span>
                  </a>
                </li>
              
                <li>

                  <a href="/blog/galleries " style="margin-left:75px">
				  
				   <i class="fa fas fa-image" style="position: absolute;left:50px" ></i>
			      
		          <span>Galleries</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/isLinXu/isLinXu.github.io.git" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/isLinXu/isLinXu.github.io.git" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="/blog/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/blog/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/blog/medias/featureimages/26.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">基于COCO数据集验证的目标检测算法天梯排行榜</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/blog/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/blog/tags/Object-Detection/">
                                <span class="chip bg-color">Object_Detection</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2022-02-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2022-04-29
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    105 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/blog/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="基于COCO数据集验证的目标检测算法天梯排行榜"><a href="#基于COCO数据集验证的目标检测算法天梯排行榜" class="headerlink" title="基于COCO数据集验证的目标检测算法天梯排行榜"></a>基于COCO数据集验证的目标检测算法天梯排行榜</h1><hr>
<h2 id="AP50"><a href="#AP50" class="headerlink" title="AP50"></a>AP50</h2><p><img src="/blog/blog/2022/02/16/ji-yu-coco-shu-ju-ji-yan-zheng-de-mu-biao-jian-ce-suan-fa-tian-ti-pai-xing-bang/%E5%9F%BA%E4%BA%8ECOCO%E6%95%B0%E6%8D%AE%E9%9B%86%E9%AA%8C%E8%AF%81%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E5%A4%A9%E6%A2%AF%E6%8E%92%E8%A1%8C%E6%A6%9C/1571518-20220216175113001-141667941.jpg"></p>
<table>
<thead>
<tr>
<th align="center">Rank</th>
<th>Model</th>
<th><strong>box AP</strong></th>
<th>AP50</th>
<th>Paper</th>
<th>Code</th>
<th>Result</th>
<th>Year</th>
<th>Tags</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and">SwinV2-G (HTC++)</a></td>
<td>63.1</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and">Swin Transformer V2: Scaling Up Capacity and Resolution</a></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and#code">Link</a></td>
<td></td>
<td>2021</td>
<td><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/florence-a-new-foundation-model-for-computer">Florence-CoSwin-H</a></td>
<td>62.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/florence-a-new-foundation-model-for-computer">Florence: A New Foundation Model for Computer Vision</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/grounded-language-image-pre-training">GLIP (Swin-L, multi-scale)</a></td>
<td>61.5</td>
<td>79.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/grounded-language-image-pre-training">Grounded Language-Image Pre-training</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong>;<br>Vision Language**;<br><strong>Dynamic Head</strong>;<br>**BERT-Base</td>
</tr>
<tr>
<td align="center">4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/end-to-end-semi-supervised-object-detection">Soft Teacher + Swin-L (HTC++, multi-scale)</a></td>
<td>61.3</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/end-to-end-semi-supervised-object-detection">End-to-End Semi-Supervised Object Detection with Soft Teacher</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong>;<br>Swin-Transformer</td>
</tr>
<tr>
<td align="center">5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">DyHead (Swin-L, multi scale, self-training)</a></td>
<td>60.6</td>
<td>78.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">Dynamic Head: Unifying Object Detection Heads with Attentions</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong>;<br>Swin-Transformer</td>
</tr>
<tr>
<td align="center">6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">Dual-Swin-L (HTC, multi-scale)</a></td>
<td>60.1</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">CBNetV2: A Composite Backbone Network Architecture for Object Detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br>Swin-Transformer</td>
</tr>
<tr>
<td align="center">7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">Dual-Swin-L (HTC, single-scale)</a></td>
<td>59.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">CBNetV2: A Composite Backbone Network Architecture for Object Detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal-L (DyHead, multi-scale)</a></td>
<td>58.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal Self-attention for Local-Global Interactions in Vision Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br>Focal-Transformer</td>
</tr>
<tr>
<td align="center">9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">DyHead (Swin-L, multi scale)</a></td>
<td>58.7</td>
<td>77.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">Dynamic Head: Unifying Object Detection Heads with Attentions</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br>Swin-Transformer</td>
</tr>
<tr>
<td align="center">10</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin-L (HTC++, multi scale)</a></td>
<td>58.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">11</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal-L (HTC++, multi-scale)</a></td>
<td>58.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal Self-attention for Local-Global Interactions in Vision Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">12</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin-L (HTC++, single scale)</a></td>
<td>57.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">13</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-D6 (1280, single-scale, 34 fps)</a></td>
<td>57.3</td>
<td>75.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br>YOLO</td>
</tr>
<tr>
<td align="center">14</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (Swin-L, single)</a></td>
<td>56.5</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Transformer</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">15</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-E6 (1280, single-scale, 45 fps)</a></td>
<td>56.4</td>
<td>74.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">16</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/probabilistic-two-stage-detection">CenterNet2 (Res2Net-101-DCN-BiFPN, self-training, 1560 single-scale)</a></td>
<td>56.4</td>
<td>74.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/probabilistic-two-stage-detection">Probabilistic two-stage detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>FPN</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">17</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/queryinst-parallelly-supervised-mask-query">QueryInst (single-scale)</a></td>
<td>56.1</td>
<td>75.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/queryinst-parallelly-supervised-mask-query">Instances as Queries</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">18</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P7 with TTA</a></td>
<td>55.8</td>
<td>73.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">19</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-64x4d, multi-scale)</a></td>
<td>55.7</td>
<td>74.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">20</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-W6 (1280, single-scale, 66 fps)</a></td>
<td>55.5</td>
<td>73.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">21</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P7 CSP-P7 (single-scale, 16 fps)</a></td>
<td>55.4</td>
<td>73.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">22</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural">CSP-p6 + Mish (multi-scale)</a></td>
<td>55.2</td>
<td>72.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural">Mish: A Self Regularized Non-Monotonic Activation Function</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">23</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P6 with TTA</a></td>
<td>54.9</td>
<td>72.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">24</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/simple-copy-paste-is-a-strong-data">Cascade Eff-B7 NAS-FPN (1280)</a></td>
<td>54.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/simple-copy-paste-is-a-strong-data">Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>NAS-FPN</strong></td>
</tr>
<tr>
<td align="center">25</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-32x4d, multi-scale)</a></td>
<td>54.7</td>
<td>73.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">26</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P6 CSP-P6 (single-scale, 32 fps)</a></td>
<td>54.3</td>
<td>72.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">27</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/rethinking-pre-training-and-self-training">SpineNet-190 (1280, with Self-training on OpenImages, single-scale)</a></td>
<td>54.3</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/rethinking-pre-training-and-self-training">Rethinking Pre-training and Self-training</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">28</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08d (Res2Net-101, DCN, multi-scale)</a></td>
<td>54.1</td>
<td>71.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">29</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet-D7 (single-scale)</a></td>
<td>53.7</td>
<td>72.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet: Scalable and Efficient Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">30</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/probabilistic-anchor-assignment-with-iou">PAA (ResNext-152-32x8d + DCN, multi-scale)</a></td>
<td>53.5</td>
<td>71.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/probabilistic-anchor-assignment-with-iou">Probabilistic Anchor Assignment with IoU Prediction for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">31</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/location-sensitive-visual-recognition-with">LSNet (Res2Net-101+ DCN, multi-scale)</a></td>
<td>53.5</td>
<td>71.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/location-sensitive-visual-recognition-with">Location-Sensitive Visual Recognition with Cross-IOU Loss</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">32</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/resnest-split-attention-networks">ResNeSt-200 (multi-scale)</a></td>
<td>53.3</td>
<td>72.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/resnest-split-attention-networks">ResNeSt: Split-Attention Networks</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">33</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnet-a-novel-composite-backbone-network">Cascade Mask R-CNN (Triple-ResNeXt152, multi-scale)</a></td>
<td>53.3</td>
<td>71.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnet-a-novel-composite-backbone-network">CBNet: A Novel Composite Backbone Network Architecture for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">34</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-32x4d, single-scale)</a></td>
<td>53.3</td>
<td>71.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">35</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (Res2Net-101, DCN, multiscale)</a></td>
<td>53.3</td>
<td>70.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">36</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/relationnet-bridging-visual-representations">RelationNet++ (ResNeXt-64x4d-101-DCN)</a></td>
<td>52.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/relationnet-bridging-visual-representations">RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">37</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P5 with TTA</a></td>
<td>52.5</td>
<td>70.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">38</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-detr-deformable-transformers-for-1">Deformable DETR (ResNeXt-101+DCN)</a></td>
<td>52.3</td>
<td>71.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-detr-deformable-transformers-for-1">Deformable DETR: Deformable Transformers for End-to-End Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">39</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/global-context-networks">GCNet (ResNeXt-101 + DCN + cascade + GC r4)</a></td>
<td>52.3</td>
<td>70.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/global-context-networks">Global Context Networks</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong><br><strong>GCN</strong></td>
</tr>
<tr>
<td align="center">40</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-190, 1280x1280)</a></td>
<td>52.1</td>
<td>71.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">41</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints v2 (ResNeXt-101, DCN, multi-scale)</a></td>
<td>52.1</td>
<td>70.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints V2: Verification Meets Regression for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong>;<br><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="center"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="center">42</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">AC-FPN Cascade R-CNN (X-152-32x8d-FPN-IN5k, multi scale, only CEM)</a></td>
<td>51.9</td>
<td>70.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">Attention-guided Context Feature Pyramid Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">43</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/ota-optimal-transport-assignment-for-object">OTA (ResNeXt-101+DCN, multiscale)</a></td>
<td>51.5</td>
<td>68.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/ota-optimal-transport-assignment-for-object">OTA: Optimal Transport Assignment for Object Detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">44</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08d (Res2Net-101, DCN, single-scale)</a></td>
<td>51.3</td>
<td>70.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">45</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">TSD (SENet154-DCN,multi-scale)</a></td>
<td>51.2</td>
<td>71.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">Revisiting the Sibling Head in Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">46</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolox-exceeding-yolo-series-in-2021">YOLOX-X (Modified CSP v5)</a></td>
<td>51.2</td>
<td>69.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolox-exceeding-yolo-series-in-2021">YOLOX: Exceeding YOLO Series in 2021</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">47</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-143, 1280x1280)</a></td>
<td>50.7</td>
<td>70.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">48</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bridging-the-gap-between-anchor-based-and">ATSS (ResNetXt-64x4d-101+DCN,multi-scale)</a></td>
<td>50.7</td>
<td>68.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bridging-the-gap-between-anchor-based-and">Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">49</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/learning-data-augmentation-strategies-for">NAS-FPN (AmoebaNet-D, learned aug)</a></td>
<td>50.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/learning-data-augmentation-strategies-for">Learning Data Augmentation Strategies for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">50</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (Res2Net-101, DCN)</a></td>
<td>50.6</td>
<td>69</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">51</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, DCN, multiscale test)</a></td>
<td>50.2</td>
<td>70.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">52</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-equalizing-pyramid-convolution-for">FreeAnchor + SEPC (DCN, ResNext-101-64x4d)</a></td>
<td>50.1</td>
<td>69.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-equalizing-pyramid-convolution-for">Scale-Equalizing Pyramid Convolution for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">53</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/d2det-towards-high-quality-object-detection">D2Det (ResNet-101-DCN, multi-scale test)</a></td>
<td>50.1</td>
<td>69.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/d2det-towards-high-quality-object-detection">D2Det: Towards High Quality Object Detection and Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">54</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-r-cnn-towards-high-quality-object">Dynamic R-CNN (ResNet-101-DCN, multi-scale)</a></td>
<td>50.1</td>
<td>68.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-r-cnn-towards-high-quality-object">Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">55</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">TSD (ResNet-101-Deformable, Image Pyramid)</a></td>
<td>49.4</td>
<td>69.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">Revisiting the Sibling Head in Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">56</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints v2 (ResNeXt-101, DCN)</a></td>
<td>49.4</td>
<td>68.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints V2: Verification Meets Regression for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">57</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/corner-proposal-network-for-anchor-free-two">CPNDet (Hourglass-104, multi-scale)</a></td>
<td>49.2</td>
<td>67.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/corner-proposal-network-for-anchor-free-two">Corner Proposal Network for Anchor-free, Two-stage Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">58</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNeXt-101, 32x4d, DCN)</a></td>
<td>49</td>
<td>67.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">59</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, DCN, single scale)</a></td>
<td>48.9</td>
<td>69.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">60</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08 (Res2Net-50, DCN, single-scale)</a></td>
<td>48.8</td>
<td>67.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">61</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (ResNet101, single scale)</a></td>
<td>48.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Transformer</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">62</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-96, 1024x1024)</a></td>
<td>48.6</td>
<td>68.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">63</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">TridentNet (ResNet-101-Deformable, Image Pyramid)</a></td>
<td>48.4</td>
<td>69.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">Scale-Aware Trident Networks for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">64</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze">GCNet (ResNeXt-101 + DCN + cascade + GC r4)</a></td>
<td>48.4</td>
<td>67.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong><br><strong>GCN</strong></td>
</tr>
<tr>
<td align="center">65</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-101-DCN)</a></td>
<td>48.3</td>
<td>66.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">66</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-learning-qualified-and">GFL (X-101-32x4d-DCN, single-scale)</a></td>
<td>48.2</td>
<td>67.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-learning-qualified-and">Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">67</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet101-FPN-3x, single-scale)</a></td>
<td>48.1</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">68</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, single scale)</a></td>
<td>47.8</td>
<td>68.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">69</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/matrix-nets-a-new-deep-architecture-for">MatrixNet Corners (ResNet-152, multi-scale)</a></td>
<td>47.8</td>
<td>66.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/matrix-nets-a-new-deep-architecture-for">Matrix Nets: A New Deep Architecture for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">70</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (ResNet50, single scale)</a></td>
<td>47.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Transformer</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">71</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/soft-anchor-point-object-detection">SAPD (ResNeXt-101, single-scale)</a></td>
<td>47.4</td>
<td>67.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/soft-anchor-point-object-detection">Soft Anchor-Point Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">72</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/path-aggregation-network-for-instance">PANet (ResNeXt-101, multi-scale)</a></td>
<td>47.4</td>
<td>67.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/path-aggregation-network-for-instance">Path Aggregation Network for Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">73</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">HTC (HRNetV2p-W48)</a></td>
<td>47.3</td>
<td>65.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">74</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/hybrid-task-cascade-for-instance-segmentation">HTC (ResNeXt-101-FPN)</a></td>
<td>47.1</td>
<td>63.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/hybrid-task-cascade-for-instance-segmentation">Hybrid Task Cascade for Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">75</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centernet-object-detection-with-keypoint">CenterNet511 (Hourglass-104, multi-scale)</a></td>
<td>47.0</td>
<td>64.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centernet-object-detection-with-keypoint">CenterNet: Keypoint Triplets for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">76</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNeXt101, multi-scale)</a></td>
<td>47.0</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">77</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet50-FPN-3x)</a></td>
<td>46.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">78</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49, 896x896)</a></td>
<td>46.7</td>
<td>66.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">79</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101-DCN, multi-scale)</a></td>
<td>46.5</td>
<td>67.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">80</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/houghnet-integrating-near-and-long-range">HoughNet (MS)</a></td>
<td>46.4</td>
<td>65.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/houghnet-integrating-near-and-long-range">HoughNet: Integrating near and long-range evidence for bottom-up object detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">81</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reducing-label-noise-in-anchor-free-object">PPDet (ResNeXt-101-FPN, multiscale)</a></td>
<td>46.3</td>
<td>64.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reducing-label-noise-in-anchor-free-object">Reducing Label Noise in Anchor-Free Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">82</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-101)</a></td>
<td>46.2</td>
<td>64.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">83</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER (ResNet-101)</a></td>
<td>46.1</td>
<td>67.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER: Efficient Multi-Scale Training</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">84</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Mask R-CNN (HRNetV2p-W48 + cascade)</a></td>
<td>46.1</td>
<td>64.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">85</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-convnets-v2-more-deformable-better">DCNv2 (ResNet-101, multi-scale)</a></td>
<td>46.0</td>
<td>67.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-convnets-v2-more-deformable-better">Deformable ConvNets v2: More Deformable, Better Results</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">86</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/localization-uncertainty-estimation-for">Gaussian-FCOS</a></td>
<td>46</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/localization-uncertainty-estimation-for">Localization Uncertainty Estimation for Anchor-Free Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td></td>
</tr>
<tr>
<td align="center">87</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/instaboost-boosting-instance-segmentation-via">Cascade R-CNN-FPN (ResNet-101, map-guided)</a></td>
<td>45.9</td>
<td>64.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/instaboost-boosting-instance-segmentation-via">InstaBoost: Boosting Instance Segmentation via Probability Map Guided Copy-Pasting</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">88</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNeXt101, single-scale)</a></td>
<td>45.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">89</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask+VoVNetV2-99 (single-scale)</a></td>
<td>45.8</td>
<td>64.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">90</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">D-RFCN + SNIP (DPN-98 with flip, multi-scale)</a></td>
<td>45.7</td>
<td>67.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">An Analysis of Scale Invariance in Object Detection - SNIP</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">91</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4 (CD53)</a></td>
<td>45.5</td>
<td>64.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">92</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO (608x608)</a></td>
<td>45.2</td>
<td>65.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">93</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">AC-FPN Cascade R-CNN (ResNet-101, single scale)</a></td>
<td>45</td>
<td>64.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">Attention-guided Context Feature Pyramid Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong><br><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">94</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/freeanchor-learning-to-match-anchors-for">FreeAnchor (ResNeXt-101)</a></td>
<td>44.8</td>
<td>64.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/freeanchor-learning-to-match-anchors-for">FreeAnchor: Learning to Match Anchors for Visual Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">95</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-64x4d-101-FPN 4 + improvements)</a></td>
<td>44.7</td>
<td>64.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">96</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask+VoVNet2-57 (single-scale)</a></td>
<td>44.7</td>
<td>63.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">97</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">FSAF (ResNeXt-101, multi-scale)</a></td>
<td>44.6</td>
<td>65.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">98</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101, DCN, 500 scale)</a></td>
<td>44.6</td>
<td>65.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">99</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask + X-101-32x8d (single-scale)</a></td>
<td>44.6</td>
<td>63.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">100</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49, 640x640)</a></td>
<td>44.3</td>
<td>63.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">101</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-look-one-level-feature">YOLOF-DC5</a></td>
<td>44.3</td>
<td>62.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-look-one-level-feature">You Only Look One-level Feature</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">102</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-50)</a></td>
<td>44.3</td>
<td>62.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">103</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-intertwiner-for-object-detection-1">InterNet (ResNet-101-FPN, multi-scale)</a></td>
<td>44.2</td>
<td>67.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-intertwiner-for-object-detection-1">Feature Intertwiner for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong><br><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">104</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (VGG-16, multi-scale)</a></td>
<td>44.2</td>
<td>64.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">105</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/lip-local-importance-based-pooling">Faster R-CNN (LIP-ResNet-101-MD w FPN)</a></td>
<td>43.9</td>
<td>65.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/lip-local-importance-based-pooling">LIP: Local Importance-based Pooling</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">106</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (ResNet-101, multi-scale)</a></td>
<td>43.9</td>
<td>64.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>multiscale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">107</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/learning-spatial-fusion-for-single-shot">YOLOv3 @800 + ASFF* (Darknet-53)</a></td>
<td>43.9</td>
<td>64.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/learning-spatial-fusion-for-single-shot">Learning Spatial Fusion for Single-Shot Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">108</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td>
<td>43.9</td>
<td>63.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">109</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">ExtremeNet (Hourglass-104, multi-scale)</a></td>
<td>43.7</td>
<td>60.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">Bottom-up Object Detection by Grouping Extreme and Center Points</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">110</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolov4-optimal-speed-and-accuracy-of-object">YOLOv4-608</a></td>
<td>43.5</td>
<td>65.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolov4-optimal-speed-and-accuracy-of-object">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">111</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER (ResNet-50)</a></td>
<td>43.5</td>
<td>65.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER: Efficient Multi-Scale Training</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">112</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">CenterNet (HRNetV2-W48)</a></td>
<td>43.5</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">113</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">D-RFCN + SNIP (ResNet-101, multi-scale)</a></td>
<td>43.4</td>
<td>65.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">An Analysis of Scale Invariance in Object Detection - SNIP</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>multiscale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">114</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/grid-r-cnn">Grid R-CNN (ResNeXt-101-FPN)</a></td>
<td>43.2</td>
<td>63.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/grid-r-cnn">Grid R-CNN</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">115</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-101-64x4d-FPN)</a></td>
<td>43.2</td>
<td>62.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">116</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190408900">CornerNet-Saccade (Hourglass-104, multi-scale)</a></td>
<td>43.2</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190408900">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">117</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/libra-r-cnn-towards-balanced-learning-for">Libra R-CNN (ResNeXt-101-FPN)</a></td>
<td>43.0</td>
<td>64</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/libra-r-cnn-towards-balanced-learning-for">Libra R-CNN: Towards Balanced Learning for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">118</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101-DCN)</a></td>
<td>42.8</td>
<td>65.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">119</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet-49 (640, RetinaNet, single-scale)</a></td>
<td>42.8</td>
<td>62.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">120</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-101-FPN+, cascade)</a></td>
<td>42.8</td>
<td>62.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">121</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-high-quality-object-detection">Cascade R-CNN</a></td>
<td>42.8</td>
<td>62.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-high-quality-object-detection">Cascade R-CNN: High Quality Object Detection and Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">122</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">TridentNet (ResNet-101)</a></td>
<td>42.7</td>
<td>63.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">Scale-Aware Trident Networks for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">123</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-32x8d-101-FPN)</a></td>
<td>42.7</td>
<td>62.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">124</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask (ResNeXt-101-FPN-GN)</a></td>
<td>42.6</td>
<td>62.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">125</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/tood-task-aligned-one-stage-object-detection">TAL + TAP</a></td>
<td>42.5</td>
<td>60.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/tood-task-aligned-one-stage-object-detection">TOOD: Task-aligned One-stage Object Detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">126</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Faster R-CNN (HRNetV2p-W48)</a></td>
<td>42.4</td>
<td>63.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">127</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/hierarchical-shot-detector">HSD (Rest101, 768x768, single-scale test)</a></td>
<td>42.3</td>
<td>61.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/hierarchical-shot-detector">Hierarchical Shot Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">128</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet511 (Hourglass-104, multi-scale)</a></td>
<td>42.1</td>
<td>57.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet: Detecting Objects as Paired Keypoints</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">129</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td>
<td>42.1</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">130</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (HRNet-W32-5l)</a></td>
<td>42.0</td>
<td>60.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">131</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512+ (ResNet-101)</a></td>
<td>41.8</td>
<td>62.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">132</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/gradient-harmonized-single-stage-detector">GHM-C + GHM-R (RetinaNet-FPN-ResNeXt-101)</a></td>
<td>41.6</td>
<td>62.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/gradient-harmonized-single-stage-detector">Gradient Harmonized Single-stage Detector</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">133</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/objects-as-points">CenterNet-DLA (DLA-34, multi-scale)</a></td>
<td>41.6</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/objects-as-points">Objects as Points</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">134</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49S, 640x640)</a></td>
<td>41.5</td>
<td>60.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">135</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101)</a></td>
<td>41</td>
<td>62.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">136</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (VGG-16, single-scale)</a></td>
<td>41.0</td>
<td>59.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">137</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">FSAF (ResNet-101, single-scale)</a></td>
<td>40.9</td>
<td>61.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">138</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">RetinaNet (ResNeXt-101-FPN)</a></td>
<td>40.8</td>
<td>61.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">Focal Loss for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">139</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-50-FPN+, cascade)</a></td>
<td>40.6</td>
<td>59.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">140</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Faster R-CNN (Cascade RPN)</a></td>
<td>40.6</td>
<td>58.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">141</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-kernels-adapting-effective">ResNet-50-DW-DPN (Deformable Kernels)</a></td>
<td>40.6</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-kernels-adapting-effective">Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">142</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/acquisition-of-localization-confidence-for">IoU-Net</a></td>
<td>40.6</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/acquisition-of-localization-confidence-for">Acquisition of Localization Confidence for Accurate Object Detection</a></td>
<td></td>
<td></td>
<td>2018</td>
<td></td>
</tr>
<tr>
<td align="center">143</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">FCOS (HRNetV2p-W48)</a></td>
<td>40.5</td>
<td>59.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">144</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/softer-nms-rethinking-bounding-box-regression">ResNet-50-FPN Mask R-CNN + KL Loss + var voting + soft-NMS</a></td>
<td>40.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/softer-nms-rethinking-bounding-box-regression">Bounding Box Regression with Uncertainty for Accurate Object Detection</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">145</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/rdsnet-a-new-deep-architecture-for-reciprocal">RDSNet (ResNet-101, RetinaNet, mask, MBRM)</a></td>
<td>40.3</td>
<td>60.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/rdsnet-a-new-deep-architecture-for-reciprocal">RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">146</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">ExtremeNet (Hourglass-104, single-scale)</a></td>
<td>40.2</td>
<td>55.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">Bottom-up Object Detection by Grouping Extreme and Center Points</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">147</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cross-iteration-batch-normalization">Mask R-CNN (ResNet-101-FPN, CBN)</a></td>
<td>40.1</td>
<td>60.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cross-iteration-batch-normalization">Cross-Iteration Batch Normalization</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">148</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Fast R-CNN (Cascade RPN)</a></td>
<td>40.1</td>
<td>59.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">149</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN (ResNeXt-101-FPN)</a></td>
<td>39.8</td>
<td>62.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">150</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/region-proposal-by-guided-anchoring">GA-Faster-RCNN</a></td>
<td>39.8</td>
<td>59.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/region-proposal-by-guided-anchoring">Region Proposal by Guided Anchoring</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">151</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/chainercv-a-library-for-deep-learning-in">FPN (ResNet101 backbone)</a></td>
<td>39.5</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/chainercv-a-library-for-deep-learning-in">ChainerCV: a Library for Deep Learning in Computer Vision</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">152</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask (ResNet-50-FPN)</a></td>
<td>39.4</td>
<td>58.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">153</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO (320x320)</a></td>
<td>39.3</td>
<td>59.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">154</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190409925">AA-ResNet-10 + RetinaNet</a></td>
<td>39.2</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190409925">Attention Augmented Convolutional Networks</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">155</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNet50, single-scale)</a></td>
<td>39.2</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">156</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">RetinaNet (ResNet-101-FPN)</a></td>
<td>39.1</td>
<td>59.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">Focal Loss for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">157</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-101-FPN+)</a></td>
<td>38.8</td>
<td>61.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">158</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (ResNet-101, single-scale)</a></td>
<td>38.8</td>
<td>59.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>single scale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">159</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/saccadenet-a-fast-and-accurate-object">SaccadeNet (DLA-34-DCN)</a></td>
<td>38.5</td>
<td>55.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/saccadenet-a-fast-and-accurate-object">SaccadeNet: A Fast and Accurate Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">160</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN (ResNet-101-FPN)</a></td>
<td>38.2</td>
<td>60.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">161</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/segmentation-is-all-you-need">WSMA-Seg</a></td>
<td>38.1</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/segmentation-is-all-you-need">Segmentation is All You Need</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">162</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Faster R-CNN + FPN + CGD</a></td>
<td>37.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Compact Global Descriptor for Neural Networks</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">163</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet511 (Hourglass-52, single-scale)</a></td>
<td>37.8</td>
<td>53.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet: Detecting Objects as Paired Keypoints</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">164</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512+ (VGG-16)</a></td>
<td>37.6</td>
<td>58.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">165</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-convolutional-networks">DeformConv-R-FCN (Aligned-Inception-ResNet)</a></td>
<td>37.5</td>
<td>58.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-convolutional-networks">Deformable Convolutional Networks</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">166</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data">Faster R-CNN (ImageNet+300M)</a></td>
<td>37.4</td>
<td>58</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">167</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">Mask R-CNN (Bottleneck-injected ResNet-50, FPN)</a></td>
<td>36.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">168</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/beyond-skip-connections-top-down-modulation">Faster R-CNN + TDM</a></td>
<td>36.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/beyond-skip-connections-top-down-modulation">Beyond Skip Connections: Top-Down Modulation for Object Detection</a></td>
<td></td>
<td></td>
<td>2016</td>
<td></td>
</tr>
<tr>
<td align="center">169</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-50-FPN+)</a></td>
<td>36.5</td>
<td>59</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong>;<br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">170</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512 (ResNet-101)</a></td>
<td>36.4</td>
<td>57.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">171</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection">Faster R-CNN + FPN</a></td>
<td>36.2</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection">Feature Pyramid Networks for Object Detection</a></td>
<td></td>
<td></td>
<td>2016</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">172</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">Faster R-CNN (Bottleneck-injected ResNet-50 and FPN)</a></td>
<td>35.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>FPN</strong>;<br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">173</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition">Faster R-CNN (box refinement, context, multi-scale testing)</a></td>
<td>34.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition">Deep Residual Learning for Image Recognition</a></td>
<td></td>
<td></td>
<td>2015</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">174</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/speedaccuracy-trade-offs-for-modern">Faster R-CNN</a></td>
<td>34.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/speedaccuracy-trade-offs-for-modern">Speed/accuracy trade-offs for modern convolutional object detectors</a></td>
<td></td>
<td></td>
<td>2016</td>
<td></td>
</tr>
<tr>
<td align="center">175</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190408900">CornerNet-Squeeze</a></td>
<td>34.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190408900">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">176</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-multipath-network-for-object-detection">MultiPath Network</a></td>
<td>33.2</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-multipath-network-for-object-detection">A MultiPath Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2016</td>
<td></td>
</tr>
<tr>
<td align="center">177</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/inside-outside-net-detecting-objects-in">ION</a></td>
<td>33.1</td>
<td>55.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/inside-outside-net-detecting-objects-in">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</a></td>
<td></td>
<td></td>
<td>2015</td>
<td></td>
</tr>
<tr>
<td align="center">178</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512 (VGG-16)</a></td>
<td>33</td>
<td>54.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">179</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolov3-an-incremental-improvement">YOLOv3 + Darknet-53</a></td>
<td>33.0</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolov3-an-incremental-improvement">YOLOv3: An Incremental Improvement</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">180</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/ssd-single-shot-multibox-detector">SSD512</a></td>
<td>28.8</td>
<td>48.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/ssd-single-shot-multibox-detector">SSD: Single Shot MultiBox Detector</a></td>
<td></td>
<td></td>
<td>2015</td>
<td></td>
</tr>
<tr>
<td align="center">181</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MobileNetV2)</a></td>
<td>26.1</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">182</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/espnetv2-a-light-weight-power-efficient-and">ESPNetv2-512</a></td>
<td>26.0</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/espnetv2-a-light-weight-power-efficient-and">ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td></td>
</tr>
<tr>
<td align="center">183</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MobileNetV3)</a></td>
<td>25.5</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">184</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MNASNet-B1)</a></td>
<td>24.6</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">185</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN x0.7 (MobileNetV2)</a></td>
<td>23.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">186</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">MobielNet-v1-SSD-300x300+CGD</a></td>
<td>21.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Compact Global Descriptor for Neural Networks</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">187</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fast-r-cnn">Fast-RCNN</a></td>
<td>19.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fast-r-cnn">Fast R-CNN</a></td>
<td></td>
<td></td>
<td>2015</td>
<td></td>
</tr>
<tr>
<td align="center">188</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mobilenets-efficient-convolutional-neural">MobileNet</a></td>
<td>19.3</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mobilenets-efficient-convolutional-neural">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">189</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/vision-transformer-with-deformable-attention">DAT-S (RetinaNet)</a></td>
<td></td>
<td>69.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/vision-transformer-with-deformable-attention">Vision Transformer with Deformable Attention</a></td>
<td></td>
<td></td>
<td>2022</td>
<td></td>
</tr>
<tr>
<td align="center">190</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask-VoVNet99 (multi-scale)</a></td>
<td></td>
<td>68.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">191</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Mask R-CNN (HRNetV2p-W32 + cascade)</a></td>
<td></td>
<td>62.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">192</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td>
<td></td>
<td>61.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">193</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/virtex-learning-visual-representations-from">VirTex Mask R-CNN (ResNet-50-FPN)</a></td>
<td></td>
<td>61.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/virtex-learning-visual-representations-from">VirTex: Learning Visual Representations from Textual Annotations</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>FPN</strong>;<br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">194</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">Centermask + ResNet101</a></td>
<td></td>
<td>61.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">195</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pafnet-an-efficient-anchor-free-object">PAFNet (ResNet50-vd)</a></td>
<td></td>
<td>59.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pafnet-an-efficient-anchor-free-object">PAFNet: An Efficient Anchor-Free Object Detector Guidance</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">196</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dctd-deep-conditional-target-densities-for">IoU-Net+EnergyRegression</a></td>
<td></td>
<td>58.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dctd-deep-conditional-target-densities-for">Energy-Based Models for Deep Probabilistic Regression</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">197</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Cascade R-CNN (HRNetV2p-W48)</a></td>
<td></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">198</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet50-FPN-3x, single-scale)</a></td>
<td></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">199</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td>
<td></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">200</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet-D7x (single-scale)</a></td>
<td></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet: Scalable and Efficient Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
</tbody></table>
<h2 id="AP75"><a href="#AP75" class="headerlink" title="AP75"></a>AP75</h2><p><img src="/blog/blog/2022/02/16/ji-yu-coco-shu-ju-ji-yan-zheng-de-mu-biao-jian-ce-suan-fa-tian-ti-pai-xing-bang/%E5%9F%BA%E4%BA%8ECOCO%E6%95%B0%E6%8D%AE%E9%9B%86%E9%AA%8C%E8%AF%81%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E5%A4%A9%E6%A2%AF%E6%8E%92%E8%A1%8C%E6%A6%9C/1571518-20220216180708632-632288615.jpg"></p>
<table>
<thead>
<tr>
<th align="center">Rank</th>
<th>Model</th>
<th><strong>box AP</strong></th>
<th>AP75</th>
<th>Paper</th>
<th>Code</th>
<th>Result</th>
<th>Year</th>
<th>Tags</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and">SwinV2-G (HTC++)</a></td>
<td>63.1</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and">Swin Transformer V2: Scaling Up Capacity and Resolution</a></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and#code">Link</a></td>
<td></td>
<td>2021</td>
<td><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/florence-a-new-foundation-model-for-computer">Florence-CoSwin-H</a></td>
<td>62.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/florence-a-new-foundation-model-for-computer">Florence: A New Foundation Model for Computer Vision</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/grounded-language-image-pre-training">GLIP (Swin-L, multi-scale)</a></td>
<td>61.5</td>
<td>67.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/grounded-language-image-pre-training">Grounded Language-Image Pre-training</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong>;<br>Vision Language**;<br><strong>Dynamic Head</strong>;<br>**BERT-Base</td>
</tr>
<tr>
<td align="center">4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/end-to-end-semi-supervised-object-detection">Soft Teacher + Swin-L (HTC++, multi-scale)</a></td>
<td>61.3</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/end-to-end-semi-supervised-object-detection">End-to-End Semi-Supervised Object Detection with Soft Teacher</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong>;<br>Swin-Transformer</td>
</tr>
<tr>
<td align="center">5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">DyHead (Swin-L, multi scale, self-training)</a></td>
<td>60.6</td>
<td>66.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">Dynamic Head: Unifying Object Detection Heads with Attentions</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong>;<br>Swin-Transformer</td>
</tr>
<tr>
<td align="center">6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">Dual-Swin-L (HTC, multi-scale)</a></td>
<td>60.1</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">CBNetV2: A Composite Backbone Network Architecture for Object Detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br>Swin-Transformer</td>
</tr>
<tr>
<td align="center">7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">Dual-Swin-L (HTC, single-scale)</a></td>
<td>59.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">CBNetV2: A Composite Backbone Network Architecture for Object Detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal-L (DyHead, multi-scale)</a></td>
<td>58.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal Self-attention for Local-Global Interactions in Vision Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br>Focal-Transformer</td>
</tr>
<tr>
<td align="center">9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">DyHead (Swin-L, multi scale)</a></td>
<td>58.7</td>
<td>64.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">Dynamic Head: Unifying Object Detection Heads with Attentions</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br>Swin-Transformer</td>
</tr>
<tr>
<td align="center">10</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin-L (HTC++, multi scale)</a></td>
<td>58.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">11</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal-L (HTC++, multi-scale)</a></td>
<td>58.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal Self-attention for Local-Global Interactions in Vision Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">12</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin-L (HTC++, single scale)</a></td>
<td>57.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">13</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-D6 (1280, single-scale, 34 fps)</a></td>
<td>57.3</td>
<td>62.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br>YOLO</td>
</tr>
<tr>
<td align="center">14</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (Swin-L, single)</a></td>
<td>56.5</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Transformer</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">15</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-E6 (1280, single-scale, 45 fps)</a></td>
<td>56.4</td>
<td>61.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">16</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/probabilistic-two-stage-detection">CenterNet2 (Res2Net-101-DCN-BiFPN, self-training, 1560 single-scale)</a></td>
<td>56.4</td>
<td>61.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/probabilistic-two-stage-detection">Probabilistic two-stage detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>FPN</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">17</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/queryinst-parallelly-supervised-mask-query">QueryInst (single-scale)</a></td>
<td>56.1</td>
<td>61.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/queryinst-parallelly-supervised-mask-query">Instances as Queries</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">18</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P7 with TTA</a></td>
<td>55.8</td>
<td>61.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">19</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-64x4d, multi-scale)</a></td>
<td>55.7</td>
<td>61.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">20</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-W6 (1280, single-scale, 66 fps)</a></td>
<td>55.5</td>
<td>60.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">21</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P7 CSP-P7 (single-scale, 16 fps)</a></td>
<td>55.4</td>
<td>60.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">22</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural">CSP-p6 + Mish (multi-scale)</a></td>
<td>55.2</td>
<td>60.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural">Mish: A Self Regularized Non-Monotonic Activation Function</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">23</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P6 with TTA</a></td>
<td>54.9</td>
<td>60.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">24</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/simple-copy-paste-is-a-strong-data">Cascade Eff-B7 NAS-FPN (1280)</a></td>
<td>54.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/simple-copy-paste-is-a-strong-data">Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>NAS-FPN</strong></td>
</tr>
<tr>
<td align="center">25</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-32x4d, multi-scale)</a></td>
<td>54.7</td>
<td>60.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">26</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P6 CSP-P6 (single-scale, 32 fps)</a></td>
<td>54.3</td>
<td>59.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">27</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/rethinking-pre-training-and-self-training">SpineNet-190 (1280, with Self-training on OpenImages, single-scale)</a></td>
<td>54.3</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/rethinking-pre-training-and-self-training">Rethinking Pre-training and Self-training</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">28</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08d (Res2Net-101, DCN, multi-scale)</a></td>
<td>54.1</td>
<td>59.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">29</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet-D7 (single-scale)</a></td>
<td>53.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet: Scalable and Efficient Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">30</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/probabilistic-anchor-assignment-with-iou">PAA (ResNext-152-32x8d + DCN, multi-scale)</a></td>
<td>53.5</td>
<td>59.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/probabilistic-anchor-assignment-with-iou">Probabilistic Anchor Assignment with IoU Prediction for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">31</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/location-sensitive-visual-recognition-with">LSNet (Res2Net-101+ DCN, multi-scale)</a></td>
<td>53.5</td>
<td>59.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/location-sensitive-visual-recognition-with">Location-Sensitive Visual Recognition with Cross-IOU Loss</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">32</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/resnest-split-attention-networks">ResNeSt-200 (multi-scale)</a></td>
<td>53.3</td>
<td>58.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/resnest-split-attention-networks">ResNeSt: Split-Attention Networks</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">33</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnet-a-novel-composite-backbone-network">Cascade Mask R-CNN (Triple-ResNeXt152, multi-scale)</a></td>
<td>53.3</td>
<td>58.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnet-a-novel-composite-backbone-network">CBNet: A Novel Composite Backbone Network Architecture for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">34</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-32x4d, single-scale)</a></td>
<td>53.3</td>
<td>58.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">35</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (Res2Net-101, DCN, multiscale)</a></td>
<td>53.3</td>
<td>59.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">36</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/relationnet-bridging-visual-representations">RelationNet++ (ResNeXt-64x4d-101-DCN)</a></td>
<td>52.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/relationnet-bridging-visual-representations">RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">37</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P5 with TTA</a></td>
<td>52.5</td>
<td>58</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">38</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-detr-deformable-transformers-for-1">Deformable DETR (ResNeXt-101+DCN)</a></td>
<td>52.3</td>
<td>58.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-detr-deformable-transformers-for-1">Deformable DETR: Deformable Transformers for End-to-End Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">39</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/global-context-networks">GCNet (ResNeXt-101 + DCN + cascade + GC r4)</a></td>
<td>52.3</td>
<td>56.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/global-context-networks">Global Context Networks</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong><br><strong>GCN</strong></td>
</tr>
<tr>
<td align="center">40</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-190, 1280x1280)</a></td>
<td>52.1</td>
<td>56.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">41</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints v2 (ResNeXt-101, DCN, multi-scale)</a></td>
<td>52.1</td>
<td>57.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints V2: Verification Meets Regression for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong>;<br><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="center"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="center">42</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">AC-FPN Cascade R-CNN (X-152-32x8d-FPN-IN5k, multi scale, only CEM)</a></td>
<td>51.9</td>
<td>57</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">Attention-guided Context Feature Pyramid Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">43</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/ota-optimal-transport-assignment-for-object">OTA (ResNeXt-101+DCN, multiscale)</a></td>
<td>51.5</td>
<td>57.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/ota-optimal-transport-assignment-for-object">OTA: Optimal Transport Assignment for Object Detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">44</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08d (Res2Net-101, DCN, single-scale)</a></td>
<td>51.3</td>
<td>55.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">45</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">TSD (SENet154-DCN,multi-scale)</a></td>
<td>51.2</td>
<td>56.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">Revisiting the Sibling Head in Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">46</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolox-exceeding-yolo-series-in-2021">YOLOX-X (Modified CSP v5)</a></td>
<td>51.2</td>
<td>55.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolox-exceeding-yolo-series-in-2021">YOLOX: Exceeding YOLO Series in 2021</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">47</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-143, 1280x1280)</a></td>
<td>50.7</td>
<td>54.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">48</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bridging-the-gap-between-anchor-based-and">ATSS (ResNetXt-64x4d-101+DCN,multi-scale)</a></td>
<td>50.7</td>
<td>56.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bridging-the-gap-between-anchor-based-and">Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">49</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/learning-data-augmentation-strategies-for">NAS-FPN (AmoebaNet-D, learned aug)</a></td>
<td>50.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/learning-data-augmentation-strategies-for">Learning Data Augmentation Strategies for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">50</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (Res2Net-101, DCN)</a></td>
<td>50.6</td>
<td>55.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">51</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, DCN, multiscale test)</a></td>
<td>50.2</td>
<td>53.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">52</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-equalizing-pyramid-convolution-for">FreeAnchor + SEPC (DCN, ResNext-101-64x4d)</a></td>
<td>50.1</td>
<td>54.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-equalizing-pyramid-convolution-for">Scale-Equalizing Pyramid Convolution for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">53</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/d2det-towards-high-quality-object-detection">D2Det (ResNet-101-DCN, multi-scale test)</a></td>
<td>50.1</td>
<td>54.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/d2det-towards-high-quality-object-detection">D2Det: Towards High Quality Object Detection and Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">54</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-r-cnn-towards-high-quality-object">Dynamic R-CNN (ResNet-101-DCN, multi-scale)</a></td>
<td>50.1</td>
<td>55.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-r-cnn-towards-high-quality-object">Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">55</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">TSD (ResNet-101-Deformable, Image Pyramid)</a></td>
<td>49.4</td>
<td>54.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">Revisiting the Sibling Head in Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">56</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints v2 (ResNeXt-101, DCN)</a></td>
<td>49.4</td>
<td>53.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints V2: Verification Meets Regression for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">57</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/corner-proposal-network-for-anchor-free-two">CPNDet (Hourglass-104, multi-scale)</a></td>
<td>49.2</td>
<td>53.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/corner-proposal-network-for-anchor-free-two">Corner Proposal Network for Anchor-free, Two-stage Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">58</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNeXt-101, 32x4d, DCN)</a></td>
<td>49</td>
<td>53.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">59</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, DCN, single scale)</a></td>
<td>48.9</td>
<td>52.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">60</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08 (Res2Net-50, DCN, single-scale)</a></td>
<td>48.8</td>
<td>53.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">61</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (ResNet101, single scale)</a></td>
<td>48.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Transformer</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">62</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-96, 1024x1024)</a></td>
<td>48.6</td>
<td>52.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">63</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">TridentNet (ResNet-101-Deformable, Image Pyramid)</a></td>
<td>48.4</td>
<td>53.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">Scale-Aware Trident Networks for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">64</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze">GCNet (ResNeXt-101 + DCN + cascade + GC r4)</a></td>
<td>48.4</td>
<td>52.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong><br><strong>GCN</strong></td>
</tr>
<tr>
<td align="center">65</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-101-DCN)</a></td>
<td>48.3</td>
<td>52.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">66</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-learning-qualified-and">GFL (X-101-32x4d-DCN, single-scale)</a></td>
<td>48.2</td>
<td>52.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-learning-qualified-and">Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">67</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet101-FPN-3x, single-scale)</a></td>
<td>48.1</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">68</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, single scale)</a></td>
<td>47.8</td>
<td>51.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">69</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/matrix-nets-a-new-deep-architecture-for">MatrixNet Corners (ResNet-152, multi-scale)</a></td>
<td>47.8</td>
<td>52.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/matrix-nets-a-new-deep-architecture-for">Matrix Nets: A New Deep Architecture for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">70</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (ResNet50, single scale)</a></td>
<td>47.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Transformer</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">71</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/soft-anchor-point-object-detection">SAPD (ResNeXt-101, single-scale)</a></td>
<td>47.4</td>
<td>51.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/soft-anchor-point-object-detection">Soft Anchor-Point Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">72</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/path-aggregation-network-for-instance">PANet (ResNeXt-101, multi-scale)</a></td>
<td>47.4</td>
<td>51.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/path-aggregation-network-for-instance">Path Aggregation Network for Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">73</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">HTC (HRNetV2p-W48)</a></td>
<td>47.3</td>
<td>51.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">74</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/hybrid-task-cascade-for-instance-segmentation">HTC (ResNeXt-101-FPN)</a></td>
<td>47.1</td>
<td>44.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/hybrid-task-cascade-for-instance-segmentation">Hybrid Task Cascade for Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">75</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centernet-object-detection-with-keypoint">CenterNet511 (Hourglass-104, multi-scale)</a></td>
<td>47.0</td>
<td>50.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centernet-object-detection-with-keypoint">CenterNet: Keypoint Triplets for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">76</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNeXt101, multi-scale)</a></td>
<td>47.0</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">77</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet50-FPN-3x)</a></td>
<td>46.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">78</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49, 896x896)</a></td>
<td>46.7</td>
<td>50.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">79</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101-DCN, multi-scale)</a></td>
<td>46.5</td>
<td>50.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">80</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/houghnet-integrating-near-and-long-range">HoughNet (MS)</a></td>
<td>46.4</td>
<td>50.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/houghnet-integrating-near-and-long-range">HoughNet: Integrating near and long-range evidence for bottom-up object detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">81</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reducing-label-noise-in-anchor-free-object">PPDet (ResNeXt-101-FPN, multiscale)</a></td>
<td>46.3</td>
<td>51.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reducing-label-noise-in-anchor-free-object">Reducing Label Noise in Anchor-Free Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">82</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-101)</a></td>
<td>46.2</td>
<td>50.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">83</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER (ResNet-101)</a></td>
<td>46.1</td>
<td>51.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER: Efficient Multi-Scale Training</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">84</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Mask R-CNN (HRNetV2p-W48 + cascade)</a></td>
<td>46.1</td>
<td>50.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">85</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-convnets-v2-more-deformable-better">DCNv2 (ResNet-101, multi-scale)</a></td>
<td>46.0</td>
<td>50.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-convnets-v2-more-deformable-better">Deformable ConvNets v2: More Deformable, Better Results</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">86</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/localization-uncertainty-estimation-for">Gaussian-FCOS</a></td>
<td>46</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/localization-uncertainty-estimation-for">Localization Uncertainty Estimation for Anchor-Free Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td></td>
</tr>
<tr>
<td align="center">87</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/instaboost-boosting-instance-segmentation-via">Cascade R-CNN-FPN (ResNet-101, map-guided)</a></td>
<td>45.9</td>
<td>50</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/instaboost-boosting-instance-segmentation-via">InstaBoost: Boosting Instance Segmentation via Probability Map Guided Copy-Pasting</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">88</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNeXt101, single-scale)</a></td>
<td>45.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">89</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask+VoVNetV2-99 (single-scale)</a></td>
<td>45.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">90</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">D-RFCN + SNIP (DPN-98 with flip, multi-scale)</a></td>
<td>45.7</td>
<td>51.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">An Analysis of Scale Invariance in Object Detection - SNIP</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">91</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4 (CD53)</a></td>
<td>45.5</td>
<td>49.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">92</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO (608x608)</a></td>
<td>45.2</td>
<td>49.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">93</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">AC-FPN Cascade R-CNN (ResNet-101, single scale)</a></td>
<td>45</td>
<td>49</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">Attention-guided Context Feature Pyramid Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong><br><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">94</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/freeanchor-learning-to-match-anchors-for">FreeAnchor (ResNeXt-101)</a></td>
<td>44.8</td>
<td>48.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/freeanchor-learning-to-match-anchors-for">FreeAnchor: Learning to Match Anchors for Visual Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">95</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-64x4d-101-FPN 4 + improvements)</a></td>
<td>44.7</td>
<td>48.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">96</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask+VoVNet2-57 (single-scale)</a></td>
<td>44.7</td>
<td>48.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">97</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">FSAF (ResNeXt-101, multi-scale)</a></td>
<td>44.6</td>
<td>48.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">98</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101, DCN, 500 scale)</a></td>
<td>44.6</td>
<td>47.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">99</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask + X-101-32x8d (single-scale)</a></td>
<td>44.6</td>
<td>48.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">100</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49, 640x640)</a></td>
<td>44.3</td>
<td>47.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">101</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-look-one-level-feature">YOLOF-DC5</a></td>
<td>44.3</td>
<td>47.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-look-one-level-feature">You Only Look One-level Feature</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">102</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-50)</a></td>
<td>44.3</td>
<td>48.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">103</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-intertwiner-for-object-detection-1">InterNet (ResNet-101-FPN, multi-scale)</a></td>
<td>44.2</td>
<td>51.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-intertwiner-for-object-detection-1">Feature Intertwiner for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong><br><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">104</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (VGG-16, multi-scale)</a></td>
<td>44.2</td>
<td>49.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">105</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/lip-local-importance-based-pooling">Faster R-CNN (LIP-ResNet-101-MD w FPN)</a></td>
<td>43.9</td>
<td>48.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/lip-local-importance-based-pooling">LIP: Local Importance-based Pooling</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">106</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (ResNet-101, multi-scale)</a></td>
<td>43.9</td>
<td>48</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>multiscale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">107</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/learning-spatial-fusion-for-single-shot">YOLOv3 @800 + ASFF* (Darknet-53)</a></td>
<td>43.9</td>
<td>49.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/learning-spatial-fusion-for-single-shot">Learning Spatial Fusion for Single-Shot Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">108</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td>
<td>43.9</td>
<td>47.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">109</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">ExtremeNet (Hourglass-104, multi-scale)</a></td>
<td>43.7</td>
<td>47.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">Bottom-up Object Detection by Grouping Extreme and Center Points</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">110</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolov4-optimal-speed-and-accuracy-of-object">YOLOv4-608</a></td>
<td>43.5</td>
<td>47.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolov4-optimal-speed-and-accuracy-of-object">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">111</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER (ResNet-50)</a></td>
<td>43.5</td>
<td>48.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER: Efficient Multi-Scale Training</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">112</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">CenterNet (HRNetV2-W48)</a></td>
<td>43.5</td>
<td>46.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">113</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">D-RFCN + SNIP (ResNet-101, multi-scale)</a></td>
<td>43.4</td>
<td>48.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">An Analysis of Scale Invariance in Object Detection - SNIP</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>multiscale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">114</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/grid-r-cnn">Grid R-CNN (ResNeXt-101-FPN)</a></td>
<td>43.2</td>
<td>46.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/grid-r-cnn">Grid R-CNN</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">115</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-101-64x4d-FPN)</a></td>
<td>43.2</td>
<td>46.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">116</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190408900">CornerNet-Saccade (Hourglass-104, multi-scale)</a></td>
<td>43.2</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190408900">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">117</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/libra-r-cnn-towards-balanced-learning-for">Libra R-CNN (ResNeXt-101-FPN)</a></td>
<td>43.0</td>
<td>47</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/libra-r-cnn-towards-balanced-learning-for">Libra R-CNN: Towards Balanced Learning for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">118</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101-DCN)</a></td>
<td>42.8</td>
<td>46.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">119</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet-49 (640, RetinaNet, single-scale)</a></td>
<td>42.8</td>
<td>46.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">120</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-101-FPN+, cascade)</a></td>
<td>42.8</td>
<td>46.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">121</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-high-quality-object-detection">Cascade R-CNN</a></td>
<td>42.8</td>
<td>46.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-high-quality-object-detection">Cascade R-CNN: High Quality Object Detection and Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">122</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">TridentNet (ResNet-101)</a></td>
<td>42.7</td>
<td>46.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">Scale-Aware Trident Networks for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">123</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-32x8d-101-FPN)</a></td>
<td>42.7</td>
<td>46.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">124</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask (ResNeXt-101-FPN-GN)</a></td>
<td>42.6</td>
<td>46.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">125</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/tood-task-aligned-one-stage-object-detection">TAL + TAP</a></td>
<td>42.5</td>
<td>46.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/tood-task-aligned-one-stage-object-detection">TOOD: Task-aligned One-stage Object Detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">126</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Faster R-CNN (HRNetV2p-W48)</a></td>
<td>42.4</td>
<td>46.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">127</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/hierarchical-shot-detector">HSD (Rest101, 768x768, single-scale test)</a></td>
<td>42.3</td>
<td>46.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/hierarchical-shot-detector">Hierarchical Shot Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">128</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet511 (Hourglass-104, multi-scale)</a></td>
<td>42.1</td>
<td>45.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet: Detecting Objects as Paired Keypoints</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">129</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td>
<td>42.1</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">130</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (HRNet-W32-5l)</a></td>
<td>42.0</td>
<td>45.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">131</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512+ (ResNet-101)</a></td>
<td>41.8</td>
<td>45.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">132</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/gradient-harmonized-single-stage-detector">GHM-C + GHM-R (RetinaNet-FPN-ResNeXt-101)</a></td>
<td>41.6</td>
<td>44.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/gradient-harmonized-single-stage-detector">Gradient Harmonized Single-stage Detector</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">133</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/objects-as-points">CenterNet-DLA (DLA-34, multi-scale)</a></td>
<td>41.6</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/objects-as-points">Objects as Points</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">134</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49S, 640x640)</a></td>
<td>41.5</td>
<td>44.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">135</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101)</a></td>
<td>41</td>
<td>44.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">136</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (VGG-16, single-scale)</a></td>
<td>41.0</td>
<td>45</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">137</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">FSAF (ResNet-101, single-scale)</a></td>
<td>40.9</td>
<td>44</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">138</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">RetinaNet (ResNeXt-101-FPN)</a></td>
<td>40.8</td>
<td>44.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">Focal Loss for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">139</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-50-FPN+, cascade)</a></td>
<td>40.6</td>
<td>44</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">140</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Faster R-CNN (Cascade RPN)</a></td>
<td>40.6</td>
<td>44.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">141</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-kernels-adapting-effective">ResNet-50-DW-DPN (Deformable Kernels)</a></td>
<td>40.6</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-kernels-adapting-effective">Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">142</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/acquisition-of-localization-confidence-for">IoU-Net</a></td>
<td>40.6</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/acquisition-of-localization-confidence-for">Acquisition of Localization Confidence for Accurate Object Detection</a></td>
<td></td>
<td></td>
<td>2018</td>
<td></td>
</tr>
<tr>
<td align="center">143</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">FCOS (HRNetV2p-W48)</a></td>
<td>40.5</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">144</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/softer-nms-rethinking-bounding-box-regression">ResNet-50-FPN Mask R-CNN + KL Loss + var voting + soft-NMS</a></td>
<td>40.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/softer-nms-rethinking-bounding-box-regression">Bounding Box Regression with Uncertainty for Accurate Object Detection</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">145</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/rdsnet-a-new-deep-architecture-for-reciprocal">RDSNet (ResNet-101, RetinaNet, mask, MBRM)</a></td>
<td>40.3</td>
<td>43</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/rdsnet-a-new-deep-architecture-for-reciprocal">RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">146</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">ExtremeNet (Hourglass-104, single-scale)</a></td>
<td>40.2</td>
<td>43.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">Bottom-up Object Detection by Grouping Extreme and Center Points</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">147</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cross-iteration-batch-normalization">Mask R-CNN (ResNet-101-FPN, CBN)</a></td>
<td>40.1</td>
<td>44.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cross-iteration-batch-normalization">Cross-Iteration Batch Normalization</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">148</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Fast R-CNN (Cascade RPN)</a></td>
<td>40.1</td>
<td>43.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">149</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN (ResNeXt-101-FPN)</a></td>
<td>39.8</td>
<td>43.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">150</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/region-proposal-by-guided-anchoring">GA-Faster-RCNN</a></td>
<td>39.8</td>
<td>43.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/region-proposal-by-guided-anchoring">Region Proposal by Guided Anchoring</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">151</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/chainercv-a-library-for-deep-learning-in">FPN (ResNet101 backbone)</a></td>
<td>39.5</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/chainercv-a-library-for-deep-learning-in">ChainerCV: a Library for Deep Learning in Computer Vision</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">152</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask (ResNet-50-FPN)</a></td>
<td>39.4</td>
<td>42.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">153</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO (320x320)</a></td>
<td>39.3</td>
<td>42.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">154</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190409925">AA-ResNet-10 + RetinaNet</a></td>
<td>39.2</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190409925">Attention Augmented Convolutional Networks</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">155</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNet50, single-scale)</a></td>
<td>39.2</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">156</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">RetinaNet (ResNet-101-FPN)</a></td>
<td>39.1</td>
<td>42.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">Focal Loss for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">157</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-101-FPN+)</a></td>
<td>38.8</td>
<td>41.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">158</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (ResNet-101, single-scale)</a></td>
<td>38.8</td>
<td>41.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>single scale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">159</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/saccadenet-a-fast-and-accurate-object">SaccadeNet (DLA-34-DCN)</a></td>
<td>38.5</td>
<td>41.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/saccadenet-a-fast-and-accurate-object">SaccadeNet: A Fast and Accurate Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">160</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN (ResNet-101-FPN)</a></td>
<td>38.2</td>
<td>41.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">161</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/segmentation-is-all-you-need">WSMA-Seg</a></td>
<td>38.1</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/segmentation-is-all-you-need">Segmentation is All You Need</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">162</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Faster R-CNN + FPN + CGD</a></td>
<td>37.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Compact Global Descriptor for Neural Networks</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">163</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet511 (Hourglass-52, single-scale)</a></td>
<td>37.8</td>
<td>40.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet: Detecting Objects as Paired Keypoints</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">164</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512+ (VGG-16)</a></td>
<td>37.6</td>
<td>40.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">165</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-convolutional-networks">DeformConv-R-FCN (Aligned-Inception-ResNet)</a></td>
<td>37.5</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-convolutional-networks">Deformable Convolutional Networks</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">166</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data">Faster R-CNN (ImageNet+300M)</a></td>
<td>37.4</td>
<td>40.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">167</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">Mask R-CNN (Bottleneck-injected ResNet-50, FPN)</a></td>
<td>36.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>FPN</strong><br>！！<strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">168</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/beyond-skip-connections-top-down-modulation">Faster R-CNN + TDM</a></td>
<td>36.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/beyond-skip-connections-top-down-modulation">Beyond Skip Connections: Top-Down Modulation for Object Detection</a></td>
<td></td>
<td></td>
<td>2016</td>
<td></td>
</tr>
<tr>
<td align="center">169</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-50-FPN+)</a></td>
<td>36.5</td>
<td>39.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong>;<br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">170</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512 (ResNet-101)</a></td>
<td>36.4</td>
<td>39.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">171</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection">Faster R-CNN + FPN</a></td>
<td>36.2</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection">Feature Pyramid Networks for Object Detection</a></td>
<td></td>
<td></td>
<td>2016</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">172</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">Faster R-CNN (Bottleneck-injected ResNet-50 and FPN)</a></td>
<td>35.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>FPN</strong>;<br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">173</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition">Faster R-CNN (box refinement, context, multi-scale testing)</a></td>
<td>34.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition">Deep Residual Learning for Image Recognition</a></td>
<td></td>
<td></td>
<td>2015</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">174</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/speedaccuracy-trade-offs-for-modern">Faster R-CNN</a></td>
<td>34.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/speedaccuracy-trade-offs-for-modern">Speed/accuracy trade-offs for modern convolutional object detectors</a></td>
<td></td>
<td></td>
<td>2016</td>
<td></td>
</tr>
<tr>
<td align="center">175</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190408900">CornerNet-Squeeze</a></td>
<td>34.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190408900">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">176</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-multipath-network-for-object-detection">MultiPath Network</a></td>
<td>33.2</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-multipath-network-for-object-detection">A MultiPath Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2016</td>
<td></td>
</tr>
<tr>
<td align="center">177</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/inside-outside-net-detecting-objects-in">ION</a></td>
<td>33.1</td>
<td>34.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/inside-outside-net-detecting-objects-in">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</a></td>
<td></td>
<td></td>
<td>2015</td>
<td></td>
</tr>
<tr>
<td align="center">178</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512 (VGG-16)</a></td>
<td>33</td>
<td>35.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">179</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolov3-an-incremental-improvement">YOLOv3 + Darknet-53</a></td>
<td>33.0</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolov3-an-incremental-improvement">YOLOv3: An Incremental Improvement</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">180</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/ssd-single-shot-multibox-detector">SSD512</a></td>
<td>28.8</td>
<td>30.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/ssd-single-shot-multibox-detector">SSD: Single Shot MultiBox Detector</a></td>
<td></td>
<td></td>
<td>2015</td>
<td></td>
</tr>
<tr>
<td align="center">181</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MobileNetV2)</a></td>
<td>26.1</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">182</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/espnetv2-a-light-weight-power-efficient-and">ESPNetv2-512</a></td>
<td>26.0</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/espnetv2-a-light-weight-power-efficient-and">ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td></td>
</tr>
<tr>
<td align="center">183</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MobileNetV3)</a></td>
<td>25.5</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">184</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MNASNet-B1)</a></td>
<td>24.6</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">185</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN x0.7 (MobileNetV2)</a></td>
<td>23.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">186</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">MobielNet-v1-SSD-300x300+CGD</a></td>
<td>21.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Compact Global Descriptor for Neural Networks</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">187</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fast-r-cnn">Fast-RCNN</a></td>
<td>19.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fast-r-cnn">Fast R-CNN</a></td>
<td></td>
<td></td>
<td>2015</td>
<td></td>
</tr>
<tr>
<td align="center">188</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mobilenets-efficient-convolutional-neural">MobileNet</a></td>
<td>19.3</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mobilenets-efficient-convolutional-neural">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">189</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/vision-transformer-with-deformable-attention">DAT-S (RetinaNet)</a></td>
<td></td>
<td>51.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/vision-transformer-with-deformable-attention">Vision Transformer with Deformable Attention</a></td>
<td></td>
<td></td>
<td>2022</td>
<td></td>
</tr>
<tr>
<td align="center">190</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask-VoVNet99 (multi-scale)</a></td>
<td></td>
<td>53.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">191</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Mask R-CNN (HRNetV2p-W32 + cascade)</a></td>
<td></td>
<td>48.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">192</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td>
<td></td>
<td>45.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">193</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/virtex-learning-visual-representations-from">VirTex Mask R-CNN (ResNet-50-FPN)</a></td>
<td></td>
<td>44.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/virtex-learning-visual-representations-from">VirTex: Learning Visual Representations from Textual Annotations</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>FPN</strong>;<br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">194</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">Centermask + ResNet101</a></td>
<td></td>
<td>46.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">195</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pafnet-an-efficient-anchor-free-object">PAFNet (ResNet50-vd)</a></td>
<td></td>
<td>45.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pafnet-an-efficient-anchor-free-object">PAFNet: An Efficient Anchor-Free Object Detector Guidance</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">196</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dctd-deep-conditional-target-densities-for">IoU-Net+EnergyRegression</a></td>
<td></td>
<td>41.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dctd-deep-conditional-target-densities-for">Energy-Based Models for Deep Probabilistic Regression</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">197</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Cascade R-CNN (HRNetV2p-W48)</a></td>
<td></td>
<td>48.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">198</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet50-FPN-3x, single-scale)</a></td>
<td></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">199</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td>
<td></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">200</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet-D7x (single-scale)</a></td>
<td></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet: Scalable and Efficient Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
</tbody></table>
<hr>
<h2 id="APS"><a href="#APS" class="headerlink" title="APS"></a>APS</h2><p><img src="/blog/blog/2022/02/16/ji-yu-coco-shu-ju-ji-yan-zheng-de-mu-biao-jian-ce-suan-fa-tian-ti-pai-xing-bang/%E5%9F%BA%E4%BA%8ECOCO%E6%95%B0%E6%8D%AE%E9%9B%86%E9%AA%8C%E8%AF%81%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E5%A4%A9%E6%A2%AF%E6%8E%92%E8%A1%8C%E6%A6%9C/1571518-20220216181109388-1416537678.jpg"></p>
<table>
<thead>
<tr>
<th align="center">Rank</th>
<th>Model</th>
<th><strong>box AP</strong></th>
<th>APS</th>
<th>Paper</th>
<th>Code</th>
<th>Result</th>
<th>Year</th>
<th>Tags</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and">SwinV2-G (HTC++)</a></td>
<td>63.1</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and">Swin Transformer V2: Scaling Up Capacity and Resolution</a></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and#code">Link</a></td>
<td></td>
<td>2021</td>
<td><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/florence-a-new-foundation-model-for-computer">Florence-CoSwin-H</a></td>
<td>62.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/florence-a-new-foundation-model-for-computer">Florence: A New Foundation Model for Computer Vision</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/grounded-language-image-pre-training">GLIP (Swin-L, multi-scale)</a></td>
<td>61.5</td>
<td>45.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/grounded-language-image-pre-training">Grounded Language-Image Pre-training</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong>;<br>Vision Language**;<br><strong>Dynamic Head</strong>;<br>**BERT-Base</td>
</tr>
<tr>
<td align="center">4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/end-to-end-semi-supervised-object-detection">Soft Teacher + Swin-L (HTC++, multi-scale)</a></td>
<td>61.3</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/end-to-end-semi-supervised-object-detection">End-to-End Semi-Supervised Object Detection with Soft Teacher</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong>;<br>Swin-Transformer</td>
</tr>
<tr>
<td align="center">5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">DyHead (Swin-L, multi scale, self-training)</a></td>
<td>60.6</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">Dynamic Head: Unifying Object Detection Heads with Attentions</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong>;<br>Swin-Transformer</td>
</tr>
<tr>
<td align="center">6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">Dual-Swin-L (HTC, multi-scale)</a></td>
<td>60.1</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">CBNetV2: A Composite Backbone Network Architecture for Object Detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br>Swin-Transformer</td>
</tr>
<tr>
<td align="center">7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">Dual-Swin-L (HTC, single-scale)</a></td>
<td>59.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">CBNetV2: A Composite Backbone Network Architecture for Object Detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal-L (DyHead, multi-scale)</a></td>
<td>58.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal Self-attention for Local-Global Interactions in Vision Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br>Focal-Transformer</td>
</tr>
<tr>
<td align="center">9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">DyHead (Swin-L, multi scale)</a></td>
<td>58.7</td>
<td>41.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">Dynamic Head: Unifying Object Detection Heads with Attentions</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br>Swin-Transformer</td>
</tr>
<tr>
<td align="center">10</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin-L (HTC++, multi scale)</a></td>
<td>58.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">11</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal-L (HTC++, multi-scale)</a></td>
<td>58.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal Self-attention for Local-Global Interactions in Vision Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">12</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin-L (HTC++, single scale)</a></td>
<td>57.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">13</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-D6 (1280, single-scale, 34 fps)</a></td>
<td>57.3</td>
<td>40.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br>YOLO</td>
</tr>
<tr>
<td align="center">14</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (Swin-L, single)</a></td>
<td>56.5</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Transformer</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">15</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-E6 (1280, single-scale, 45 fps)</a></td>
<td>56.4</td>
<td>39.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">16</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/probabilistic-two-stage-detection">CenterNet2 (Res2Net-101-DCN-BiFPN, self-training, 1560 single-scale)</a></td>
<td>56.4</td>
<td>38.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/probabilistic-two-stage-detection">Probabilistic two-stage detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>FPN</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">17</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/queryinst-parallelly-supervised-mask-query">QueryInst (single-scale)</a></td>
<td>56.1</td>
<td>37.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/queryinst-parallelly-supervised-mask-query">Instances as Queries</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">18</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P7 with TTA</a></td>
<td>55.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">19</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-64x4d, multi-scale)</a></td>
<td>55.7</td>
<td>37.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">20</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-W6 (1280, single-scale, 66 fps)</a></td>
<td>55.5</td>
<td>37.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">21</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P7 CSP-P7 (single-scale, 16 fps)</a></td>
<td>55.4</td>
<td>38.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">22</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural">CSP-p6 + Mish (multi-scale)</a></td>
<td>55.2</td>
<td>37.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural">Mish: A Self Regularized Non-Monotonic Activation Function</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">23</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P6 with TTA</a></td>
<td>54.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">24</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/simple-copy-paste-is-a-strong-data">Cascade Eff-B7 NAS-FPN (1280)</a></td>
<td>54.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/simple-copy-paste-is-a-strong-data">Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>NAS-FPN</strong></td>
</tr>
<tr>
<td align="center">25</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-32x4d, multi-scale)</a></td>
<td>54.7</td>
<td>37.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">26</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P6 CSP-P6 (single-scale, 32 fps)</a></td>
<td>54.3</td>
<td>36.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">27</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/rethinking-pre-training-and-self-training">SpineNet-190 (1280, with Self-training on OpenImages, single-scale)</a></td>
<td>54.3</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/rethinking-pre-training-and-self-training">Rethinking Pre-training and Self-training</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">28</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08d (Res2Net-101, DCN, multi-scale)</a></td>
<td>54.1</td>
<td>35.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">29</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet-D7 (single-scale)</a></td>
<td>53.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet: Scalable and Efficient Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">30</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/probabilistic-anchor-assignment-with-iou">PAA (ResNext-152-32x8d + DCN, multi-scale)</a></td>
<td>53.5</td>
<td>36.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/probabilistic-anchor-assignment-with-iou">Probabilistic Anchor Assignment with IoU Prediction for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">31</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/location-sensitive-visual-recognition-with">LSNet (Res2Net-101+ DCN, multi-scale)</a></td>
<td>53.5</td>
<td>35.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/location-sensitive-visual-recognition-with">Location-Sensitive Visual Recognition with Cross-IOU Loss</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">32</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/resnest-split-attention-networks">ResNeSt-200 (multi-scale)</a></td>
<td>53.3</td>
<td>35.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/resnest-split-attention-networks">ResNeSt: Split-Attention Networks</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">33</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnet-a-novel-composite-backbone-network">Cascade Mask R-CNN (Triple-ResNeXt152, multi-scale)</a></td>
<td>53.3</td>
<td>35.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnet-a-novel-composite-backbone-network">CBNet: A Novel Composite Backbone Network Architecture for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">34</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-32x4d, single-scale)</a></td>
<td>53.3</td>
<td>33.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">35</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (Res2Net-101, DCN, multiscale)</a></td>
<td>53.3</td>
<td>35.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">36</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/relationnet-bridging-visual-representations">RelationNet++ (ResNeXt-64x4d-101-DCN)</a></td>
<td>52.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/relationnet-bridging-visual-representations">RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">37</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P5 with TTA</a></td>
<td>52.5</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">38</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-detr-deformable-transformers-for-1">Deformable DETR (ResNeXt-101+DCN)</a></td>
<td>52.3</td>
<td>34.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-detr-deformable-transformers-for-1">Deformable DETR: Deformable Transformers for End-to-End Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">39</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/global-context-networks">GCNet (ResNeXt-101 + DCN + cascade + GC r4)</a></td>
<td>52.3</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/global-context-networks">Global Context Networks</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong><br><strong>GCN</strong></td>
</tr>
<tr>
<td align="center">40</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-190, 1280x1280)</a></td>
<td>52.1</td>
<td>35.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">41</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints v2 (ResNeXt-101, DCN, multi-scale)</a></td>
<td>52.1</td>
<td>34.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints V2: Verification Meets Regression for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong>;<br><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="center"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="center">42</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">AC-FPN Cascade R-CNN (X-152-32x8d-FPN-IN5k, multi scale, only CEM)</a></td>
<td>51.9</td>
<td>34.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">Attention-guided Context Feature Pyramid Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">43</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/ota-optimal-transport-assignment-for-object">OTA (ResNeXt-101+DCN, multiscale)</a></td>
<td>51.5</td>
<td>34.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/ota-optimal-transport-assignment-for-object">OTA: Optimal Transport Assignment for Object Detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">44</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08d (Res2Net-101, DCN, single-scale)</a></td>
<td>51.3</td>
<td>31.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">45</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">TSD (SENet154-DCN,multi-scale)</a></td>
<td>51.2</td>
<td>33.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">Revisiting the Sibling Head in Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">46</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolox-exceeding-yolo-series-in-2021">YOLOX-X (Modified CSP v5)</a></td>
<td>51.2</td>
<td>31.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolox-exceeding-yolo-series-in-2021">YOLOX: Exceeding YOLO Series in 2021</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">47</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-143, 1280x1280)</a></td>
<td>50.7</td>
<td>33.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">48</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bridging-the-gap-between-anchor-based-and">ATSS (ResNetXt-64x4d-101+DCN,multi-scale)</a></td>
<td>50.7</td>
<td>33.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bridging-the-gap-between-anchor-based-and">Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">49</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/learning-data-augmentation-strategies-for">NAS-FPN (AmoebaNet-D, learned aug)</a></td>
<td>50.7</td>
<td>34.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/learning-data-augmentation-strategies-for">Learning Data Augmentation Strategies for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">50</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (Res2Net-101, DCN)</a></td>
<td>50.6</td>
<td>31.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">51</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, DCN, multiscale test)</a></td>
<td>50.2</td>
<td>32.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">52</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-equalizing-pyramid-convolution-for">FreeAnchor + SEPC (DCN, ResNext-101-64x4d)</a></td>
<td>50.1</td>
<td>31.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-equalizing-pyramid-convolution-for">Scale-Equalizing Pyramid Convolution for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">53</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/d2det-towards-high-quality-object-detection">D2Det (ResNet-101-DCN, multi-scale test)</a></td>
<td>50.1</td>
<td>32.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/d2det-towards-high-quality-object-detection">D2Det: Towards High Quality Object Detection and Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">54</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-r-cnn-towards-high-quality-object">Dynamic R-CNN (ResNet-101-DCN, multi-scale)</a></td>
<td>50.1</td>
<td>32.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-r-cnn-towards-high-quality-object">Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">55</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">TSD (ResNet-101-Deformable, Image Pyramid)</a></td>
<td>49.4</td>
<td>32.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">Revisiting the Sibling Head in Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">56</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints v2 (ResNeXt-101, DCN)</a></td>
<td>49.4</td>
<td>30.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints V2: Verification Meets Regression for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">57</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/corner-proposal-network-for-anchor-free-two">CPNDet (Hourglass-104, multi-scale)</a></td>
<td>49.2</td>
<td>31.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/corner-proposal-network-for-anchor-free-two">Corner Proposal Network for Anchor-free, Two-stage Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">58</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNeXt-101, 32x4d, DCN)</a></td>
<td>49</td>
<td>29.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">59</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, DCN, single scale)</a></td>
<td>48.9</td>
<td>30.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">60</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08 (Res2Net-50, DCN, single-scale)</a></td>
<td>48.8</td>
<td>30.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">61</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (ResNet101, single scale)</a></td>
<td>48.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Transformer</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">62</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-96, 1024x1024)</a></td>
<td>48.6</td>
<td>32</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">63</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">TridentNet (ResNet-101-Deformable, Image Pyramid)</a></td>
<td>48.4</td>
<td>31.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">Scale-Aware Trident Networks for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">64</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze">GCNet (ResNeXt-101 + DCN + cascade + GC r4)</a></td>
<td>48.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong><br><strong>GCN</strong></td>
</tr>
<tr>
<td align="center">65</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-101-DCN)</a></td>
<td>48.3</td>
<td>28.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">66</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-learning-qualified-and">GFL (X-101-32x4d-DCN, single-scale)</a></td>
<td>48.2</td>
<td>29.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-learning-qualified-and">Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">67</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet101-FPN-3x, single-scale)</a></td>
<td>48.1</td>
<td>28.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">68</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, single scale)</a></td>
<td>47.8</td>
<td>30.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">69</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/matrix-nets-a-new-deep-architecture-for">MatrixNet Corners (ResNet-152, multi-scale)</a></td>
<td>47.8</td>
<td>29.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/matrix-nets-a-new-deep-architecture-for">Matrix Nets: A New Deep Architecture for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">70</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (ResNet50, single scale)</a></td>
<td>47.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Transformer</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">71</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/soft-anchor-point-object-detection">SAPD (ResNeXt-101, single-scale)</a></td>
<td>47.4</td>
<td>28.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/soft-anchor-point-object-detection">Soft Anchor-Point Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">72</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/path-aggregation-network-for-instance">PANet (ResNeXt-101, multi-scale)</a></td>
<td>47.4</td>
<td>30.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/path-aggregation-network-for-instance">Path Aggregation Network for Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">73</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">HTC (HRNetV2p-W48)</a></td>
<td>47.3</td>
<td>28.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">74</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/hybrid-task-cascade-for-instance-segmentation">HTC (ResNeXt-101-FPN)</a></td>
<td>47.1</td>
<td>22.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/hybrid-task-cascade-for-instance-segmentation">Hybrid Task Cascade for Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">75</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centernet-object-detection-with-keypoint">CenterNet511 (Hourglass-104, multi-scale)</a></td>
<td>47.0</td>
<td>28.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centernet-object-detection-with-keypoint">CenterNet: Keypoint Triplets for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">76</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNeXt101, multi-scale)</a></td>
<td>47.0</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">77</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet50-FPN-3x)</a></td>
<td>46.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">78</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49, 896x896)</a></td>
<td>46.7</td>
<td>29.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">79</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101-DCN, multi-scale)</a></td>
<td>46.5</td>
<td>30.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">80</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/houghnet-integrating-near-and-long-range">HoughNet (MS)</a></td>
<td>46.4</td>
<td>29.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/houghnet-integrating-near-and-long-range">HoughNet: Integrating near and long-range evidence for bottom-up object detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">81</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reducing-label-noise-in-anchor-free-object">PPDet (ResNeXt-101-FPN, multiscale)</a></td>
<td>46.3</td>
<td>31.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reducing-label-noise-in-anchor-free-object">Reducing Label Noise in Anchor-Free Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">82</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-101)</a></td>
<td>46.2</td>
<td>27.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">83</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER (ResNet-101)</a></td>
<td>46.1</td>
<td>29.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER: Efficient Multi-Scale Training</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">84</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Mask R-CNN (HRNetV2p-W48 + cascade)</a></td>
<td>46.1</td>
<td>27.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">85</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-convnets-v2-more-deformable-better">DCNv2 (ResNet-101, multi-scale)</a></td>
<td>46.0</td>
<td>27.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-convnets-v2-more-deformable-better">Deformable ConvNets v2: More Deformable, Better Results</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">86</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/localization-uncertainty-estimation-for">Gaussian-FCOS</a></td>
<td>46</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/localization-uncertainty-estimation-for">Localization Uncertainty Estimation for Anchor-Free Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td></td>
</tr>
<tr>
<td align="center">87</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/instaboost-boosting-instance-segmentation-via">Cascade R-CNN-FPN (ResNet-101, map-guided)</a></td>
<td>45.9</td>
<td>26.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/instaboost-boosting-instance-segmentation-via">InstaBoost: Boosting Instance Segmentation via Probability Map Guided Copy-Pasting</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">88</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNeXt101, single-scale)</a></td>
<td>45.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">89</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask+VoVNetV2-99 (single-scale)</a></td>
<td>45.8</td>
<td>27.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">90</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">D-RFCN + SNIP (DPN-98 with flip, multi-scale)</a></td>
<td>45.7</td>
<td>29.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">An Analysis of Scale Invariance in Object Detection - SNIP</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">91</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4 (CD53)</a></td>
<td>45.5</td>
<td>27</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">92</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO (608x608)</a></td>
<td>45.2</td>
<td>26.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">93</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">AC-FPN Cascade R-CNN (ResNet-101, single scale)</a></td>
<td>45</td>
<td>26.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">Attention-guided Context Feature Pyramid Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong><br><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">94</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/freeanchor-learning-to-match-anchors-for">FreeAnchor (ResNeXt-101)</a></td>
<td>44.8</td>
<td>27</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/freeanchor-learning-to-match-anchors-for">FreeAnchor: Learning to Match Anchors for Visual Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">95</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-64x4d-101-FPN 4 + improvements)</a></td>
<td>44.7</td>
<td>27.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">96</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask+VoVNet2-57 (single-scale)</a></td>
<td>44.7</td>
<td>27.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">97</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">FSAF (ResNeXt-101, multi-scale)</a></td>
<td>44.6</td>
<td>29.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">98</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101, DCN, 500 scale)</a></td>
<td>44.6</td>
<td>24.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">99</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask + X-101-32x8d (single-scale)</a></td>
<td>44.6</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">100</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49, 640x640)</a></td>
<td>44.3</td>
<td>25.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">101</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-look-one-level-feature">YOLOF-DC5</a></td>
<td>44.3</td>
<td>24.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-look-one-level-feature">You Only Look One-level Feature</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">102</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-50)</a></td>
<td>44.3</td>
<td>26.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">103</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-intertwiner-for-object-detection-1">InterNet (ResNet-101-FPN, multi-scale)</a></td>
<td>44.2</td>
<td>27.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-intertwiner-for-object-detection-1">Feature Intertwiner for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong><br><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">104</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (VGG-16, multi-scale)</a></td>
<td>44.2</td>
<td>29.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">105</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/lip-local-importance-based-pooling">Faster R-CNN (LIP-ResNet-101-MD w FPN)</a></td>
<td>43.9</td>
<td>25.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/lip-local-importance-based-pooling">LIP: Local Importance-based Pooling</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">106</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (ResNet-101, multi-scale)</a></td>
<td>43.9</td>
<td>29.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>multiscale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">107</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/learning-spatial-fusion-for-single-shot">YOLOv3 @800 + ASFF* (Darknet-53)</a></td>
<td>43.9</td>
<td>27.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/learning-spatial-fusion-for-single-shot">Learning Spatial Fusion for Single-Shot Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">108</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td>
<td>43.9</td>
<td>26.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">109</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">ExtremeNet (Hourglass-104, multi-scale)</a></td>
<td>43.7</td>
<td>24.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">Bottom-up Object Detection by Grouping Extreme and Center Points</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">110</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolov4-optimal-speed-and-accuracy-of-object">YOLOv4-608</a></td>
<td>43.5</td>
<td>26.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolov4-optimal-speed-and-accuracy-of-object">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">111</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER (ResNet-50)</a></td>
<td>43.5</td>
<td>26.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER: Efficient Multi-Scale Training</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">112</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">CenterNet (HRNetV2-W48)</a></td>
<td>43.5</td>
<td>22.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">113</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">D-RFCN + SNIP (ResNet-101, multi-scale)</a></td>
<td>43.4</td>
<td>27.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">An Analysis of Scale Invariance in Object Detection - SNIP</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>multiscale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">114</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/grid-r-cnn">Grid R-CNN (ResNeXt-101-FPN)</a></td>
<td>43.2</td>
<td>25.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/grid-r-cnn">Grid R-CNN</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">115</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-101-64x4d-FPN)</a></td>
<td>43.2</td>
<td>26.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">116</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190408900">CornerNet-Saccade (Hourglass-104, multi-scale)</a></td>
<td>43.2</td>
<td>24.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190408900">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">117</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/libra-r-cnn-towards-balanced-learning-for">Libra R-CNN (ResNeXt-101-FPN)</a></td>
<td>43.0</td>
<td>25.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/libra-r-cnn-towards-balanced-learning-for">Libra R-CNN: Towards Balanced Learning for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">118</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101-DCN)</a></td>
<td>42.8</td>
<td>24.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">119</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet-49 (640, RetinaNet, single-scale)</a></td>
<td>42.8</td>
<td>23.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">120</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-101-FPN+, cascade)</a></td>
<td>42.8</td>
<td>23.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">121</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-high-quality-object-detection">Cascade R-CNN</a></td>
<td>42.8</td>
<td>23.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-high-quality-object-detection">Cascade R-CNN: High Quality Object Detection and Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">122</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">TridentNet (ResNet-101)</a></td>
<td>42.7</td>
<td>23.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">Scale-Aware Trident Networks for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">123</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-32x8d-101-FPN)</a></td>
<td>42.7</td>
<td>26.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">124</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask (ResNeXt-101-FPN-GN)</a></td>
<td>42.6</td>
<td>24.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">125</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/tood-task-aligned-one-stage-object-detection">TAL + TAP</a></td>
<td>42.5</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/tood-task-aligned-one-stage-object-detection">TOOD: Task-aligned One-stage Object Detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">126</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Faster R-CNN (HRNetV2p-W48)</a></td>
<td>42.4</td>
<td>24.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">127</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/hierarchical-shot-detector">HSD (Rest101, 768x768, single-scale test)</a></td>
<td>42.3</td>
<td>22.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/hierarchical-shot-detector">Hierarchical Shot Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">128</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet511 (Hourglass-104, multi-scale)</a></td>
<td>42.1</td>
<td>20.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet: Detecting Objects as Paired Keypoints</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">129</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td>
<td>42.1</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">130</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (HRNet-W32-5l)</a></td>
<td>42.0</td>
<td>25.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">131</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512+ (ResNet-101)</a></td>
<td>41.8</td>
<td>25.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">132</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/gradient-harmonized-single-stage-detector">GHM-C + GHM-R (RetinaNet-FPN-ResNeXt-101)</a></td>
<td>41.6</td>
<td>22.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/gradient-harmonized-single-stage-detector">Gradient Harmonized Single-stage Detector</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">133</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/objects-as-points">CenterNet-DLA (DLA-34, multi-scale)</a></td>
<td>41.6</td>
<td>21.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/objects-as-points">Objects as Points</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">134</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49S, 640x640)</a></td>
<td>41.5</td>
<td>23.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">135</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101)</a></td>
<td>41</td>
<td>23.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">136</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (VGG-16, single-scale)</a></td>
<td>41.0</td>
<td>22.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">137</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">FSAF (ResNet-101, single-scale)</a></td>
<td>40.9</td>
<td>24</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">138</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">RetinaNet (ResNeXt-101-FPN)</a></td>
<td>40.8</td>
<td>24.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">Focal Loss for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">139</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-50-FPN+, cascade)</a></td>
<td>40.6</td>
<td>22.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">140</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Faster R-CNN (Cascade RPN)</a></td>
<td>40.6</td>
<td>22.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">141</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-kernels-adapting-effective">ResNet-50-DW-DPN (Deformable Kernels)</a></td>
<td>40.6</td>
<td>24.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-kernels-adapting-effective">Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">142</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/acquisition-of-localization-confidence-for">IoU-Net</a></td>
<td>40.6</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/acquisition-of-localization-confidence-for">Acquisition of Localization Confidence for Accurate Object Detection</a></td>
<td></td>
<td></td>
<td>2018</td>
<td></td>
</tr>
<tr>
<td align="center">143</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">FCOS (HRNetV2p-W48)</a></td>
<td>40.5</td>
<td>23.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">144</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/softer-nms-rethinking-bounding-box-regression">ResNet-50-FPN Mask R-CNN + KL Loss + var voting + soft-NMS</a></td>
<td>40.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/softer-nms-rethinking-bounding-box-regression">Bounding Box Regression with Uncertainty for Accurate Object Detection</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">145</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/rdsnet-a-new-deep-architecture-for-reciprocal">RDSNet (ResNet-101, RetinaNet, mask, MBRM)</a></td>
<td>40.3</td>
<td>22.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/rdsnet-a-new-deep-architecture-for-reciprocal">RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">146</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">ExtremeNet (Hourglass-104, single-scale)</a></td>
<td>40.2</td>
<td>20.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">Bottom-up Object Detection by Grouping Extreme and Center Points</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">147</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cross-iteration-batch-normalization">Mask R-CNN (ResNet-101-FPN, CBN)</a></td>
<td>40.1</td>
<td>35.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cross-iteration-batch-normalization">Cross-Iteration Batch Normalization</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">148</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Fast R-CNN (Cascade RPN)</a></td>
<td>40.1</td>
<td>22.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">149</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN (ResNeXt-101-FPN)</a></td>
<td>39.8</td>
<td>22.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">150</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/region-proposal-by-guided-anchoring">GA-Faster-RCNN</a></td>
<td>39.8</td>
<td>21.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/region-proposal-by-guided-anchoring">Region Proposal by Guided Anchoring</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">151</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/chainercv-a-library-for-deep-learning-in">FPN (ResNet101 backbone)</a></td>
<td>39.5</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/chainercv-a-library-for-deep-learning-in">ChainerCV: a Library for Deep Learning in Computer Vision</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">152</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask (ResNet-50-FPN)</a></td>
<td>39.4</td>
<td>21.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">153</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO (320x320)</a></td>
<td>39.3</td>
<td>16.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">154</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190409925">AA-ResNet-10 + RetinaNet</a></td>
<td>39.2</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190409925">Attention Augmented Convolutional Networks</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">155</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNet50, single-scale)</a></td>
<td>39.2</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">156</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">RetinaNet (ResNet-101-FPN)</a></td>
<td>39.1</td>
<td>21.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">Focal Loss for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">157</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-101-FPN+)</a></td>
<td>38.8</td>
<td>21.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">158</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (ResNet-101, single-scale)</a></td>
<td>38.8</td>
<td>20.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>single scale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">159</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/saccadenet-a-fast-and-accurate-object">SaccadeNet (DLA-34-DCN)</a></td>
<td>38.5</td>
<td>19.2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/saccadenet-a-fast-and-accurate-object">SaccadeNet: A Fast and Accurate Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">160</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN (ResNet-101-FPN)</a></td>
<td>38.2</td>
<td>20.1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">161</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/segmentation-is-all-you-need">WSMA-Seg</a></td>
<td>38.1</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/segmentation-is-all-you-need">Segmentation is All You Need</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">162</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Faster R-CNN + FPN + CGD</a></td>
<td>37.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Compact Global Descriptor for Neural Networks</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">163</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet511 (Hourglass-52, single-scale)</a></td>
<td>37.8</td>
<td>17.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet: Detecting Objects as Paired Keypoints</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">164</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512+ (VGG-16)</a></td>
<td>37.6</td>
<td>22.7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">165</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-convolutional-networks">DeformConv-R-FCN (Aligned-Inception-ResNet)</a></td>
<td>37.5</td>
<td>19.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-convolutional-networks">Deformable Convolutional Networks</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">166</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data">Faster R-CNN (ImageNet+300M)</a></td>
<td>37.4</td>
<td>17.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">167</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">Mask R-CNN (Bottleneck-injected ResNet-50, FPN)</a></td>
<td>36.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>FPN</strong><br>！！<strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">168</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/beyond-skip-connections-top-down-modulation">Faster R-CNN + TDM</a></td>
<td>36.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/beyond-skip-connections-top-down-modulation">Beyond Skip Connections: Top-Down Modulation for Object Detection</a></td>
<td></td>
<td></td>
<td>2016</td>
<td></td>
</tr>
<tr>
<td align="center">169</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-50-FPN+)</a></td>
<td>36.5</td>
<td>20.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong>;<br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">170</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512 (ResNet-101)</a></td>
<td>36.4</td>
<td>16.6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">171</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection">Faster R-CNN + FPN</a></td>
<td>36.2</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection">Feature Pyramid Networks for Object Detection</a></td>
<td></td>
<td></td>
<td>2016</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">172</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">Faster R-CNN (Bottleneck-injected ResNet-50 and FPN)</a></td>
<td>35.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>FPN</strong>;<br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">173</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition">Faster R-CNN (box refinement, context, multi-scale testing)</a></td>
<td>34.9</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition">Deep Residual Learning for Image Recognition</a></td>
<td></td>
<td></td>
<td>2015</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">174</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/speedaccuracy-trade-offs-for-modern">Faster R-CNN</a></td>
<td>34.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/speedaccuracy-trade-offs-for-modern">Speed/accuracy trade-offs for modern convolutional object detectors</a></td>
<td></td>
<td></td>
<td>2016</td>
<td></td>
</tr>
<tr>
<td align="center">175</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190408900">CornerNet-Squeeze</a></td>
<td>34.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190408900">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">176</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-multipath-network-for-object-detection">MultiPath Network</a></td>
<td>33.2</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-multipath-network-for-object-detection">A MultiPath Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2016</td>
<td></td>
</tr>
<tr>
<td align="center">177</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/inside-outside-net-detecting-objects-in">ION</a></td>
<td>33.1</td>
<td>14.5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/inside-outside-net-detecting-objects-in">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</a></td>
<td></td>
<td></td>
<td>2015</td>
<td></td>
</tr>
<tr>
<td align="center">178</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512 (VGG-16)</a></td>
<td>33</td>
<td>16.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">179</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolov3-an-incremental-improvement">YOLOv3 + Darknet-53</a></td>
<td>33.0</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolov3-an-incremental-improvement">YOLOv3: An Incremental Improvement</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">180</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/ssd-single-shot-multibox-detector">SSD512</a></td>
<td>28.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/ssd-single-shot-multibox-detector">SSD: Single Shot MultiBox Detector</a></td>
<td></td>
<td></td>
<td>2015</td>
<td></td>
</tr>
<tr>
<td align="center">181</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MobileNetV2)</a></td>
<td>26.1</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">182</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/espnetv2-a-light-weight-power-efficient-and">ESPNetv2-512</a></td>
<td>26.0</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/espnetv2-a-light-weight-power-efficient-and">ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td></td>
</tr>
<tr>
<td align="center">183</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MobileNetV3)</a></td>
<td>25.5</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">184</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MNASNet-B1)</a></td>
<td>24.6</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">185</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN x0.7 (MobileNetV2)</a></td>
<td>23.8</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">186</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">MobielNet-v1-SSD-300x300+CGD</a></td>
<td>21.4</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Compact Global Descriptor for Neural Networks</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">187</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fast-r-cnn">Fast-RCNN</a></td>
<td>19.7</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fast-r-cnn">Fast R-CNN</a></td>
<td></td>
<td></td>
<td>2015</td>
<td></td>
</tr>
<tr>
<td align="center">188</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mobilenets-efficient-convolutional-neural">MobileNet</a></td>
<td>19.3</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mobilenets-efficient-convolutional-neural">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">189</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/vision-transformer-with-deformable-attention">DAT-S (RetinaNet)</a></td>
<td></td>
<td>32.3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/vision-transformer-with-deformable-attention">Vision Transformer with Deformable Attention</a></td>
<td></td>
<td></td>
<td>2022</td>
<td></td>
</tr>
<tr>
<td align="center">190</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask-VoVNet99 (multi-scale)</a></td>
<td></td>
<td>32.4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">191</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Mask R-CNN (HRNetV2p-W32 + cascade)</a></td>
<td></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">192</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td>
<td></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">193</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/virtex-learning-visual-representations-from">VirTex Mask R-CNN (ResNet-50-FPN)</a></td>
<td></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/virtex-learning-visual-representations-from">VirTex: Learning Visual Representations from Textual Annotations</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>FPN</strong>;<br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">194</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">Centermask + ResNet101</a></td>
<td></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">195</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pafnet-an-efficient-anchor-free-object">PAFNet (ResNet50-vd)</a></td>
<td></td>
<td>22.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pafnet-an-efficient-anchor-free-object">PAFNet: An Efficient Anchor-Free Object Detector Guidance</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">196</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dctd-deep-conditional-target-densities-for">IoU-Net+EnergyRegression</a></td>
<td></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dctd-deep-conditional-target-densities-for">Energy-Based Models for Deep Probabilistic Regression</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">197</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Cascade R-CNN (HRNetV2p-W48)</a></td>
<td></td>
<td>26.0</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">198</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet50-FPN-3x, single-scale)</a></td>
<td></td>
<td>27.8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">199</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td>
<td></td>
<td>24.9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">200</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet-D7x (single-scale)</a></td>
<td></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet: Scalable and Efficient Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
</tbody></table>
<hr>
<table>
<thead>
<tr>
<th align="center">Rank</th>
<th>Model</th>
<th><strong>box AP</strong></th>
<th>AP50</th>
<th>AP75</th>
<th>APS</th>
<th>APM</th>
<th>APL</th>
<th>AP</th>
<th align="center">Extra Training Data</th>
<th>Paper</th>
<th>Code</th>
<th>Result</th>
<th>Year</th>
<th>Tags</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and">SwinV2-G (HTC++)</a></td>
<td>63.1</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and">Swin Transformer V2: Scaling Up Capacity and Resolution</a></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and#code">Link</a></td>
<td></td>
<td>2021</td>
<td><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">2</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/florence-a-new-foundation-model-for-computer">Florence-CoSwin-H</a></td>
<td>62.4</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/florence-a-new-foundation-model-for-computer">Florence: A New Foundation Model for Computer Vision</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">3</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/grounded-language-image-pre-training">GLIP (Swin-L, multi-scale)</a></td>
<td>61.5</td>
<td>79.5</td>
<td>67.7</td>
<td>45.3</td>
<td>64.9</td>
<td>75.0</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/grounded-language-image-pre-training">Grounded Language-Image Pre-training</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong>;<br>Vision Language**;<br><strong>Dynamic Head</strong>;<br>**BERT-Base</td>
</tr>
<tr>
<td align="center">4</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/end-to-end-semi-supervised-object-detection">Soft Teacher + Swin-L (HTC++, multi-scale)</a></td>
<td>61.3</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/end-to-end-semi-supervised-object-detection">End-to-End Semi-Supervised Object Detection with Soft Teacher</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong>;<br>Swin-Transformer</td>
</tr>
<tr>
<td align="center">5</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">DyHead (Swin-L, multi scale, self-training)</a></td>
<td>60.6</td>
<td>78.5</td>
<td>66.6</td>
<td></td>
<td>64.0</td>
<td>74.2</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">Dynamic Head: Unifying Object Detection Heads with Attentions</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong>;<br>Swin-Transformer</td>
</tr>
<tr>
<td align="center">6</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">Dual-Swin-L (HTC, multi-scale)</a></td>
<td>60.1</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">CBNetV2: A Composite Backbone Network Architecture for Object Detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br>Swin-Transformer</td>
</tr>
<tr>
<td align="center">7</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">Dual-Swin-L (HTC, single-scale)</a></td>
<td>59.4</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnetv2-a-composite-backbone-network">CBNetV2: A Composite Backbone Network Architecture for Object Detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">8</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal-L (DyHead, multi-scale)</a></td>
<td>58.9</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal Self-attention for Local-Global Interactions in Vision Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br>Focal-Transformer</td>
</tr>
<tr>
<td align="center">9</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">DyHead (Swin-L, multi scale)</a></td>
<td>58.7</td>
<td>77.1</td>
<td>64.5</td>
<td>41.7</td>
<td>62.0</td>
<td>72.8</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-head-unifying-object-detection-heads">Dynamic Head: Unifying Object Detection Heads with Attentions</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br>Swin-Transformer</td>
</tr>
<tr>
<td align="center">10</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin-L (HTC++, multi scale)</a></td>
<td>58.7</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">11</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal-L (HTC++, multi-scale)</a></td>
<td>58.4</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-self-attention-for-local-global">Focal Self-attention for Local-Global Interactions in Vision Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">12</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin-L (HTC++, single scale)</a></td>
<td>57.7</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>Swin-Transformer</strong></td>
</tr>
<tr>
<td align="center">13</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-D6 (1280, single-scale, 34 fps)</a></td>
<td>57.3</td>
<td>75.0</td>
<td>62.7</td>
<td>40.4</td>
<td>61.2</td>
<td>69.2</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br>YOLO</td>
</tr>
<tr>
<td align="center">14</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (Swin-L, single)</a></td>
<td>56.5</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Transformer</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">15</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-E6 (1280, single-scale, 45 fps)</a></td>
<td>56.4</td>
<td>74.1</td>
<td>61.6</td>
<td>39.1</td>
<td>60.1</td>
<td>68.2</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">16</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/probabilistic-two-stage-detection">CenterNet2 (Res2Net-101-DCN-BiFPN, self-training, 1560 single-scale)</a></td>
<td>56.4</td>
<td>74.0</td>
<td>61.6</td>
<td>38.7</td>
<td>59.7</td>
<td>68.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/probabilistic-two-stage-detection">Probabilistic two-stage detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>FPN</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">17</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/queryinst-parallelly-supervised-mask-query">QueryInst (single-scale)</a></td>
<td>56.1</td>
<td>75.9</td>
<td>61.9</td>
<td>37.4</td>
<td>58.9</td>
<td>70.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/queryinst-parallelly-supervised-mask-query">Instances as Queries</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">18</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P7 with TTA</a></td>
<td>55.8</td>
<td>73.2</td>
<td>61.2</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">19</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-64x4d, multi-scale)</a></td>
<td>55.7</td>
<td>74.2</td>
<td>61.1</td>
<td>37.7</td>
<td>58.4</td>
<td>68.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">20</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">YOLOR-W6 (1280, single-scale, 66 fps)</a></td>
<td>55.5</td>
<td>73.2</td>
<td>60.6</td>
<td>37.6</td>
<td>59.5</td>
<td>67.7</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-learn-one-representation-unified">You Only Learn One Representation: Unified Network for Multiple Tasks</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">21</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P7 CSP-P7 (single-scale, 16 fps)</a></td>
<td>55.4</td>
<td>73.3</td>
<td>60.7</td>
<td>38.1</td>
<td>59.5</td>
<td>67.4</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">22</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural">CSP-p6 + Mish (multi-scale)</a></td>
<td>55.2</td>
<td>72.9</td>
<td>60.5</td>
<td>37.6</td>
<td>59.0</td>
<td>66.9</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mish-a-self-regularized-non-monotonic-neural">Mish: A Self Regularized Non-Monotonic Activation Function</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">23</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P6 with TTA</a></td>
<td>54.9</td>
<td>72.6</td>
<td>60.2</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">24</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/simple-copy-paste-is-a-strong-data">Cascade Eff-B7 NAS-FPN (1280)</a></td>
<td>54.8</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/simple-copy-paste-is-a-strong-data">Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>NAS-FPN</strong></td>
</tr>
<tr>
<td align="center">25</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-32x4d, multi-scale)</a></td>
<td>54.7</td>
<td>73.5</td>
<td>60.1</td>
<td>37.4</td>
<td>57.3</td>
<td>66.4</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">26</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P6 CSP-P6 (single-scale, 32 fps)</a></td>
<td>54.3</td>
<td>72.3</td>
<td>59.5</td>
<td>36.6</td>
<td>58.2</td>
<td>65.5</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">27</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/rethinking-pre-training-and-self-training">SpineNet-190 (1280, with Self-training on OpenImages, single-scale)</a></td>
<td>54.3</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/rethinking-pre-training-and-self-training">Rethinking Pre-training and Self-training</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">28</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08d (Res2Net-101, DCN, multi-scale)</a></td>
<td>54.1</td>
<td>71.6</td>
<td>59.9</td>
<td>35.8</td>
<td>57.2</td>
<td>67.4</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">29</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet-D7 (single-scale)</a></td>
<td>53.7</td>
<td>72.4</td>
<td></td>
<td></td>
<td>57.0</td>
<td>66.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet: Scalable and Efficient Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">30</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/probabilistic-anchor-assignment-with-iou">PAA (ResNext-152-32x8d + DCN, multi-scale)</a></td>
<td>53.5</td>
<td>71.6</td>
<td>59.1</td>
<td>36.0</td>
<td>56.3</td>
<td>66.9</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/probabilistic-anchor-assignment-with-iou">Probabilistic Anchor Assignment with IoU Prediction for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">31</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/location-sensitive-visual-recognition-with">LSNet (Res2Net-101+ DCN, multi-scale)</a></td>
<td>53.5</td>
<td>71.1</td>
<td>59.2</td>
<td>35.2</td>
<td>56.4</td>
<td>65.8</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/location-sensitive-visual-recognition-with">Location-Sensitive Visual Recognition with Cross-IOU Loss</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">32</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/resnest-split-attention-networks">ResNeSt-200 (multi-scale)</a></td>
<td>53.3</td>
<td>72.0</td>
<td>58.0</td>
<td>35.1</td>
<td>56.2</td>
<td>66.8</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/resnest-split-attention-networks">ResNeSt: Split-Attention Networks</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">33</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnet-a-novel-composite-backbone-network">Cascade Mask R-CNN (Triple-ResNeXt152, multi-scale)</a></td>
<td>53.3</td>
<td>71.9</td>
<td>58.5</td>
<td>35.5</td>
<td>55.8</td>
<td>66.7</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cbnet-a-novel-composite-backbone-network">CBNet: A Novel Composite Backbone Network Architecture for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">34</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS (ResNeXt-101-32x4d, single-scale)</a></td>
<td>53.3</td>
<td>71.6</td>
<td>58.5</td>
<td>33.9</td>
<td>56.5</td>
<td>66.9</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/detectors-detecting-objects-with-recursive-1">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">35</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (Res2Net-101, DCN, multiscale)</a></td>
<td>53.3</td>
<td>70.9</td>
<td>59.2</td>
<td>35.7</td>
<td>56.1</td>
<td>65.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">36</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/relationnet-bridging-visual-representations">RelationNet++ (ResNeXt-64x4d-101-DCN)</a></td>
<td>52.7</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/relationnet-bridging-visual-representations">RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">37</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4-P5 with TTA</a></td>
<td>52.5</td>
<td>70.3</td>
<td>58</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">38</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-detr-deformable-transformers-for-1">Deformable DETR (ResNeXt-101+DCN)</a></td>
<td>52.3</td>
<td>71.9</td>
<td>58.1</td>
<td>34.4</td>
<td>54.4</td>
<td>65.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-detr-deformable-transformers-for-1">Deformable DETR: Deformable Transformers for End-to-End Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">39</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/global-context-networks">GCNet (ResNeXt-101 + DCN + cascade + GC r4)</a></td>
<td>52.3</td>
<td>70.9</td>
<td>56.9</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/global-context-networks">Global Context Networks</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong><br><strong>GCN</strong></td>
</tr>
<tr>
<td align="center">40</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-190, 1280x1280)</a></td>
<td>52.1</td>
<td>71.8</td>
<td>56.5</td>
<td>35.4</td>
<td>55</td>
<td>63.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">41</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints v2 (ResNeXt-101, DCN, multi-scale)</a></td>
<td>52.1</td>
<td>70.1</td>
<td>57.5</td>
<td>34.5</td>
<td>54.6</td>
<td>63.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints V2: Verification Meets Regression for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong>;<br><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="center"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="center">42</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">AC-FPN Cascade R-CNN (X-152-32x8d-FPN-IN5k, multi scale, only CEM)</a></td>
<td>51.9</td>
<td>70.4</td>
<td>57</td>
<td>34.2</td>
<td>54.8</td>
<td>64.7</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">Attention-guided Context Feature Pyramid Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">43</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/ota-optimal-transport-assignment-for-object">OTA (ResNeXt-101+DCN, multiscale)</a></td>
<td>51.5</td>
<td>68.6</td>
<td>57.1</td>
<td>34.1</td>
<td>53.7</td>
<td>64.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/ota-optimal-transport-assignment-for-object">OTA: Optimal Transport Assignment for Object Detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">44</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08d (Res2Net-101, DCN, single-scale)</a></td>
<td>51.3</td>
<td>70.0</td>
<td>55.8</td>
<td>31.7</td>
<td>55.3</td>
<td>64.9</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">45</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">TSD (SENet154-DCN,multi-scale)</a></td>
<td>51.2</td>
<td>71.9</td>
<td>56.0</td>
<td>33.8</td>
<td>54.8</td>
<td>64.2</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">Revisiting the Sibling Head in Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">46</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolox-exceeding-yolo-series-in-2021">YOLOX-X (Modified CSP v5)</a></td>
<td>51.2</td>
<td>69.6</td>
<td>55.7</td>
<td>31.2</td>
<td>56.1</td>
<td>66.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolox-exceeding-yolo-series-in-2021">YOLOX: Exceeding YOLO Series in 2021</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">47</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-143, 1280x1280)</a></td>
<td>50.7</td>
<td>70.4</td>
<td>54.9</td>
<td>33.6</td>
<td>53.9</td>
<td>62.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">48</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bridging-the-gap-between-anchor-based-and">ATSS (ResNetXt-64x4d-101+DCN,multi-scale)</a></td>
<td>50.7</td>
<td>68.9</td>
<td>56.3</td>
<td>33.2</td>
<td>52.9</td>
<td>62.4</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bridging-the-gap-between-anchor-based-and">Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">49</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/learning-data-augmentation-strategies-for">NAS-FPN (AmoebaNet-D, learned aug)</a></td>
<td>50.7</td>
<td></td>
<td></td>
<td>34.2</td>
<td>55.5</td>
<td>64.5</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/learning-data-augmentation-strategies-for">Learning Data Augmentation Strategies for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">50</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (Res2Net-101, DCN)</a></td>
<td>50.6</td>
<td>69</td>
<td>55.3</td>
<td>31.3</td>
<td>54.3</td>
<td>63.5</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">51</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, DCN, multiscale test)</a></td>
<td>50.2</td>
<td>70.3</td>
<td>53.9</td>
<td>32.0</td>
<td>53.1</td>
<td>63.0</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">52</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-equalizing-pyramid-convolution-for">FreeAnchor + SEPC (DCN, ResNext-101-64x4d)</a></td>
<td>50.1</td>
<td>69.8</td>
<td>54.3</td>
<td>31.3</td>
<td>53.3</td>
<td>63.7</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-equalizing-pyramid-convolution-for">Scale-Equalizing Pyramid Convolution for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">53</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/d2det-towards-high-quality-object-detection">D2Det (ResNet-101-DCN, multi-scale test)</a></td>
<td>50.1</td>
<td>69.4</td>
<td>54.9</td>
<td>32.7</td>
<td>52.7</td>
<td>62.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/d2det-towards-high-quality-object-detection">D2Det: Towards High Quality Object Detection and Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">54</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-r-cnn-towards-high-quality-object">Dynamic R-CNN (ResNet-101-DCN, multi-scale)</a></td>
<td>50.1</td>
<td>68.3</td>
<td>55.6</td>
<td>32.8</td>
<td>53.0</td>
<td>61.2</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dynamic-r-cnn-towards-high-quality-object">Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">55</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">TSD (ResNet-101-Deformable, Image Pyramid)</a></td>
<td>49.4</td>
<td>69.6</td>
<td>54.4</td>
<td>32.7</td>
<td>52.5</td>
<td>61.0</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-the-sibling-head-in-object">Revisiting the Sibling Head in Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">56</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints v2 (ResNeXt-101, DCN)</a></td>
<td>49.4</td>
<td>68.9</td>
<td>53.4</td>
<td>30.3</td>
<td>52.1</td>
<td>62.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-v2-verification-meets-regression">RepPoints V2: Verification Meets Regression for Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">57</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/corner-proposal-network-for-anchor-free-two">CPNDet (Hourglass-104, multi-scale)</a></td>
<td>49.2</td>
<td>67.3</td>
<td>53.7</td>
<td>31.0</td>
<td>51.9</td>
<td>62.4</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/corner-proposal-network-for-anchor-free-two">Corner Proposal Network for Anchor-free, Two-stage Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">58</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNeXt-101, 32x4d, DCN)</a></td>
<td>49</td>
<td>67.6</td>
<td>53.5</td>
<td>29.7</td>
<td>52.4</td>
<td>61.4</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">59</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, DCN, single scale)</a></td>
<td>48.9</td>
<td>69.3</td>
<td>52.5</td>
<td>30.8</td>
<td>51.5</td>
<td>62.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">60</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">UniverseNet-20.08 (Res2Net-50, DCN, single-scale)</a></td>
<td>48.8</td>
<td>67.5</td>
<td>53.0</td>
<td>30.1</td>
<td>52.3</td>
<td>61.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/usb-universal-scale-object-detection">USB: Universal-Scale Object Detection Benchmark</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>single scale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">61</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (ResNet101, single scale)</a></td>
<td>48.7</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Transformer</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">62</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-96, 1024x1024)</a></td>
<td>48.6</td>
<td>68.4</td>
<td>52.5</td>
<td>32</td>
<td>52.3</td>
<td>62</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">63</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">TridentNet (ResNet-101-Deformable, Image Pyramid)</a></td>
<td>48.4</td>
<td>69.7</td>
<td>53.5</td>
<td>31.8</td>
<td>51.3</td>
<td>60.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">Scale-Aware Trident Networks for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">64</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze">GCNet (ResNeXt-101 + DCN + cascade + GC r4)</a></td>
<td>48.4</td>
<td>67.6</td>
<td>52.7</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong><br><strong>GCN</strong></td>
</tr>
<tr>
<td align="center">65</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-101-DCN)</a></td>
<td>48.3</td>
<td>66.5</td>
<td>52.8</td>
<td>28.8</td>
<td>51.9</td>
<td>60.7</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">66</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-learning-qualified-and">GFL (X-101-32x4d-DCN, single-scale)</a></td>
<td>48.2</td>
<td>67.4</td>
<td>52.6</td>
<td>29.2</td>
<td>51.7</td>
<td>60.2</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-learning-qualified-and">Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">67</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet101-FPN-3x, single-scale)</a></td>
<td>48.1</td>
<td></td>
<td></td>
<td>28.7</td>
<td>50.4</td>
<td>61.5</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">68</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101-64x4d, single scale)</a></td>
<td>47.8</td>
<td>68.4</td>
<td>51.1</td>
<td>30.2</td>
<td>50.8</td>
<td>59.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">69</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/matrix-nets-a-new-deep-architecture-for">MatrixNet Corners (ResNet-152, multi-scale)</a></td>
<td>47.8</td>
<td>66.2</td>
<td>52.3</td>
<td>29.7</td>
<td>50.4</td>
<td>60.7</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/matrix-nets-a-new-deep-architecture-for">Matrix Nets: A New Deep Architecture for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">70</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ (ResNet50, single scale)</a></td>
<td>47.8</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/solq-segmenting-objects-by-learning-queries">SOLQ: Segmenting Objects by Learning Queries</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>Transformer</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">71</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/soft-anchor-point-object-detection">SAPD (ResNeXt-101, single-scale)</a></td>
<td>47.4</td>
<td>67.4</td>
<td>51.1</td>
<td>28.1</td>
<td>50.3</td>
<td>61.5</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/soft-anchor-point-object-detection">Soft Anchor-Point Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">72</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/path-aggregation-network-for-instance">PANet (ResNeXt-101, multi-scale)</a></td>
<td>47.4</td>
<td>67.2</td>
<td>51.8</td>
<td>30.1</td>
<td>51.7</td>
<td>60.0</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/path-aggregation-network-for-instance">Path Aggregation Network for Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">73</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">HTC (HRNetV2p-W48)</a></td>
<td>47.3</td>
<td>65.9</td>
<td>51.2</td>
<td>28.0</td>
<td>49.7</td>
<td>59.8</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">74</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/hybrid-task-cascade-for-instance-segmentation">HTC (ResNeXt-101-FPN)</a></td>
<td>47.1</td>
<td>63.9</td>
<td>44.7</td>
<td>22.8</td>
<td>43.9</td>
<td>54.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/hybrid-task-cascade-for-instance-segmentation">Hybrid Task Cascade for Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">75</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centernet-object-detection-with-keypoint">CenterNet511 (Hourglass-104, multi-scale)</a></td>
<td>47.0</td>
<td>64.5</td>
<td>50.7</td>
<td>28.9</td>
<td>49.9</td>
<td>58.9</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centernet-object-detection-with-keypoint">CenterNet: Keypoint Triplets for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">76</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNeXt101, multi-scale)</a></td>
<td>47.0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">77</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet50-FPN-3x)</a></td>
<td>46.8</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">78</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49, 896x896)</a></td>
<td>46.7</td>
<td>66.3</td>
<td>50.6</td>
<td>29.1</td>
<td>50.1</td>
<td>61.7</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">79</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101-DCN, multi-scale)</a></td>
<td>46.5</td>
<td>67.4</td>
<td>50.9</td>
<td>30.3</td>
<td>49.7</td>
<td>57.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">80</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/houghnet-integrating-near-and-long-range">HoughNet (MS)</a></td>
<td>46.4</td>
<td>65.1</td>
<td>50.7</td>
<td>29.1</td>
<td>48.5</td>
<td>58.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/houghnet-integrating-near-and-long-range">HoughNet: Integrating near and long-range evidence for bottom-up object detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">81</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reducing-label-noise-in-anchor-free-object">PPDet (ResNeXt-101-FPN, multiscale)</a></td>
<td>46.3</td>
<td>64.8</td>
<td>51.6</td>
<td>31.4</td>
<td>49.9</td>
<td>56.4</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reducing-label-noise-in-anchor-free-object">Reducing Label Noise in Anchor-Free Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">82</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-101)</a></td>
<td>46.2</td>
<td>64.3</td>
<td>50.5</td>
<td>27.8</td>
<td>49.9</td>
<td>57</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">83</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER (ResNet-101)</a></td>
<td>46.1</td>
<td>67.0</td>
<td>51.6</td>
<td>29.6</td>
<td>48.9</td>
<td>58.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER: Efficient Multi-Scale Training</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">84</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Mask R-CNN (HRNetV2p-W48 + cascade)</a></td>
<td>46.1</td>
<td>64.0</td>
<td>50.3</td>
<td>27.1</td>
<td>48.6</td>
<td>58.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">85</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-convnets-v2-more-deformable-better">DCNv2 (ResNet-101, multi-scale)</a></td>
<td>46.0</td>
<td>67.9</td>
<td>50.8</td>
<td>27.8</td>
<td>49.1</td>
<td>59.5</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-convnets-v2-more-deformable-better">Deformable ConvNets v2: More Deformable, Better Results</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>multiscale</strong><br><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">86</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/localization-uncertainty-estimation-for">Gaussian-FCOS</a></td>
<td>46</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/localization-uncertainty-estimation-for">Localization Uncertainty Estimation for Anchor-Free Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td></td>
</tr>
<tr>
<td align="center">87</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/instaboost-boosting-instance-segmentation-via">Cascade R-CNN-FPN (ResNet-101, map-guided)</a></td>
<td>45.9</td>
<td>64.2</td>
<td>50</td>
<td>26.3</td>
<td>49</td>
<td>58.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/instaboost-boosting-instance-segmentation-via">InstaBoost: Boosting Instance Segmentation via Probability Map Guided Copy-Pasting</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">88</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNeXt101, single-scale)</a></td>
<td>45.9</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">89</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask+VoVNetV2-99 (single-scale)</a></td>
<td>45.8</td>
<td>64.5</td>
<td></td>
<td>27.8</td>
<td>48.3</td>
<td>57.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">90</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">D-RFCN + SNIP (DPN-98 with flip, multi-scale)</a></td>
<td>45.7</td>
<td>67.3</td>
<td>51.1</td>
<td>29.3</td>
<td>48.8</td>
<td>57.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">An Analysis of Scale Invariance in Object Detection - SNIP</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">91</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">YOLOv4 (CD53)</a></td>
<td>45.5</td>
<td>64.1</td>
<td>49.5</td>
<td>27</td>
<td>49</td>
<td>56.7</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scaled-yolov4-scaling-cross-stage-partial">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">92</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO (608x608)</a></td>
<td>45.2</td>
<td>65.2</td>
<td>49.9</td>
<td>26.3</td>
<td>47.8</td>
<td>57.2</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">93</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">AC-FPN Cascade R-CNN (ResNet-101, single scale)</a></td>
<td>45</td>
<td>64.4</td>
<td>49</td>
<td>26.9</td>
<td>47.7</td>
<td>56.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/attention-guided-context-feature-pyramid">Attention-guided Context Feature Pyramid Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong><br><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">94</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/freeanchor-learning-to-match-anchors-for">FreeAnchor (ResNeXt-101)</a></td>
<td>44.8</td>
<td>64.3</td>
<td>48.4</td>
<td>27</td>
<td>47.9</td>
<td>56</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/freeanchor-learning-to-match-anchors-for">FreeAnchor: Learning to Match Anchors for Visual Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">95</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-64x4d-101-FPN 4 + improvements)</a></td>
<td>44.7</td>
<td>64.1</td>
<td>48.4</td>
<td>27.6</td>
<td>47.5</td>
<td>55.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">96</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask+VoVNet2-57 (single-scale)</a></td>
<td>44.7</td>
<td>63.1</td>
<td>48.6</td>
<td>27.1</td>
<td></td>
<td>55.9</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">97</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">FSAF (ResNeXt-101, multi-scale)</a></td>
<td>44.6</td>
<td>65.2</td>
<td>48.6</td>
<td>29.7</td>
<td>47.1</td>
<td>54.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">98</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">aLRP Loss (ResNext-101, DCN, 500 scale)</a></td>
<td>44.6</td>
<td>65.0</td>
<td>47.5</td>
<td>24.6</td>
<td>48.1</td>
<td>58.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-ranking-based-balanced-loss-function">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNeXt</strong><br><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">99</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask + X-101-32x8d (single-scale)</a></td>
<td>44.6</td>
<td>63.4</td>
<td>48.4</td>
<td></td>
<td>47.2</td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">100</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49, 640x640)</a></td>
<td>44.3</td>
<td>63.8</td>
<td>47.6</td>
<td>25.9</td>
<td>47.7</td>
<td>61.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">101</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-look-one-level-feature">YOLOF-DC5</a></td>
<td>44.3</td>
<td>62.9</td>
<td>47.5</td>
<td>24.0</td>
<td>48.5</td>
<td>60.4</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/you-only-look-one-level-feature">You Only Look One-level Feature</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">102</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">GFLV2 (ResNet-50)</a></td>
<td>44.3</td>
<td>62.3</td>
<td>48.5</td>
<td>26.8</td>
<td>47.7</td>
<td>54.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/generalized-focal-loss-v2-learning-reliable">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">103</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-intertwiner-for-object-detection-1">InterNet (ResNet-101-FPN, multi-scale)</a></td>
<td>44.2</td>
<td>67.5</td>
<td>51.1</td>
<td>27.2</td>
<td>50.3</td>
<td>57.7</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-intertwiner-for-object-detection-1">Feature Intertwiner for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong><br><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">104</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (VGG-16, multi-scale)</a></td>
<td>44.2</td>
<td>64.6</td>
<td>49.3</td>
<td>29.2</td>
<td>47.9</td>
<td>55.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">105</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/lip-local-importance-based-pooling">Faster R-CNN (LIP-ResNet-101-MD w FPN)</a></td>
<td>43.9</td>
<td>65.7</td>
<td>48.1</td>
<td>25.4</td>
<td>46.7</td>
<td>56.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/lip-local-importance-based-pooling">LIP: Local Importance-based Pooling</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">106</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (ResNet-101, multi-scale)</a></td>
<td>43.9</td>
<td>64.4</td>
<td>48</td>
<td>29.6</td>
<td>49.6</td>
<td>54.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>multiscale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">107</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/learning-spatial-fusion-for-single-shot">YOLOv3 @800 + ASFF* (Darknet-53)</a></td>
<td>43.9</td>
<td>64.1</td>
<td>49.2</td>
<td>27.0</td>
<td>46.6</td>
<td>53.4</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/learning-spatial-fusion-for-single-shot">Learning Spatial Fusion for Single-Shot Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">108</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td>
<td>43.9</td>
<td>63.5</td>
<td>47.7</td>
<td>26.8</td>
<td>46.9</td>
<td>55.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">109</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">ExtremeNet (Hourglass-104, multi-scale)</a></td>
<td>43.7</td>
<td>60.5</td>
<td>47.0</td>
<td>24.1</td>
<td>46.9</td>
<td>57.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">Bottom-up Object Detection by Grouping Extreme and Center Points</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">110</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolov4-optimal-speed-and-accuracy-of-object">YOLOv4-608</a></td>
<td>43.5</td>
<td>65.7</td>
<td>47.3</td>
<td>26.7</td>
<td>46.7</td>
<td>53.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolov4-optimal-speed-and-accuracy-of-object">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>single scale</strong><br><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">111</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER (ResNet-50)</a></td>
<td>43.5</td>
<td>65.0</td>
<td>48.6</td>
<td>26.1</td>
<td>46.3</td>
<td>56.0</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/sniper-efficient-multi-scale-training">SNIPER: Efficient Multi-Scale Training</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">112</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">CenterNet (HRNetV2-W48)</a></td>
<td>43.5</td>
<td></td>
<td>46.5</td>
<td>22.2</td>
<td></td>
<td>57.8</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">113</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">D-RFCN + SNIP (ResNet-101, multi-scale)</a></td>
<td>43.4</td>
<td>65.5</td>
<td>48.4</td>
<td>27.2</td>
<td>46.5</td>
<td>54.9</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/an-analysis-of-scale-invariance-in-object-1">An Analysis of Scale Invariance in Object Detection - SNIP</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>multiscale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">114</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/grid-r-cnn">Grid R-CNN (ResNeXt-101-FPN)</a></td>
<td>43.2</td>
<td>63.0</td>
<td>46.6</td>
<td>25.1</td>
<td>46.5</td>
<td>55.2</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/grid-r-cnn">Grid R-CNN</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">115</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-101-64x4d-FPN)</a></td>
<td>43.2</td>
<td>62.8</td>
<td>46.6</td>
<td>26.5</td>
<td>46.2</td>
<td>53.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">116</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190408900">CornerNet-Saccade (Hourglass-104, multi-scale)</a></td>
<td>43.2</td>
<td></td>
<td></td>
<td>24.4</td>
<td>44.6</td>
<td>57.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190408900">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">117</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/libra-r-cnn-towards-balanced-learning-for">Libra R-CNN (ResNeXt-101-FPN)</a></td>
<td>43.0</td>
<td>64</td>
<td>47</td>
<td>25.3</td>
<td>45.6</td>
<td>54.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/libra-r-cnn-towards-balanced-learning-for">Libra R-CNN: Towards Balanced Learning for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">118</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101-DCN)</a></td>
<td>42.8</td>
<td>65.0</td>
<td>46.3</td>
<td>24.9</td>
<td>46.2</td>
<td>54.7</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>DCN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">119</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet-49 (640, RetinaNet, single-scale)</a></td>
<td>42.8</td>
<td>62.3</td>
<td>46.1</td>
<td>23.7</td>
<td>45.2</td>
<td>57.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">120</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-101-FPN+, cascade)</a></td>
<td>42.8</td>
<td>62.1</td>
<td>46.3</td>
<td>23.7</td>
<td>45.5</td>
<td>55.2</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">121</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-high-quality-object-detection">Cascade R-CNN</a></td>
<td>42.8</td>
<td>62.1</td>
<td>46.3</td>
<td>23.7</td>
<td>45.5</td>
<td>55.2</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-high-quality-object-detection">Cascade R-CNN: High Quality Object Detection and Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">122</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">TridentNet (ResNet-101)</a></td>
<td>42.7</td>
<td>63.6</td>
<td>46.5</td>
<td>23.9</td>
<td>46.6</td>
<td>56.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/scale-aware-trident-networks-for-object">Scale-Aware Trident Networks for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">123</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (ResNeXt-32x8d-101-FPN)</a></td>
<td>42.7</td>
<td>62.2</td>
<td>46.1</td>
<td>26.0</td>
<td>45.6</td>
<td>52.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">124</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask (ResNeXt-101-FPN-GN)</a></td>
<td>42.6</td>
<td>62.5</td>
<td>46.0</td>
<td>24.8</td>
<td>45.6</td>
<td>53.8</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">125</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/tood-task-aligned-one-stage-object-detection">TAL + TAP</a></td>
<td>42.5</td>
<td>60.3</td>
<td>46.4</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/tood-task-aligned-one-stage-object-detection">TOOD: Task-aligned One-stage Object Detection</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">126</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Faster R-CNN (HRNetV2p-W48)</a></td>
<td>42.4</td>
<td>63.6</td>
<td>46.4</td>
<td>24.9</td>
<td>44.6</td>
<td>53.0</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">127</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/hierarchical-shot-detector">HSD (Rest101, 768x768, single-scale test)</a></td>
<td>42.3</td>
<td>61.2</td>
<td>46.9</td>
<td>22.8</td>
<td>47.3</td>
<td>55.9</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/hierarchical-shot-detector">Hierarchical Shot Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">128</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet511 (Hourglass-104, multi-scale)</a></td>
<td>42.1</td>
<td>57.8</td>
<td>45.3</td>
<td>20.8</td>
<td>44.8</td>
<td>56.7</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet: Detecting Objects as Paired Keypoints</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">129</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td>
<td>42.1</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">130</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS (HRNet-W32-5l)</a></td>
<td>42.0</td>
<td>60.4</td>
<td>45.3</td>
<td>25.4</td>
<td>45.0</td>
<td>51.0</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fcos-fully-convolutional-one-stage-object">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">131</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512+ (ResNet-101)</a></td>
<td>41.8</td>
<td>62.9</td>
<td>45.7</td>
<td>25.6</td>
<td>45.1</td>
<td>54.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">132</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/gradient-harmonized-single-stage-detector">GHM-C + GHM-R (RetinaNet-FPN-ResNeXt-101)</a></td>
<td>41.6</td>
<td>62.8</td>
<td>44.2</td>
<td>22.3</td>
<td>45.1</td>
<td>55.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/gradient-harmonized-single-stage-detector">Gradient Harmonized Single-stage Detector</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">133</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/objects-as-points">CenterNet-DLA (DLA-34, multi-scale)</a></td>
<td>41.6</td>
<td></td>
<td></td>
<td>21.5</td>
<td>43.9</td>
<td>56.0</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/objects-as-points">Objects as Points</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">134</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">RetinaNet (SpineNet-49S, 640x640)</a></td>
<td>41.5</td>
<td>60.5</td>
<td>44.6</td>
<td>23.3</td>
<td>45</td>
<td>58</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/spinenet-learning-scale-permuted-backbone-for">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">135</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RPDet (ResNet-101)</a></td>
<td>41</td>
<td>62.9</td>
<td>44.3</td>
<td>23.6</td>
<td>44.1</td>
<td>51.7</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/reppoints-point-set-representation-for-object">RepPoints: Point Set Representation for Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">136</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (VGG-16, single-scale)</a></td>
<td>41.0</td>
<td>59.7</td>
<td>45</td>
<td>22.1</td>
<td>46.5</td>
<td>53.8</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">137</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">FSAF (ResNet-101, single-scale)</a></td>
<td>40.9</td>
<td>61.5</td>
<td>44</td>
<td>24</td>
<td>44.2</td>
<td>51.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-selective-anchor-free-module-for">Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">138</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">RetinaNet (ResNeXt-101-FPN)</a></td>
<td>40.8</td>
<td>61.1</td>
<td>44.1</td>
<td>24.1</td>
<td>44.2</td>
<td>51.2</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">Focal Loss for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">139</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-50-FPN+, cascade)</a></td>
<td>40.6</td>
<td>59.9</td>
<td>44</td>
<td>22.6</td>
<td>42.7</td>
<td>52.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">140</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Faster R-CNN (Cascade RPN)</a></td>
<td>40.6</td>
<td>58.9</td>
<td>44.5</td>
<td>22.0</td>
<td>42.8</td>
<td>52.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">141</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-kernels-adapting-effective">ResNet-50-DW-DPN (Deformable Kernels)</a></td>
<td>40.6</td>
<td></td>
<td></td>
<td>24.6</td>
<td>43.9</td>
<td>53.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-kernels-adapting-effective">Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">142</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/acquisition-of-localization-confidence-for">IoU-Net</a></td>
<td>40.6</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/acquisition-of-localization-confidence-for">Acquisition of Localization Confidence for Accurate Object Detection</a></td>
<td></td>
<td></td>
<td>2018</td>
<td></td>
</tr>
<tr>
<td align="center">143</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">FCOS (HRNetV2p-W48)</a></td>
<td>40.5</td>
<td>59.3</td>
<td></td>
<td>23.4</td>
<td>42.6</td>
<td>51.0</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">144</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/softer-nms-rethinking-bounding-box-regression">ResNet-50-FPN Mask R-CNN + KL Loss + var voting + soft-NMS</a></td>
<td>40.4</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/softer-nms-rethinking-bounding-box-regression">Bounding Box Regression with Uncertainty for Accurate Object Detection</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">145</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/rdsnet-a-new-deep-architecture-for-reciprocal">RDSNet (ResNet-101, RetinaNet, mask, MBRM)</a></td>
<td>40.3</td>
<td>60.1</td>
<td>43</td>
<td>22.1</td>
<td>43.5</td>
<td>51.5</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/rdsnet-a-new-deep-architecture-for-reciprocal">RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">146</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">ExtremeNet (Hourglass-104, single-scale)</a></td>
<td>40.2</td>
<td>55.5</td>
<td>43.2</td>
<td>20.4</td>
<td>43.2</td>
<td>53.1</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bottom-up-object-detection-by-grouping">Bottom-up Object Detection by Grouping Extreme and Center Points</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">147</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cross-iteration-batch-normalization">Mask R-CNN (ResNet-101-FPN, CBN)</a></td>
<td>40.1</td>
<td>60.5</td>
<td>44.1</td>
<td>35.8</td>
<td>57.3</td>
<td>38.5</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cross-iteration-batch-normalization">Cross-Iteration Batch Normalization</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">148</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Fast R-CNN (Cascade RPN)</a></td>
<td>40.1</td>
<td>59.4</td>
<td>43.8</td>
<td>22.1</td>
<td>42.4</td>
<td>51.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-rpn-delving-into-high-quality-region">Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">149</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN (ResNeXt-101-FPN)</a></td>
<td>39.8</td>
<td>62.3</td>
<td>43.4</td>
<td>22.1</td>
<td>43.2</td>
<td>51.2</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>ResNeXt</strong><br><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">150</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/region-proposal-by-guided-anchoring">GA-Faster-RCNN</a></td>
<td>39.8</td>
<td>59.2</td>
<td>43.5</td>
<td>21.8</td>
<td>42.6</td>
<td>50.7</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/region-proposal-by-guided-anchoring">Region Proposal by Guided Anchoring</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">151</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/chainercv-a-library-for-deep-learning-in">FPN (ResNet101 backbone)</a></td>
<td>39.5</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/chainercv-a-library-for-deep-learning-in">ChainerCV: a Library for Deep Learning in Computer Vision</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">152</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask (ResNet-50-FPN)</a></td>
<td>39.4</td>
<td>58.6</td>
<td>42.3</td>
<td>21.9</td>
<td>42.0</td>
<td>51.0</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/retinamask-learning-to-predict-masks-improves">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">153</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO (320x320)</a></td>
<td>39.3</td>
<td>59.3</td>
<td>42.7</td>
<td>16.7</td>
<td>41.4</td>
<td>57.8</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pp-yolo-an-effective-and-efficient">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">154</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190409925">AA-ResNet-10 + RetinaNet</a></td>
<td>39.2</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190409925">Attention Augmented Convolutional Networks</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">155</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">MAL (ResNet50, single-scale)</a></td>
<td>39.2</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multiple-anchor-learning-for-visual-object">Multiple Anchor Learning for Visual Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">156</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">RetinaNet (ResNet-101-FPN)</a></td>
<td>39.1</td>
<td>59.1</td>
<td>42.3</td>
<td>21.8</td>
<td>42.7</td>
<td>50.2</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/focal-loss-for-dense-object-detection">Focal Loss for Dense Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">157</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-101-FPN+)</a></td>
<td>38.8</td>
<td>61.1</td>
<td>41.9</td>
<td>21.3</td>
<td>41.8</td>
<td>49.8</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">158</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det (ResNet-101, single-scale)</a></td>
<td>38.8</td>
<td>59.4</td>
<td>41.7</td>
<td>20.5</td>
<td>43.9</td>
<td>53.4</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>single scale</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">159</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/saccadenet-a-fast-and-accurate-object">SaccadeNet (DLA-34-DCN)</a></td>
<td>38.5</td>
<td>55.6</td>
<td>41.4</td>
<td>19.2</td>
<td>42.1</td>
<td>50.6</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/saccadenet-a-fast-and-accurate-object">SaccadeNet: A Fast and Accurate Object Detector</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>DCN</strong></td>
</tr>
<tr>
<td align="center">160</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN (ResNet-101-FPN)</a></td>
<td>38.2</td>
<td>60.3</td>
<td>41.7</td>
<td>20.1</td>
<td>41.1</td>
<td>50.2</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mask-r-cnn">Mask R-CNN</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong><br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">161</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/segmentation-is-all-you-need">WSMA-Seg</a></td>
<td>38.1</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/segmentation-is-all-you-need">Segmentation is All You Need</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">162</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Faster R-CNN + FPN + CGD</a></td>
<td>37.9</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Compact Global Descriptor for Neural Networks</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">163</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet511 (Hourglass-52, single-scale)</a></td>
<td>37.8</td>
<td>53.7</td>
<td>40.1</td>
<td>17.0</td>
<td>39.0</td>
<td>50.5</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cornernet-detecting-objects-as-paired">CornerNet: Detecting Objects as Paired Keypoints</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>single scale</strong></td>
</tr>
<tr>
<td align="center">164</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512+ (VGG-16)</a></td>
<td>37.6</td>
<td>58.7</td>
<td>40.8</td>
<td>22.7</td>
<td>40.3</td>
<td>48.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">165</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-convolutional-networks">DeformConv-R-FCN (Aligned-Inception-ResNet)</a></td>
<td>37.5</td>
<td>58.0</td>
<td></td>
<td>19.4</td>
<td>40.1</td>
<td>52.5</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deformable-convolutional-networks">Deformable Convolutional Networks</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">166</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data">Faster R-CNN (ImageNet+300M)</a></td>
<td>37.4</td>
<td>58</td>
<td>40.1</td>
<td>17.5</td>
<td>41.1</td>
<td>51.2</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">167</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">Mask R-CNN (Bottleneck-injected ResNet-50, FPN)</a></td>
<td>36.9</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>FPN</strong><br>！！<strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">168</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/beyond-skip-connections-top-down-modulation">Faster R-CNN + TDM</a></td>
<td>36.8</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/beyond-skip-connections-top-down-modulation">Beyond Skip Connections: Top-Down Modulation for Object Detection</a></td>
<td></td>
<td></td>
<td>2016</td>
<td></td>
</tr>
<tr>
<td align="center">169</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN (ResNet-50-FPN+)</a></td>
<td>36.5</td>
<td>59</td>
<td>39.2</td>
<td>20.3</td>
<td>38.8</td>
<td>46.4</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/cascade-r-cnn-delving-into-high-quality">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>FPN</strong>;<br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">170</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512 (ResNet-101)</a></td>
<td>36.4</td>
<td>57.5</td>
<td>39.5</td>
<td>16.6</td>
<td>39.9</td>
<td>51.4</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">171</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection">Faster R-CNN + FPN</a></td>
<td>36.2</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection">Feature Pyramid Networks for Object Detection</a></td>
<td></td>
<td></td>
<td>2016</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">172</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">Faster R-CNN (Bottleneck-injected ResNet-50 and FPN)</a></td>
<td>35.9</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/torchdistill-a-modular-configuration-driven">torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>FPN</strong>;<br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">173</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition">Faster R-CNN (box refinement, context, multi-scale testing)</a></td>
<td>34.9</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition">Deep Residual Learning for Image Recognition</a></td>
<td></td>
<td></td>
<td>2015</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">174</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/speedaccuracy-trade-offs-for-modern">Faster R-CNN</a></td>
<td>34.7</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/speedaccuracy-trade-offs-for-modern">Speed/accuracy trade-offs for modern convolutional object detectors</a></td>
<td></td>
<td></td>
<td>2016</td>
<td></td>
</tr>
<tr>
<td align="center">175</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190408900">CornerNet-Squeeze</a></td>
<td>34.4</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190408900">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">176</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-multipath-network-for-object-detection">MultiPath Network</a></td>
<td>33.2</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/a-multipath-network-for-object-detection">A MultiPath Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2016</td>
<td></td>
</tr>
<tr>
<td align="center">177</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/inside-outside-net-detecting-objects-in">ION</a></td>
<td>33.1</td>
<td>55.7</td>
<td>34.6</td>
<td>14.5</td>
<td>35.2</td>
<td>47.2</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/inside-outside-net-detecting-objects-in">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</a></td>
<td></td>
<td></td>
<td>2015</td>
<td></td>
</tr>
<tr>
<td align="center">178</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">RefineDet512 (VGG-16)</a></td>
<td>33</td>
<td>54.5</td>
<td>35.5</td>
<td>16.3</td>
<td>36.3</td>
<td>44.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/single-shot-refinement-neural-network-for">Single-Shot Refinement Neural Network for Object Detection</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">179</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolov3-an-incremental-improvement">YOLOv3 + Darknet-53</a></td>
<td>33.0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/yolov3-an-incremental-improvement">YOLOv3: An Incremental Improvement</a></td>
<td></td>
<td></td>
<td>2018</td>
<td><strong>YOLO</strong></td>
</tr>
<tr>
<td align="center">180</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/ssd-single-shot-multibox-detector">SSD512</a></td>
<td>28.8</td>
<td>48.5</td>
<td>30.3</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/ssd-single-shot-multibox-detector">SSD: Single Shot MultiBox Detector</a></td>
<td></td>
<td></td>
<td>2015</td>
<td></td>
</tr>
<tr>
<td align="center">181</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MobileNetV2)</a></td>
<td>26.1</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">182</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/espnetv2-a-light-weight-power-efficient-and">ESPNetv2-512</a></td>
<td>26.0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/espnetv2-a-light-weight-power-efficient-and">ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network</a></td>
<td></td>
<td></td>
<td>2018</td>
<td></td>
</tr>
<tr>
<td align="center">183</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MobileNetV3)</a></td>
<td>25.5</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">184</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN (MNASNet-B1)</a></td>
<td>24.6</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">185</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN x0.7 (MobileNetV2)</a></td>
<td>23.8</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mnasfpn-learning-latency-aware-pyramid">MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>FPN</strong></td>
</tr>
<tr>
<td align="center">186</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">MobielNet-v1-SSD-300x300+CGD</a></td>
<td>21.4</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/compact-global-descriptor-for-neural-networks">Compact Global Descriptor for Neural Networks</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">187</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fast-r-cnn">Fast-RCNN</a></td>
<td>19.7</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/fast-r-cnn">Fast R-CNN</a></td>
<td></td>
<td></td>
<td>2015</td>
<td></td>
</tr>
<tr>
<td align="center">188</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mobilenets-efficient-convolutional-neural">MobileNet</a></td>
<td>19.3</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/mobilenets-efficient-convolutional-neural">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></td>
<td></td>
<td></td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td align="center">189</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/vision-transformer-with-deformable-attention">DAT-S (RetinaNet)</a></td>
<td></td>
<td>69.6</td>
<td>51.2</td>
<td>32.3</td>
<td>51.8</td>
<td>63.4</td>
<td>47.9</td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/vision-transformer-with-deformable-attention">Vision Transformer with Deformable Attention</a></td>
<td></td>
<td></td>
<td>2022</td>
<td></td>
</tr>
<tr>
<td align="center">190</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask-VoVNet99 (multi-scale)</a></td>
<td></td>
<td>68.3</td>
<td>53.2</td>
<td>32.4</td>
<td></td>
<td>60.0</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>multiscale</strong></td>
</tr>
<tr>
<td align="center">191</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Mask R-CNN (HRNetV2p-W32 + cascade)</a></td>
<td></td>
<td>62.5</td>
<td>48.6</td>
<td></td>
<td></td>
<td>56.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">192</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td>
<td></td>
<td>61.9</td>
<td>45.2</td>
<td></td>
<td>46.8</td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">193</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/virtex-learning-visual-representations-from">VirTex Mask R-CNN (ResNet-50-FPN)</a></td>
<td></td>
<td>61.7</td>
<td>44.8</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/virtex-learning-visual-representations-from">VirTex: Learning Visual Representations from Textual Annotations</a></td>
<td></td>
<td></td>
<td>2020</td>
<td><strong>FPN</strong>;<br><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">194</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">Centermask + ResNet101</a></td>
<td></td>
<td>61.6</td>
<td>46.9</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1">CenterMask : Real-Time Anchor-Free Instance Segmentation</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">195</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pafnet-an-efficient-anchor-free-object">PAFNet (ResNet50-vd)</a></td>
<td></td>
<td>59.8</td>
<td>45.3</td>
<td>22.8</td>
<td>45.8</td>
<td>59.2</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/pafnet-an-efficient-anchor-free-object">PAFNet: An Efficient Anchor-Free Object Detector Guidance</a></td>
<td></td>
<td></td>
<td>2021</td>
<td><strong>ResNet</strong></td>
</tr>
<tr>
<td align="center">196</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dctd-deep-conditional-target-densities-for">IoU-Net+EnergyRegression</a></td>
<td></td>
<td>58.5</td>
<td>41.8</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/dctd-deep-conditional-target-densities-for">Energy-Based Models for Deep Probabilistic Regression</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">197</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Cascade R-CNN (HRNetV2p-W48)</a></td>
<td></td>
<td></td>
<td>48.6</td>
<td>26.0</td>
<td>47.3</td>
<td>56.3</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
<td></td>
<td></td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td align="center">198</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR (ResNet50-FPN-3x, single-scale)</a></td>
<td></td>
<td></td>
<td></td>
<td>27.8</td>
<td>48.7</td>
<td>59.9</td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/istr-end-to-end-instance-segmentation-with">ISTR: End-to-End Instance Segmentation with Transformers</a></td>
<td></td>
<td></td>
<td>2021</td>
<td></td>
</tr>
<tr>
<td align="center">199</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox (ResNeXt-101)</a></td>
<td></td>
<td></td>
<td></td>
<td>24.9</td>
<td></td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>ResNeXt</strong></td>
</tr>
<tr>
<td align="center">200</td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet-D7x (single-scale)</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>57.9</td>
<td></td>
<td></td>
<td align="center"></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/efficientdet-scalable-and-efficient-object">EfficientDet: Scalable and Efficient Object Detection</a></td>
<td></td>
<td></td>
<td>2019</td>
<td><strong>single scale</strong></td>
</tr>
</tbody></table>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/blog/about" rel="external nofollow noreferrer">LinXu</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://islinxu.github.io/blog/blog/2022/02/16/ji-yu-coco-shu-ju-ji-yan-zheng-de-mu-biao-jian-ce-suan-fa-tian-ti-pai-xing-bang/">https://islinxu.github.io/blog/blog/2022/02/16/ji-yu-coco-shu-ju-ji-yan-zheng-de-mu-biao-jian-ce-suan-fa-tian-ti-pai-xing-bang/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/blog/about" target="_blank">LinXu</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/blog/tags/Object-Detection/">
                                    <span class="chip bg-color">Object_Detection</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/blog/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/blog/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/blog/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/blog/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/blog/2022/02/21/ji-yu-swintransformer-de-mu-biao-jian-ce-xun-lian-mo-xing-xue-xi-zong-jie/">
                    <div class="card-image">
                        
                        
                        <img src="/blog/medias/featureimages/38.jpg" class="responsive-img" alt="SwinTranformer_Det">
                        
                        <span class="card-title">SwinTranformer_Det</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2022-02-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/blog/categories/Model/" class="post-category">
                                    Model
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/blog/tags/%E7%82%BC%E4%B8%B9%E6%9C%AF/">
                        <span class="chip bg-color">炼丹术</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/blog/2022/02/15/mu-biao-jian-ce-zong-shu-xue-xi-zong-jie/">
                    <div class="card-image">
                        
                        
                        <img src="/blog/medias/featureimages/33.jpg" class="responsive-img" alt="目标检测综述学习总结">
                        
                        <span class="card-title">目标检测综述学习总结</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2022-02-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" class="post-category">
                                    论文笔记
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/blog/tags/Survey/">
                        <span class="chip bg-color">Survey</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE' || selection.getRangeAt(0).commonAncestorContainer.nodeName === 'CODE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: LinXu<br />'
            + '文章作者: LinXu<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/blog/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/blog/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/blog/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/blog/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/blog/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/blog/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/blog/libs/aplayer/APlayer.min.js"></script>
<script src="/blog/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2022</span>
            
            <a href="/blog/about" target="_blank">LinXu</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">150.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/isLinXu" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:17746071609@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>



    <a href="https://www.facebook.com/xxx" class="tooltipped" target="_blank" data-tooltip="关注我的Facebook: https://www.facebook.com/xxx" data-position="top" data-delay="50">
        <i class="fab fa-facebook-f"></i>
    </a>



    <a href="https://twitter.com/xxx" class="tooltipped" target="_blank" data-tooltip="关注我的Twitter: https://twitter.com/xxx" data-position="top" data-delay="50">
        <i class="fab fa-twitter"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=2267379130" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 2267379130" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>



    <a href="https://weibo.com/xxx" class="tooltipped" target="_blank" data-tooltip="关注我的微博: https://weibo.com/xxx" data-position="top" data-delay="50">
        <i class="fab fa-weibo"></i>
    </a>



    <a href="https://www.zhihu.com/xxx" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/xxx" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



    <a href="/blog/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/blog/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/blog/libs/materialize/materialize.min.js"></script>
    <script src="/blog/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/blog/libs/aos/aos.js"></script>
    <script src="/blog/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/blog/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/blog/js/matery.js"></script>

    

    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/blog/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/blog/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/blog/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/blog/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
